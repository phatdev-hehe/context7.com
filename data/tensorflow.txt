TITLE: Installing seaborn in Python
DESCRIPTION: Installs the seaborn library for data visualization using pip.

LANGUAGE: Python
CODE:
!pip install -q seaborn

----------------------------------------

TITLE: Denoising Autoencoder Implementation in TensorFlow
DESCRIPTION: Implements a convolutional autoencoder for image denoising using Conv2D and Conv2DTranspose layers. The model takes noisy images as input and outputs cleaned versions.

LANGUAGE: Python
CODE:
class Denoise(Model):
  def __init__(self):
    super(Denoise, self).__init__()
    self.encoder = tf.keras.Sequential([
      layers.Input(shape=(28, 28, 1)),
      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),
      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])

    self.decoder = tf.keras.Sequential([
      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),
      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),
      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])

  def call(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return decoded

----------------------------------------

TITLE: Training and Evaluating Distributed Model
DESCRIPTION: Trains the model using Keras fit() API and evaluates performance on test dataset

LANGUAGE: python
CODE:
EPOCHS = 12

model.fit(train_dataset, epochs=EPOCHS, callbacks=callbacks)

eval_loss, eval_acc = model.evaluate(eval_dataset)
print('Eval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))

----------------------------------------

TITLE: Creating and Training Neural Network Model
DESCRIPTION: Build and compile a sequential model using TensorFlow Hub's pre-trained embedding layer and dense layers for classification

LANGUAGE: python
CODE:
model = keras.Sequential()
model.add(hub_layer)
model.add(keras.layers.Dense(16, activation='relu'))
model.add(keras.layers.Dense(1))

model.compile(optimizer='adam',
              loss=keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_data.shuffle(10000).batch(512),
                    epochs=10,
                    validation_data=validation_data.batch(512),
                    verbose=1)

----------------------------------------

TITLE: Implementing Custom Training Loop with tf.function
DESCRIPTION: Shows how to implement a custom training loop using tf.function for better performance, including gradient calculation and optimization steps.

LANGUAGE: python
CODE:
@tf.function
def train_step(inputs, labels):
  with tf.GradientTape() as tape:
    predictions = model(inputs, training=True)
    regularization_loss=tf.math.add_n(model.losses)
    pred_loss=loss_fn(labels, predictions)
    total_loss=pred_loss + regularization_loss

  gradients = tape.gradient(total_loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

for epoch in range(NUM_EPOCHS):
  for inputs, labels in train_data:
    train_step(inputs, labels)
  print("Finished epoch", epoch)

----------------------------------------

TITLE: Manual GPU Memory Management in TensorFlow
DESCRIPTION: Shows how to configure GPU memory growth and set memory limits for TensorFlow operations.

LANGUAGE: python
CODE:
gpus = tf.config.list_physical_devices('GPU')
if gpus:
  try:
    # Currently, memory growth needs to be the same across GPUs
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    logical_gpus = tf.config.list_logical_devices('GPU')
    print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
  except RuntimeError as e:
    # Memory growth must be set before GPUs have been initialized
    print(e)

----------------------------------------

TITLE: Building and Training Models with Keras
DESCRIPTION: Demonstrates creating and training a model using Keras Sequential API with custom layers and optimizers.

LANGUAGE: python
CODE:
new_model = tf.keras.Sequential([
    tf.keras.layers.Lambda(lambda x: tf.stack([x, x**2], axis=1)),
    tf.keras.layers.Dense(units=1, kernel_initializer=tf.random.normal)])

new_model.compile(
    loss=tf.keras.losses.MSE,
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01))

history = new_model.fit(x, y,
                      epochs=100,
                      batch_size=32,
                      verbose=0)

----------------------------------------

TITLE: Creating Transfer Learning Model Architecture
DESCRIPTION: Build model using MobileNetV2 base with custom classification head for transfer learning

LANGUAGE: Python
CODE:
inputs = tf.keras.Input(shape=(160, 160, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

----------------------------------------

TITLE: Creating Convolutional Base for CNN
DESCRIPTION: This code defines the convolutional base of the CNN using Conv2D and MaxPooling2D layers.

LANGUAGE: Python
CODE:
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

----------------------------------------

TITLE: Loading and Preparing MNIST Dataset
DESCRIPTION: Load the MNIST dataset and normalize pixel values to range [0,1] by dividing by 255.0.

LANGUAGE: python
CODE:
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

----------------------------------------

TITLE: Creating Transfer Learning Model Architecture
DESCRIPTION: Build model using MobileNetV2 base with custom classification head for transfer learning

LANGUAGE: Python
CODE:
inputs = tf.keras.Input(shape=(160, 160, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

----------------------------------------

TITLE: Creating and Training the Model
DESCRIPTION: Build a model using the Keras Functional API, compile it, and train it on the preprocessed data.

LANGUAGE: Python
CODE:
all_features = tf.keras.layers.concatenate(encoded_features)
x = tf.keras.layers.Dense(32, activation="relu")(all_features)
x = tf.keras.layers.Dropout(0.5)(x)
output = tf.keras.layers.Dense(1)(x)

model = tf.keras.Model(all_inputs, output)

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=["accuracy"])

model.fit(train_ds, epochs=10, validation_data=val_ds)

----------------------------------------

TITLE: Importing TensorFlow and Required Libraries for CNN
DESCRIPTION: This snippet imports TensorFlow, Keras modules, and matplotlib for visualization.

LANGUAGE: Python
CODE:
import tensorflow as tf

from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

----------------------------------------

TITLE: Defining CNN Model Architecture
DESCRIPTION: Creates a sequential CNN model with multiple convolution and pooling layers for image classification

LANGUAGE: Python
CODE:
model = tf.keras.Sequential([
  tf.keras.layers.Rescaling(1./255),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(num_classes)
])

----------------------------------------

TITLE: Implementing Multi-Step Prediction Models
DESCRIPTION: Develops models for multi-step predictions, including linear, dense, CNN, and RNN architectures, as well as an autoregressive LSTM model.

LANGUAGE: Python
CODE:
OUT_STEPS = 24
multi_window = WindowGenerator(input_width=24,
                               label_width=OUT_STEPS,
                               shift=OUT_STEPS)

# Example of multi-step CNN model
multi_conv_model = tf.keras.Sequential([
    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),
    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),
    tf.keras.layers.Dense(OUT_STEPS*num_features,
                          kernel_initializer=tf.initializers.zeros()),
    tf.keras.layers.Reshape([OUT_STEPS, num_features])
])

# Similar code blocks for other multi-step models

----------------------------------------

TITLE: Defining Training Step
DESCRIPTION: Implements the training step function that performs forward pass, loss calculation, and gradient updates for both generators and discriminators.

LANGUAGE: Python
CODE:
@tf.function
def train_step(real_x, real_y):
  with tf.GradientTape(persistent=True) as tape:
    # Generator G translates X -> Y
    # Generator F translates Y -> X.
    fake_y = generator_g(real_x, training=True)
    cycled_x = generator_f(fake_y, training=True)

    fake_x = generator_f(real_y, training=True)
    cycled_y = generator_g(fake_x, training=True)

    # same_x and same_y are used for identity loss.
    same_x = generator_f(real_x, training=True)
    same_y = generator_g(real_y, training=True)

    disc_real_x = discriminator_x(real_x, training=True)
    disc_real_y = discriminator_y(real_y, training=True)

    disc_fake_x = discriminator_x(fake_x, training=True)
    disc_fake_y = discriminator_y(fake_y, training=True)

    # calculate the loss
    gen_g_loss = generator_loss(disc_fake_y)
    gen_f_loss = generator_loss(disc_fake_x)
    
    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)
    
    # Total generator loss = adversarial loss + cycle loss
    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)
    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)

    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)
    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)
  
  # Calculate the gradients for generator and discriminator
  generator_g_gradients = tape.gradient(total_gen_g_loss, 
                                        generator_g.trainable_variables)
  generator_f_gradients = tape.gradient(total_gen_f_loss, 
                                        generator_f.trainable_variables)
  
  discriminator_x_gradients = tape.gradient(disc_x_loss, 
                                            discriminator_x.trainable_variables)
  discriminator_y_gradients = tape.gradient(disc_y_loss, 
                                            discriminator_y.trainable_variables)
  
  # Apply the gradients to the optimizer
  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, 
                                            generator_g.trainable_variables))

  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, 
                                            generator_f.trainable_variables))
  
  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,
                                                discriminator_x.trainable_variables))
  
  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,
                                                discriminator_y.trainable_variables))

----------------------------------------

TITLE: Implementing Training Loop
DESCRIPTION: Implement the main training loop that iterates over epochs and batches.

LANGUAGE: Python
CODE:
for epoch in range(EPOCHS):
  # TRAIN LOOP
  total_loss = 0.0
  num_batches = 0
  for x in train_dist_dataset:
    total_loss += distributed_train_step(x)
    num_batches += 1
  train_loss = total_loss / num_batches

  # TEST LOOP
  for x in test_dist_dataset:
    distributed_test_step(x)

  if epoch % 2 == 0:
    checkpoint.save(checkpoint_prefix)

  template = ("Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, "
              "Test Accuracy: {}")
  print(template.format(epoch + 1, train_loss,
                         train_accuracy.result() * 100, test_loss.result(),
                         test_accuracy.result() * 100))

  test_loss.reset_states()
  train_accuracy.reset_states()
  test_accuracy.reset_states()

----------------------------------------

TITLE: Creating a 3-Axis Tensor in TensorFlow
DESCRIPTION: Shows how to create a 3-dimensional tensor using tf.constant.

LANGUAGE: Python
CODE:
# There can be an arbitrary number of
# axes (sometimes called "dimensions")
rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])

print(rank_3_tensor)

----------------------------------------

TITLE: Defining CVAE Model Architecture
DESCRIPTION: Implements the Convolutional Variational Autoencoder model using tf.keras.Model, including encoder and decoder networks.

LANGUAGE: Python
CODE:
class CVAE(tf.keras.Model):
  """Convolutional variational autoencoder."""

  def __init__(self, latent_dim):
    super(CVAE, self).__init__()
    self.latent_dim = latent_dim
    self.encoder = tf.keras.Sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),
            tf.keras.layers.Conv2D(
                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),
            tf.keras.layers.Conv2D(
                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(latent_dim + latent_dim),
        ]
    )

    self.decoder = tf.keras.Sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),
            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),
            tf.keras.layers.Conv2DTranspose(
                filters=64, kernel_size=3, strides=2, padding='same',
                activation='relu'),
            tf.keras.layers.Conv2DTranspose(
                filters=32, kernel_size=3, strides=2, padding='same',
                activation='relu'),
            tf.keras.layers.Conv2DTranspose(
                filters=1, kernel_size=3, strides=1, padding='same'),
        ]
    )

  @tf.function
  def sample(self, eps=None):
    if eps is None:
      eps = tf.random.normal(shape=(100, self.latent_dim))
    return self.decode(eps, apply_sigmoid=True)

  def encode(self, x):
    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
    return mean, logvar

  def reparameterize(self, mean, logvar):
    eps = tf.random.normal(shape=mean.shape)
    return eps * tf.exp(logvar * .5) + mean

  def decode(self, z, apply_sigmoid=False):
    logits = self.decoder(z)
    if apply_sigmoid:
      probs = tf.sigmoid(logits)
      return probs
    return logits

----------------------------------------

TITLE: Creating Neural Network Model Architecture
DESCRIPTION: Defines a sequential CNN model with convolutional layers, max pooling, dropout, and dense layers for image classification.

LANGUAGE: Python
CODE:
model = Sequential([
  data_augmentation,
  layers.Rescaling(1./255),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Dropout(0.2),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes, name="outputs")
])

----------------------------------------

TITLE: Multi-GPU Strategy Implementation in TensorFlow
DESCRIPTION: Demonstrates how to implement distributed training using TensorFlow's MirroredStrategy for multiple GPUs.

LANGUAGE: python
CODE:
tf.debugging.set_log_device_placement(True)
gpus = tf.config.list_logical_devices('GPU')
strategy = tf.distribute.MirroredStrategy(gpus)
with strategy.scope():
  inputs = tf.keras.layers.Input(shape=(1,))
  predictions = tf.keras.layers.Dense(1)(inputs)
  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)
  model.compile(loss='mse',
                optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))

----------------------------------------

TITLE: Automatic Differentiation with TensorFlow
DESCRIPTION: Demonstrates automatic gradient calculation using tf.GradientTape for a simple quadratic function.

LANGUAGE: python
CODE:
x = tf.Variable(1.0)

def f(x):
  y = x**2 + 2*x - 5
  return y

with tf.GradientTape() as tape:
  y = f(x)

g_x = tape.gradient(y, x)

----------------------------------------

TITLE: U-Net Model Definition
DESCRIPTION: Implementation of U-Net architecture using MobileNetV2 as encoder and custom upsampling blocks

LANGUAGE: Python
CODE:
def unet_model(output_channels:int):
  inputs = tf.keras.layers.Input(shape=[128, 128, 3])

  # Downsampling through the model
  skips = down_stack(inputs)
  x = skips[-1]
  skips = reversed(skips[:-1])

  # Upsampling and establishing the skip connections
  for up, skip in zip(up_stack, skips):
    x = up(x)
    concat = tf.keras.layers.Concatenate()
    x = concat([x, skip])

  # This is the last layer of the model
  last = tf.keras.layers.Conv2DTranspose(
      filters=output_channels, kernel_size=3, strides=2,
      padding='same')  #64x64 -> 128x128

  x = last(x)

  return tf.keras.Model(inputs=inputs, outputs=x)

----------------------------------------

TITLE: Creating an Optimizer
DESCRIPTION: Instantiates a Stochastic Gradient Descent optimizer for model training.

LANGUAGE: Python
CODE:
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

----------------------------------------

TITLE: Building and Training Keras Model with Feature Columns
DESCRIPTION: Constructs a Keras Sequential model using the defined feature columns, compiles it, and trains on the prepared datasets.

LANGUAGE: Python
CODE:
feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

model = tf.keras.Sequential([
  feature_layer,
  layers.Dense(128, activation='relu'),
  layers.Dense(128, activation='relu'),
  layers.Dropout(.1),
  layers.Dense(1)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(train_ds,
          validation_data=val_ds,
          epochs=10)

loss, accuracy = model.evaluate(test_ds)
print("Accuracy", accuracy)

----------------------------------------

TITLE: Building and Training Convolutional Neural Network Model
DESCRIPTION: This code defines a convolutional neural network model using Keras, compiles it, and trains it on the spectrogram datasets.

LANGUAGE: Python
CODE:
model = models.Sequential([
    layers.Input(shape=input_shape),
    layers.Resizing(32, 32),
    norm_layer,
    layers.Conv2D(32, 3, activation='relu'),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.25),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_labels),
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'],
)

EPOCHS = 10
history = model.fit(
    train_spectrogram_ds,
    validation_data=val_spectrogram_ds,
    epochs=EPOCHS,
    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),
)

----------------------------------------

TITLE: Implementing Training Loop
DESCRIPTION: Defines the main training loop that handles the adversarial training process between generator and discriminator networks.

LANGUAGE: Python
CODE:
@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator(noise, training=True)

      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images, training=True)

      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

----------------------------------------

TITLE: Compiling and Training CNN Model
DESCRIPTION: This code compiles the model with Adam optimizer and sparse categorical crossentropy loss, then trains it on the CIFAR10 dataset.

LANGUAGE: Python
CODE:
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs=10, 
                    validation_data=(test_images, test_labels))

----------------------------------------

TITLE: Loading and Preparing MNIST Dataset
DESCRIPTION: Downloads MNIST dataset and separates into training and test sets using TensorFlow Datasets

LANGUAGE: python
CODE:
datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)

mnist_train, mnist_test = datasets['train'], datasets['test']

----------------------------------------

TITLE: Implementing Training and Testing Steps
DESCRIPTION: Defines traceable functions for executing single training and testing steps. These functions handle forward passes, loss calculation, gradient computation, and metric tracking.

LANGUAGE: Python
CODE:
@tf.function
def train_step(model, x_batch, y_batch, loss, metric, optimizer):
  # Execute a single training step
  with tf.GradientTape() as tape:
    y_pred = model(x_batch)
    batch_loss = loss(y_pred, y_batch)
  # Compute gradients and update the model's parameters
  grads = tape.gradient(batch_loss, model.trainable_variables)
  optimizer.apply_gradients(grads)
  # Return batch loss and accuracy
  batch_acc = metric(y_pred, y_batch)
  return batch_loss, batch_acc

@tf.function
def test_step(model, x_batch, y_batch, loss, metric):
  # Execute a single testing step
  y_pred = model(x_batch)
  batch_loss = loss(y_pred, y_batch)
  batch_acc = metric(y_pred, y_batch)
  return batch_loss, batch_acc

----------------------------------------

TITLE: Creating a CentralStorageStrategy instance
DESCRIPTION: Creates a CentralStorageStrategy instance for synchronous training with variables on CPU and operations replicated across GPUs.

LANGUAGE: Python
CODE:
central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()

----------------------------------------

TITLE: Using MirroredStrategy with Keras Model.fit
DESCRIPTION: Demonstrates how to use MirroredStrategy with Keras Model.fit for distributed training.

LANGUAGE: Python
CODE:
mirrored_strategy = tf.distribute.MirroredStrategy()

with mirrored_strategy.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(1, input_shape=(1,),
                            kernel_regularizer=tf.keras.regularizers.L2(1e-4))])
  model.compile(loss='mse', optimizer='sgd')

dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(10)
model.fit(dataset, epochs=2)
model.evaluate(dataset)

----------------------------------------

TITLE: Building 3D CNN Model for Video Classification
DESCRIPTION: Constructs a 3D convolutional neural network model using the Keras functional API, incorporating residual blocks and (2+1)D convolutions for video classification.

LANGUAGE: Python
CODE:
input_shape = (None, 10, HEIGHT, WIDTH, 3)
input = layers.Input(shape=(input_shape[1:]))
x = input

x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.ReLU()(x)
x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)

# Block 1
x = add_residual_block(x, 16, (3, 3, 3))
x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)

# Block 2
x = add_residual_block(x, 32, (3, 3, 3))
x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)

# Block 3
x = add_residual_block(x, 64, (3, 3, 3))
x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)

# Block 4
x = add_residual_block(x, 128, (3, 3, 3))

x = layers.GlobalAveragePooling3D()(x)
x = layers.Flatten()(x)
x = layers.Dense(10)(x)

model = keras.Model(input, x)

----------------------------------------

TITLE: Defining a Mean Squared Error function with tf.function
DESCRIPTION: Creates a Mean Squared Error (MSE) calculation function using tf.function and demonstrates its usage with random data.

LANGUAGE: Python
CODE:
@tf.function
def get_MSE(y_true, y_pred):
  sq_diff = tf.pow(y_true - y_pred, 2)
  return tf.reduce_mean(sq_diff)

y_true = tf.random.uniform([5], maxval=10, dtype=tf.int32)
y_pred = tf.random.uniform([5], maxval=10, dtype=tf.int32)
print(y_true)
print(y_pred)

get_MSE(y_true, y_pred)

----------------------------------------

TITLE: Implementing Custom Gradients in TensorFlow
DESCRIPTION: Show how to create a custom gradient function using tf.custom_gradient decorator to clip gradients during backpropagation.

LANGUAGE: Python
CODE:
@tf.custom_gradient
def clip_gradients(y):
  def backward(dy):
    return tf.clip_by_norm(dy, 0.5)
  return y, backward

v = tf.Variable(2.0)
with tf.GradientTape() as t:
  output = clip_gradients(v * v)
print(t.gradient(output, v))  # calls "backward", which clips 4 to 2

----------------------------------------

TITLE: Defining Neural Network Model Architecture
DESCRIPTION: Creates a sequential neural network model with dense layers and dropout for fraud detection.

LANGUAGE: Python
CODE:
def make_model(metrics=METRICS, output_bias=None):
  if output_bias is not None:
    output_bias = tf.keras.initializers.Constant(output_bias)
  model = keras.Sequential([
      keras.layers.Dense(16, activation='relu',
          input_shape=(train_features.shape[-1],)),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(1, activation='sigmoid',
                         bias_initializer=output_bias),
  ])

  model.compile(
      optimizer=keras.optimizers.Adam(learning_rate=1e-3),
      loss=keras.losses.BinaryCrossentropy(),
      metrics=metrics)

  return model

----------------------------------------

TITLE: Implementing custom training loop with MultiWorkerMirroredStrategy
DESCRIPTION: Defines a custom training step function and uses it within a loop to train the model across multiple workers, including checkpoint saving and restoring.

LANGUAGE: Python
CODE:
@tf.function
def train_step(iterator):
  def step_fn(inputs):
    x, y = inputs
    with tf.GradientTape() as tape:
      predictions = multi_worker_model(x, training=True)
      per_example_loss = tf.keras.losses.SparseCategoricalCrossentropy(
          from_logits=True,
          reduction=tf.keras.losses.Reduction.NONE)(y, predictions)
      loss = tf.nn.compute_average_loss(per_example_loss)
    grads = tape.gradient(loss, multi_worker_model.trainable_variables)
    optimizer.apply_gradients(zip(grads, multi_worker_model.trainable_variables))
    train_accuracy.update_state(y, predictions)
    return loss
  
  per_replica_losses = strategy.run(step_fn, args=(next(iterator),))
  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

# Training loop
while epoch.numpy() < num_epochs:
  iterator = iter(multi_worker_dataset)
  total_loss = 0.0
  num_batches = 0
  while step_in_epoch.numpy() < num_steps_per_epoch:
    total_loss += train_step(iterator)
    num_batches += 1
    step_in_epoch.assign_add(1)
  train_loss = total_loss / num_batches
  print('Epoch: %d, accuracy: %f, train_loss: %f.' %(epoch.numpy(), train_accuracy.result(), train_loss))
  checkpoint_manager.save()
  epoch.assign_add(1)
  step_in_epoch.assign(0)

----------------------------------------

TITLE: Implementing Distributed Training and Testing Steps
DESCRIPTION: Implement distributed versions of the training and testing steps using strategy.run().

LANGUAGE: Python
CODE:
@tf.function
def distributed_train_step(dataset_inputs):
  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)

@tf.function
def distributed_test_step(dataset_inputs):
  return strategy.run(test_step, args=(dataset_inputs,))

----------------------------------------

TITLE: Converting a Python function to a TensorFlow graph function
DESCRIPTION: Demonstrates how to use tf.function to convert a regular Python function into a TensorFlow graph function. The example shows matrix multiplication and addition operations.

LANGUAGE: Python
CODE:
# Define a Python function.
def a_regular_function(x, y, b):
  x = tf.matmul(x, y)
  x = x + b
  return x

# The Python type of `a_function_that_uses_a_graph` will now be a
# `PolymorphicFunction`.
a_function_that_uses_a_graph = tf.function(a_regular_function)

# Make some tensors.
x1 = tf.constant([[1.0, 2.0]])
y1 = tf.constant([[2.0], [3.0]])
b1 = tf.constant(4.0)

orig_value = a_regular_function(x1, y1, b1).numpy()
# Call a `tf.function` like a Python function.
tf_function_value = a_function_that_uses_a_graph(x1, y1, b1).numpy()
assert(orig_value == tf_function_value)

----------------------------------------

TITLE: Training Loop Implementation
DESCRIPTION: Custom training loop implementation for the pix2pix GAN, including generator and discriminator loss calculations and gradient updates.

LANGUAGE: Python
CODE:
@tf.function
def train_step(input_image, target, step):
  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    gen_output = generator(input_image, training=True)

    disc_real_output = discriminator([input_image, target], training=True)
    disc_generated_output = discriminator([input_image, gen_output], training=True)

    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)
    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)

  generator_gradients = gen_tape.gradient(gen_total_loss,
                                          generator.trainable_variables)
  discriminator_gradients = disc_tape.gradient(disc_loss,
                                               discriminator.trainable_variables)

  generator_optimizer.apply_gradients(zip(generator_gradients,
                                          generator.trainable_variables))
  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                              discriminator.trainable_variables))

----------------------------------------

TITLE: Implementing Compression with Energy Retention
DESCRIPTION: Defines a function to compress an image while retaining a specified proportion of its energy.

LANGUAGE: Python
CODE:
def compress_image_with_energy(I, energy_factor, verbose=False):
  # Returns a compressed image based on a desired energy factor
  # Computing SVD
  I_rescaled = tf.convert_to_tensor(I)/255
  I_batched = tf.transpose(I_rescaled, [2, 0, 1]) 
  s, U, V = tf.linalg.svd(I_batched)
  # Extracting singular values
  props_rgb = tf.map_fn(lambda x: tf.cumsum(x)/tf.reduce_sum(x), s)
  props_rgb_mean = tf.reduce_mean(props_rgb, axis=0)
  # Find closest r that corresponds to the energy factor
  r = tf.argmin(tf.abs(props_rgb_mean - energy_factor)) + 1
  actual_ef = props_rgb_mean[r]
  I_r, I_r_prop = compress_image(I, r, verbose=verbose)
  print(f"Proportion of energy captured by the first {r} singular values: {actual_ef:.3f}")
  return I_r

----------------------------------------

TITLE: Training Text Classification Model with TensorFlow
DESCRIPTION: This code compiles and trains the text classification model using binary cross-entropy loss and the Adam optimizer.

LANGUAGE: Python
CODE:
model.compile(loss=losses.BinaryCrossentropy(),
              optimizer='adam',
              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])

epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs)

----------------------------------------

TITLE: GPU Device Placement and Performance Testing
DESCRIPTION: Demonstrates explicit device placement for tensor operations and compares performance between CPU and GPU execution.

LANGUAGE: python
CODE:
import time

def time_matmul(x):
  start = time.time()
  for loop in range(10):
    tf.linalg.matmul(x, x)

  result = time.time()-start

  print("10 loops: {:0.2f}ms".format(1000*result))

# Force execution on CPU
print("On CPU:")
with tf.device("CPU:0"):
  x = tf.random.uniform([1000, 1000])
  assert x.device.endswith("CPU:0")
  time_matmul(x)

# Force execution on GPU #0 if available
if tf.config.list_physical_devices("GPU"):
  print("On GPU:")
  with tf.device("GPU:0"): 
    x = tf.random.uniform([1000, 1000])
    assert x.device.endswith("GPU:0")
    time_matmul(x)

----------------------------------------

TITLE: Building and training the transfer learning model
DESCRIPTION: Creates a new model using the pre-trained feature extractor and a new classification layer, then trains it on the flower dataset.

LANGUAGE: Python
CODE:
feature_extractor_model = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"

feature_extractor_layer = hub.KerasLayer(
    feature_extractor_model,
    input_shape=(224, 224, 3),
    trainable=False)

num_classes = len(class_names)

model = tf.keras.Sequential([
  feature_extractor_layer,
  tf.keras.layers.Dense(num_classes)
])

model.compile(
  optimizer=tf.keras.optimizers.Adam(),
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['acc'])

NUM_EPOCHS = 10

history = model.fit(train_ds,
                    validation_data=val_ds,
                    epochs=NUM_EPOCHS,
                    callbacks=tensorboard_callback)

----------------------------------------

TITLE: Training Loop Implementation
DESCRIPTION: Implements the main training loop for the CycleGAN model, including checkpoint saving and image generation.

LANGUAGE: Python
CODE:
for epoch in range(EPOCHS):
  start = time.time()

  n = 0
  for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):
    train_step(image_x, image_y)
    if n % 10 == 0:
      print ('.', end='')
    n += 1

  clear_output(wait=True)
  # Using a consistent image (sample_horse) so that the progress of the model
  # is clearly visible.
  generate_images(generator_g, sample_horse)

  if (epoch + 1) % 5 == 0:
    ckpt_save_path = ckpt_manager.save()
    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,
                                                         ckpt_save_path))

  print ('Time taken for epoch {} is {} sec\n'.format(epoch + 1,
                                                      time.time()-start))

----------------------------------------

TITLE: Installing Required Libraries for MIDI Processing and Audio Playback
DESCRIPTION: Installs fluidsynth, pyfluidsynth, and pretty_midi libraries needed for working with MIDI files and generating audio playback.

LANGUAGE: Shell
CODE:
!sudo apt install -y fluidsynth

LANGUAGE: Shell
CODE:
!pip install --upgrade pyfluidsynth

LANGUAGE: Shell
CODE:
!pip install pretty_midi

----------------------------------------

TITLE: Compiling and Training the Model
DESCRIPTION: Compiles the model with optimizer, loss function and metrics, then trains it on the training data.

LANGUAGE: python
CODE:
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=10)

----------------------------------------

TITLE: Creating Basic TensorFlow Model
DESCRIPTION: Implementing a simple sequential neural network model for the abalone dataset

LANGUAGE: Python
CODE:
abalone_model = tf.keras.Sequential([
  layers.Dense(64, activation='relu'),
  layers.Dense(1)
])

abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),
                      optimizer = tf.keras.optimizers.Adam())

----------------------------------------

TITLE: Define Training and Evaluation Steps
DESCRIPTION: Implements custom training and evaluation steps for DTensor-enabled model training

LANGUAGE: python
CODE:
@tf.function
def train_step(model, x, y, optimizer, metrics):
  with tf.GradientTape() as tape:
    logits = model(x, training=True)
    loss = tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(
        y, logits, from_logits=True))
    
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  for metric in metrics.values():
    metric.update_state(y_true=y, y_pred=logits)

  loss_per_sample = loss / len(x)
  results = {'loss': loss_per_sample}
  return results

----------------------------------------

TITLE: Downloading and Preprocessing Image Dataset
DESCRIPTION: Download cats and dogs dataset, create training and validation datasets with data augmentation

LANGUAGE: Python
CODE:
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'
path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

BATCH_SIZE = 32
IMG_SIZE = (160, 160)

train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,
                                                            shuffle=True,
                                                            batch_size=BATCH_SIZE,
                                                            image_size=IMG_SIZE)

----------------------------------------

TITLE: Implementing Dynamic RNN with tf.function
DESCRIPTION: Example of implementing a dynamic RNN using tf.function with Python control flow, showing how data-dependent control flow is handled.

LANGUAGE: python
CODE:
class DynamicRNN(tf.keras.Model):
  def __init__(self, rnn_cell):
    super(DynamicRNN, self).__init__(self)
    self.cell = rnn_cell

  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.float32, shape=[None, None, 3])])
  def call(self, input_data):
    input_data = tf.transpose(input_data, [1, 0, 2])
    timesteps =  tf.shape(input_data)[0]
    batch_size = tf.shape(input_data)[1]
    outputs = tf.TensorArray(tf.float32, timesteps)
    state = self.cell.get_initial_state(batch_size = batch_size, dtype=tf.float32)
    for i in tf.range(timesteps):
      output, state = self.cell(input_data[i], state)
      outputs = outputs.write(i, output)
    return tf.transpose(outputs.stack(), [1, 0, 2]), state

----------------------------------------

TITLE: Loading TensorFlow Model from .keras Format
DESCRIPTION: Shows how to load a TensorFlow model from a .keras file and evaluate its performance on test data.

LANGUAGE: python
CODE:
new_model = tf.keras.models.load_model('my_model.keras')

# Show the model architecture
new_model.summary()

# Evaluate the restored model
loss, acc = new_model.evaluate(test_images, test_labels, verbose=2)
print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))

print(new_model.predict(test_images).shape)

----------------------------------------

TITLE: Training Keras Model on TPU using Model.fit
DESCRIPTION: This code demonstrates how to compile and train a Keras model on TPU using the high-level Model.fit API.

LANGUAGE: Python
CODE:
with strategy.scope():
  model = create_model()
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['sparse_categorical_accuracy'])

batch_size = 200
steps_per_epoch = 60000 // batch_size
validation_steps = 10000 // batch_size

train_dataset = get_dataset(batch_size, is_training=True)
test_dataset = get_dataset(batch_size, is_training=False)

model.fit(train_dataset,
          epochs=5,
          steps_per_epoch=steps_per_epoch,
          validation_data=test_dataset,
          validation_steps=validation_steps)

----------------------------------------

TITLE: Implementing the Training Loop
DESCRIPTION: Defines and executes the training loop for the penguin classification model.

LANGUAGE: Python
CODE:
train_loss_results = []
train_accuracy_results = []

num_epochs = 201

for epoch in range(num_epochs):
  epoch_loss_avg = tf.keras.metrics.Mean()
  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

  for x, y in ds_train_batch:
    loss_value, grads = grad(model, x, y)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    epoch_loss_avg.update_state(loss_value)
    epoch_accuracy.update_state(y, model(x, training=True))

  train_loss_results.append(epoch_loss_avg.result())
  train_accuracy_results.append(epoch_accuracy.result())

  if epoch % 50 == 0:
    print("Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch,
                                                                epoch_loss_avg.result(),
                                                                epoch_accuracy.result()))

----------------------------------------

TITLE: Converting Keras Model to Estimator
DESCRIPTION: Transforms the Keras model into a TensorFlow Estimator for training

LANGUAGE: python
CODE:
import tempfile
model_dir = tempfile.mkdtemp()
keras_estimator = tf.keras.estimator.model_to_estimator(
    keras_model=model, model_dir=model_dir)

----------------------------------------

TITLE: Selecting and Loading an Image Classification Model
DESCRIPTION: Defines a dictionary of available models, selects a model based on user input, and loads it using TensorFlow Hub.

LANGUAGE: Python
CODE:
model_name = "efficientnetv2-s" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', ...]

model_handle_map = {
  "efficientnetv2-s": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2",
  # ... other model mappings ...
}

model_handle = model_handle_map[model_name]

print(f"Selected model: {model_name} : {model_handle}")

# Load the model
classifier = hub.load(model_handle)

# Warmup run
input_shape = image.shape
warmup_input = tf.random.uniform(input_shape, 0, 1.0)
warmup_logits = classifier(warmup_input).numpy()

----------------------------------------

TITLE: Defining CycleGAN Model Architecture
DESCRIPTION: Creates generator and discriminator models for the CycleGAN architecture using modified U-Net and PatchGAN structures.

LANGUAGE: Python
CODE:
OUTPUT_CHANNELS = 3

generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')
generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')

discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)
discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)

----------------------------------------

TITLE: Keras Core Model Methods
DESCRIPTION: Essential built-in methods available in tf.keras.Model class for training and evaluation of machine learning models.

LANGUAGE: python
CODE:
tf.keras.Model.fit        # Trains the model for a fixed number of epochs
tf.keras.Model.predict    # Generates output predictions for input samples
tf.keras.Model.evaluate   # Returns loss and metrics values for the model

----------------------------------------

TITLE: Creating a Dense Layer Module
DESCRIPTION: Defines a custom dense (linear) layer as a TensorFlow module with weight and bias variables.

LANGUAGE: Python
CODE:
class Dense(tf.Module):
  def __init__(self, in_features, out_features, name=None):
    super().__init__(name=name)
    self.w = tf.Variable(
      tf.random.normal([in_features, out_features]), name='w')
    self.b = tf.Variable(tf.zeros([out_features]), name='b')
  def __call__(self, x):
    y = tf.matmul(x, self.w) + self.b
    return tf.nn.relu(y)

----------------------------------------

TITLE: Loading and Previewing the Penguins Dataset
DESCRIPTION: Loads a simplified version of the penguins dataset and displays a preview of the first 5 records.

LANGUAGE: Python
CODE:
ds_preview, info = tfds.load('penguins/simple', split='train', with_info=True)
df = tfds.as_dataframe(ds_preview.take(5), info)
print(df)
print(info.features)

----------------------------------------

TITLE: Importing required libraries for multi-worker training in TensorFlow
DESCRIPTION: Imports necessary Python libraries including json, os, sys, and TensorFlow for implementing multi-worker distributed training.

LANGUAGE: Python
CODE:
import json
import os
import sys
import tensorflow as tf

----------------------------------------

TITLE: Creating and training deep neural network model with TensorFlow
DESCRIPTION: Builds a deep neural network model using TensorFlow's Keras API, compiles it, and trains it on the dataset.

LANGUAGE: Python
CODE:
def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model

dnn_model = build_and_compile_model(normalizer)

history = dnn_model.fit(
    train_features,
    train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)

----------------------------------------

TITLE: Adding Dense Layers to CNN
DESCRIPTION: This snippet adds dense layers on top of the convolutional base for classification.

LANGUAGE: Python
CODE:
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))

----------------------------------------

TITLE: Creating MirroredStrategy and Setting Up Input Pipeline
DESCRIPTION: Create a MirroredStrategy instance and set up the input pipeline with distributed datasets.

LANGUAGE: Python
CODE:
strategy = tf.distribute.MirroredStrategy()

BUFFER_SIZE = len(train_images)

BATCH_SIZE_PER_REPLICA = 64
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

EPOCHS = 10

train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)
test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)

train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)

----------------------------------------

TITLE: Building a Model with Keras Functional API
DESCRIPTION: Demonstrates how to build a model using the Keras Functional API, which allows for more complex model architectures.

LANGUAGE: Python
CODE:
inputs = tf.keras.Input(shape=(32,))  # Returns a placeholder tensor

# A layer instance is callable on a tensor, and returns a tensor.
x = layers.Dense(64, activation='relu')(inputs)
x = layers.Dense(64, activation='relu')(x)
predictions = layers.Dense(10, activation='softmax')(x)

model = tf.keras.Model(inputs=inputs, outputs=predictions)

# The compile step specifies the training configuration.
model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Trains for 5 epochs
model.fit(data, labels, batch_size=32, epochs=5)

----------------------------------------

TITLE: Building and Training Transfer Learning Model
DESCRIPTION: Adds classification layers on top of the base model, freezes the base layers, compiles the model, and trains it on the dataset.

LANGUAGE: Python
CODE:
base_model.trainable = False

model = tf.keras.Sequential([
  base_model,
  keras.layers.GlobalAveragePooling2D(),
  keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

epochs = 10
steps_per_epoch = train_generator.n // batch_size
validation_steps = validation_generator.n // batch_size

history = model.fit_generator(train_generator,
                              steps_per_epoch = steps_per_epoch,
                              epochs=epochs,
                              workers=4,
                              validation_data=validation_generator,
                              validation_steps=validation_steps)

----------------------------------------

TITLE: Importing Required Libraries for Time Series Forecasting
DESCRIPTION: Imports necessary Python libraries including TensorFlow, pandas, matplotlib, and numpy for data manipulation, visualization, and model building.

LANGUAGE: Python
CODE:
import os
import datetime

import IPython
import IPython.display
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf

mpl.rcParams['figure.figsize'] = (8, 6)
mpl.rcParams['axes.grid'] = False

----------------------------------------

TITLE: Implementing Integrated Gradients Core Function
DESCRIPTION: Main implementation of the Integrated Gradients algorithm that computes attribution scores for model predictions. Handles interpolation, gradient computation and integral approximation.

LANGUAGE: Python
CODE:
def integrated_gradients(baseline,
                         image,
                         target_class_idx,
                         m_steps=50,
                         batch_size=32):
  alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1)
  gradient_batches = []
  for alpha in tf.range(0, len(alphas), batch_size):
    from_ = alpha
    to = tf.minimum(from_ + batch_size, len(alphas))
    alpha_batch = alphas[from_:to]
    gradient_batch = one_batch(baseline, image, alpha_batch, target_class_idx)
    gradient_batches.append(gradient_batch)
  total_gradients = tf.concat(gradient_batches, axis=0)
  avg_gradients = integral_approximation(gradients=total_gradients)
  integrated_gradients = (image - baseline) * avg_gradients
  return integrated_gradients

----------------------------------------

TITLE: Implementing Training Step Function for Distributed Training
DESCRIPTION: Defines a training step function that handles model forward pass, loss calculation, and gradient updates across distributed workers.

LANGUAGE: Python
CODE:
@tf.function
def step_fn(iterator):
  def replica_fn(batch_data, labels):
    with tf.GradientTape() as tape:
      pred = model(batch_data, training=True)
      per_example_loss = tf.keras.losses.BinaryCrossentropy(
          reduction=tf.keras.losses.Reduction.NONE)(labels, pred)
      loss = tf.nn.compute_average_loss(per_example_loss)
      model_losses = model.losses
      if model_losses:
        loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))
    gradients = tape.gradient(loss, model.trainable_variables)

    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)
    accuracy.update_state(labels, actual_pred)
    return loss

  batch_data, labels = next(iterator)
  losses = strategy.run(replica_fn, args=(batch_data, labels))
  return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)

----------------------------------------

TITLE: Setting Mixed Precision Policy in TensorFlow
DESCRIPTION: Demonstrates how to enable mixed precision training by setting the global dtype policy to mixed_float16.

LANGUAGE: Python
CODE:
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

----------------------------------------

TITLE: Evaluating and comparing model performance in Python
DESCRIPTION: Evaluates the performance of different models on the test set and compares their mean absolute errors.

LANGUAGE: Python
CODE:
test_results = {}
test_results['linear_model'] = linear_model.evaluate(test_features, test_labels, verbose=0)
test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)

pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T

----------------------------------------

TITLE: Creating Basic Neural Network Model
DESCRIPTION: Defines a basic sequential neural network model with normalization and dense layers.

LANGUAGE: Python
CODE:
def get_basic_model():
  model = tf.keras.Sequential([
    normalizer,
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1)
  ])

  model.compile(optimizer='adam',
                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                metrics=['accuracy'])
  return model

----------------------------------------

TITLE: NumPy and TensorFlow Interoperability
DESCRIPTION: Shows the conversion between NumPy arrays and TensorFlow tensors, demonstrating automatic conversions and explicit numpy() method usage.

LANGUAGE: python
CODE:
import numpy as np

ndarray = np.ones([3, 3])

print("TensorFlow operations convert numpy arrays to Tensors automatically")
tensor = tf.math.multiply(ndarray, 42)
print(tensor)

print("And NumPy operations convert Tensors to NumPy arrays automatically")
print(np.add(tensor, 1))

print("The .numpy() method explicitly converts a Tensor to a numpy array")
print(tensor.numpy())

----------------------------------------

TITLE: Implementing Momentum Optimizer in TensorFlow
DESCRIPTION: Implements a gradient descent optimizer with momentum, which uses both the current gradient and previous update to determine the next step. Includes configurable learning rate and momentum parameters.

LANGUAGE: Python
CODE:
class Momentum(tf.Module):

  def __init__(self, learning_rate=1e-3, momentum=0.7):
    # Initialize parameters
    self.learning_rate = learning_rate
    self.momentum = momentum
    self.change = 0.
    self.title = f"Gradient descent optimizer: learning rate={self.learning_rate}"

  def apply_gradients(self, grads, vars):
    # Update variables 
    for grad, var in zip(grads, vars):
      curr_change = self.learning_rate*grad + self.momentum*self.change
      var.assign_sub(curr_change)
      self.change = curr_change

----------------------------------------

TITLE: Loading and preprocessing the flower dataset
DESCRIPTION: Loads the TensorFlow flowers dataset, splits it into training and validation sets, and applies normalization.

LANGUAGE: Python
CODE:
batch_size = 32
img_height = 224
img_width = 224

train_ds = tf.keras.utils.image_dataset_from_directory(
  str(data_root),
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size
)

val_ds = tf.keras.utils.image_dataset_from_directory(
  str(data_root),
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size
)

normalization_layer = tf.keras.layers.Rescaling(1./255)
train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))

----------------------------------------

TITLE: Computing Batch Jacobian in TensorFlow
DESCRIPTION: Demonstrate how to efficiently compute Jacobians for batches of inputs using tf.GradientTape.batch_jacobian method.

LANGUAGE: Python
CODE:
x = tf.random.normal([7, 5])

layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)
layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)

with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:
  tape.watch(x)
  y = layer1(x)
  y = layer2(y)

jb = tape.batch_jacobian(y, x)
print(f'jb.shape: {jb.shape}')

----------------------------------------

TITLE: Implementing Dense Layer and MLP Model
DESCRIPTION: Defines custom DenseLayer and MLP classes using TensorFlow Core APIs to build the neural network model.

LANGUAGE: Python
CODE:
class DenseLayer(tf.Module):

  def __init__(self, out_dim, weight_init=xavier_init, activation=tf.identity):
    # Initialize the dimensions and activation functions
    self.out_dim = out_dim
    self.weight_init = weight_init
    self.activation = activation
    self.built = False

  def __call__(self, x):
    if not self.built:
      # Infer the input dimension based on first call
      self.in_dim = x.shape[1]
      # Initialize the weights and biases
      self.w = tf.Variable(self.weight_init(shape=(self.in_dim, self.out_dim)))
      self.b = tf.Variable(tf.zeros(shape=(self.out_dim,)))
      self.built = True
    # Compute the forward pass
    z = tf.add(tf.matmul(x, self.w), self.b)
    return self.activation(z)

class MLP(tf.Module):

  def __init__(self, layers):
    self.layers = layers
   
  @tf.function
  def __call__(self, x, preds=False): 
    # Execute the model's layers sequentially
    for layer in self.layers:
      x = layer(x)
    return x

----------------------------------------

TITLE: Creating a Sequential Module
DESCRIPTION: Defines a sequential model using two dense layers as a TensorFlow module.

LANGUAGE: Python
CODE:
class SequentialModule(tf.Module):
  def __init__(self, name=None):
    super().__init__(name=name)

    self.dense_1 = Dense(in_features=3, out_features=3)
    self.dense_2 = Dense(in_features=3, out_features=2)

  def __call__(self, x):
    x = self.dense_1(x)
    return self.dense_2(x)

# You have made a model!
my_model = SequentialModule(name="the_model")

# Call it, with random results
print("Model results:", my_model(tf.constant([[2.0, 2.0, 2.0]])))

----------------------------------------

TITLE: Compiling Model with Optimizer and Loss Function
DESCRIPTION: Configure model training parameters including optimizer, loss function and metrics.

LANGUAGE: python
CODE:
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])

----------------------------------------

TITLE: Creating and Operating on Tensors in TensorFlow
DESCRIPTION: Demonstrates basic tensor creation and operations including shape access, arithmetic operations, and tensor manipulation functions.

LANGUAGE: python
CODE:
import tensorflow as tf

x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.]])

print(x)
print(x.shape)
print(x.dtype)

----------------------------------------

TITLE: Dense Layer Implementation with tf.function
DESCRIPTION: Shows how to compose tf.functions by implementing a dense neural network layer

LANGUAGE: Python
CODE:
@tf.function
def dense_layer(x, w, b):
  return add(tf.matmul(x, w), b)

dense_layer(tf.ones([3, 2]), tf.ones([2, 2]), tf.ones([2]))

----------------------------------------

TITLE: Working with Multiple Gradient Tapes in TensorFlow
DESCRIPTION: Show how to use multiple gradient tapes to compute gradients for different sets of variables independently.

LANGUAGE: Python
CODE:
x0 = tf.constant(0.0)
x1 = tf.constant(0.0)

with tf.GradientTape() as tape0, tf.GradientTape() as tape1:
  tape0.watch(x0)
  tape1.watch(x1)

  y0 = tf.math.sin(x0)
  y1 = tf.nn.sigmoid(x1)

  y = y0 + y1

  ys = tf.reduce_sum(y)

tape0.gradient(ys, x0).numpy()   # cos(x) => 1.0
tape1.gradient(ys, x1).numpy()   # sigmoid(x1)*(1-sigmoid(x1)) => 0.25

----------------------------------------

TITLE: Training Loop Implementation
DESCRIPTION: Training loop with gradient descent optimization using TensorFlow GradientTape

LANGUAGE: Python
CODE:
epochs = 200
learning_rate = 0.01
train_losses, test_losses = [], []
train_accs, test_accs = [], []

for epoch in range(epochs):
  batch_losses_train, batch_accs_train = [], []
  batch_losses_test, batch_accs_test = [], []

  for x_batch, y_batch in train_dataset:
    with tf.GradientTape() as tape:
      y_pred_batch = log_reg(x_batch)
      batch_loss = log_loss(y_pred_batch, y_batch)
    batch_acc = accuracy(y_pred_batch, y_batch)
    grads = tape.gradient(batch_loss, log_reg.variables)
    for g,v in zip(grads, log_reg.variables):
      v.assign_sub(learning_rate * g)
    batch_losses_train.append(batch_loss)
    batch_accs_train.append(batch_acc)

----------------------------------------

TITLE: Exporting Model with Preprocessing
DESCRIPTION: This code defines a custom TensorFlow module that includes preprocessing steps, allowing for end-to-end inference on raw audio files.

LANGUAGE: Python
CODE:
class ExportModel(tf.Module):
  def __init__(self, model):
    self.model = model

    self.__call__.get_concrete_function(
        x=tf.TensorSpec(shape=(), dtype=tf.string))
    self.__call__.get_concrete_function(
       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))

  @tf.function
  def __call__(self, x):
    if x.dtype == tf.string:
      x = tf.io.read_file(x)
      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)
      x = tf.squeeze(x, axis=-1)
      x = x[tf.newaxis, :]
    
    x = get_spectrogram(x)  
    result = self.model(x, training=False)
    
    class_ids = tf.argmax(result, axis=-1)
    class_names = tf.gather(label_names, class_ids)
    return {'predictions':result,
            'class_ids': class_ids,
            'class_names': class_names}

export = ExportModel(model)
tf.saved_model.save(export, "saved")
imported = tf.saved_model.load("saved")

----------------------------------------

TITLE: Fine-tuning the Model
DESCRIPTION: Unfreezes the top layers of the base model, recompiles with a lower learning rate, and continues training to improve performance.

LANGUAGE: Python
CODE:
base_model.trainable = True

# Fine tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False

model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=2e-5),
              loss='binary_crossentropy',
              metrics=['accuracy'])

history_fine = model.fit_generator(train_generator,
                                   steps_per_epoch = steps_per_epoch,
                                   epochs=epochs,
                                   workers=4,
                                   validation_data=validation_generator,
                                   validation_steps=validation_steps)

----------------------------------------

TITLE: Demonstrating TensorFlow 1.x Tensor Equality Behavior
DESCRIPTION: Shows how tensor equality checks work in TensorFlow 1.x, where variables with the same value return false when compared using the == operator.

LANGUAGE: Python
CODE:
tf.compat.v1.disable_tensor_equality()
x = tf.Variable(0.0)
y = tf.Variable(0.0)

x == y

----------------------------------------

TITLE: Evaluating 3D CNN Model Performance
DESCRIPTION: Evaluates the trained 3D CNN model on the test dataset and visualizes the results using confusion matrices and classification metrics.

LANGUAGE: Python
CODE:
model.evaluate(test_ds, return_dict=True)

actual, predicted = get_actual_predicted_labels(test_ds)
plot_confusion_matrix(actual, predicted, labels, 'test')

precision, recall = calculate_classification_metrics(actual, predicted, labels) # Test dataset

----------------------------------------

TITLE: Creating a Flexible Dense Module
DESCRIPTION: Defines a flexible dense layer that defers variable creation until the first call, allowing for dynamic input shapes.

LANGUAGE: Python
CODE:
class FlexibleDenseModule(tf.Module):
  # Note: No need for `in_features`
  def __init__(self, out_features, name=None):
    super().__init__(name=name)
    self.is_built = False
    self.out_features = out_features

  def __call__(self, x):
    # Create variables on first call.
    if not self.is_built:
      self.w = tf.Variable(
        tf.random.normal([x.shape[-1], self.out_features]), name='w')
      self.b = tf.Variable(tf.zeros([self.out_features]), name='b')
      self.is_built = True

    y = tf.matmul(x, self.w) + self.b
    return tf.nn.relu(y)

----------------------------------------

TITLE: Setting up imports for TensorFlow image classification
DESCRIPTION: Import necessary libraries including TensorFlow, TensorFlow Hub, NumPy, Matplotlib, and others for image classification tasks.

LANGUAGE: Python
CODE:
import collections
import io
import math
import os
import random
from six.moves import urllib

from IPython.display import clear_output, Image, display, HTML

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

import tensorflow_hub as hub

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.metrics as sk_metrics
import time

----------------------------------------

TITLE: Building Text Classification Model with TensorFlow Keras
DESCRIPTION: This code defines a sequential neural network model for text classification using TensorFlow Keras. It includes an embedding layer, dropout layers, and dense layers.

LANGUAGE: Python
CODE:
embedding_dim = 16

model = tf.keras.Sequential([
  layers.Embedding(max_features, embedding_dim),
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(1, activation='sigmoid')])

model.summary()

----------------------------------------

TITLE: Creating TPUStrategy for Distributed Training in TensorFlow
DESCRIPTION: This code creates a TPUStrategy object for distributed training across multiple TPU cores.

LANGUAGE: Python
CODE:
strategy = tf.distribute.TPUStrategy(resolver)

----------------------------------------

TITLE: Creating TensorFlow Datasets from Pandas DataFrame
DESCRIPTION: Defines a utility function to convert pandas DataFrames into TensorFlow Dataset objects for model training.

LANGUAGE: Python
CODE:
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

----------------------------------------

TITLE: Implementing Training and Testing Steps
DESCRIPTION: Implement the training and testing steps for a single batch of data.

LANGUAGE: Python
CODE:
def train_step(inputs):
  images, labels = inputs

  with tf.GradientTape() as tape:
    predictions = model(images, training=True)
    loss = compute_loss(labels, predictions, model.losses)

  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_accuracy.update_state(labels, predictions)
  return loss

def test_step(inputs):
  images, labels = inputs

  predictions = model(images, training=False)
  t_loss = loss_object(labels, predictions)

  test_loss.update_state(t_loss)
  test_accuracy.update_state(labels, predictions)

----------------------------------------

TITLE: Text-to-Video Retrieval Implementation
DESCRIPTION: Implements the core text-to-video retrieval functionality by generating embeddings and computing similarity scores between videos and text queries.

LANGUAGE: python
CODE:
videos_np = np.stack(all_videos, axis=0)
words_np = np.array(all_queries_video)
video_embd, text_embd = generate_embeddings(hub_model, videos_np, words_np)
all_scores = np.dot(text_embd, tf.transpose(video_embd))

----------------------------------------

TITLE: Implementing Custom Training Loop
DESCRIPTION: Defines functions for training steps, validation steps, and the overall training loop for the MLP model.

LANGUAGE: Python
CODE:
def train_step(x_batch, y_batch, loss, acc, model, optimizer):
  # Update the model state given a batch of data
  with tf.GradientTape() as tape:
    y_pred = model(x_batch)
    batch_loss = loss(y_pred, y_batch)
  batch_acc = acc(y_pred, y_batch)
  grads = tape.gradient(batch_loss, model.variables)
  optimizer.apply_gradients(grads, model.variables)
  return batch_loss, batch_acc

def val_step(x_batch, y_batch, loss, acc, model):
  # Evaluate the model on given a batch of validation data
  y_pred = model(x_batch)
  batch_loss = loss(y_pred, y_batch)
  batch_acc = acc(y_pred, y_batch)
  return batch_loss, batch_acc

def train_model(mlp, train_data, val_data, loss, acc, optimizer, epochs):
  # Initialize data structures
  train_losses, train_accs = [], []
  val_losses, val_accs = [], []

  # Format training loop and begin training
  for epoch in range(epochs):
    batch_losses_train, batch_accs_train = [], []
    batch_losses_val, batch_accs_val = [], []

    # Iterate over the training data
    for x_batch, y_batch in train_data:
      # Compute gradients and update the model's parameters
      batch_loss, batch_acc = train_step(x_batch, y_batch, loss, acc, mlp, optimizer)
      # Keep track of batch-level training performance
      batch_losses_train.append(batch_loss)
      batch_accs_train.append(batch_acc)

    # Iterate over the validation data
    for x_batch, y_batch in val_data:
      batch_loss, batch_acc = val_step(x_batch, y_batch, loss, acc, mlp)
      batch_losses_val.append(batch_loss)
      batch_accs_val.append(batch_acc)

    # Keep track of epoch-level model performance
    train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)
    val_loss, val_acc = tf.reduce_mean(batch_losses_val), tf.reduce_mean(batch_accs_val)
    train_losses.append(train_loss)
    train_accs.append(train_acc)
    val_losses.append(val_loss)
    val_accs.append(val_acc)
    print(f"Epoch: {epoch}")
    print(f"Training loss: {train_loss:.3f}, Training accuracy: {train_acc:.3f}")
    print(f"Validation loss: {val_loss:.3f}, Validation accuracy: {val_acc:.3f}")
  return train_losses, train_accs, val_losses, val_accs

----------------------------------------

TITLE: Loading and Preprocessing Auto MPG Dataset
DESCRIPTION: Loads the Auto MPG dataset from UCI repository, handles missing values, and converts to TensorFlow tensor format.

LANGUAGE: python
CODE:
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',
                'Acceleration', 'Model Year', 'Origin']

dataset = pd.read_csv(url, names=column_names, na_values='?', comment='\t',
                          sep=' ', skipinitialspace=True)

dataset = dataset.dropna()
dataset_tf = tf.convert_to_tensor(dataset, dtype=tf.float32)
dataset.tail()

----------------------------------------

TITLE: Using tf.stop_gradient for Precise Gradient Control
DESCRIPTION: Demonstrate how to stop gradients from flowing along a particular path using tf.stop_gradient function.

LANGUAGE: Python
CODE:
x = tf.Variable(2.0)
y = tf.Variable(3.0)

with tf.GradientTape() as t:
  y_sq = y**2
  z = x**2 + tf.stop_gradient(y_sq)

grad = t.gradient(z, {'x': x, 'y': y})

print('dz/dx:', grad['x'])  # 2*x => 4
print('dz/dy:', grad['y'])

----------------------------------------

TITLE: Reading Image Data from TFRecord in TensorFlow
DESCRIPTION: Demonstrates how to read image data from a TFRecord file, including parsing the tf.train.Example messages and recovering the images.

LANGUAGE: Python
CODE:
raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')

image_feature_description = {
    'height': tf.io.FixedLenFeature([], tf.int64),
    'width': tf.io.FixedLenFeature([], tf.int64),
    'depth': tf.io.FixedLenFeature([], tf.int64),
    'label': tf.io.FixedLenFeature([], tf.int64),
    'image_raw': tf.io.FixedLenFeature([], tf.string),
}

def _parse_image_function(example_proto):
  return tf.io.parse_single_example(example_proto, image_feature_description)

parsed_image_dataset = raw_image_dataset.map(_parse_image_function)

for image_features in parsed_image_dataset:
  image_raw = image_features['image_raw'].numpy()
  display.display(display.Image(data=image_raw))

----------------------------------------

TITLE: TensorFlow Data Pipeline with Prefetch
DESCRIPTION: Shows how to add prefetching to a tf.data pipeline for better performance by overlapping data preprocessing with model training.

LANGUAGE: python
CODE:
dataset = tf.data.Dataset.range(10)
dataset = dataset.map(lambda x: x)
dataset = dataset.repeat(2)
dataset = dataset.batch(5)
dataset = dataset.prefetch(1)

----------------------------------------

TITLE: Placing TensorFlow Variables and Tensors on Specific Devices
DESCRIPTION: This snippet shows how to explicitly place TensorFlow variables and tensors on specific devices (CPU or GPU) for computation, which can be useful for performance optimization.

LANGUAGE: Python
CODE:
with tf.device('CPU:0'):

  # Create some tensors
  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
  c = tf.matmul(a, b)

print(c)

----------------------------------------

TITLE: Initializing MultiWorkerMirroredStrategy for distributed training
DESCRIPTION: Creates an instance of tf.distribute.MultiWorkerMirroredStrategy to enable distributed training across multiple workers.

LANGUAGE: Python
CODE:
strategy = tf.distribute.MultiWorkerMirroredStrategy()

----------------------------------------

TITLE: Checking Available GPUs in TensorFlow
DESCRIPTION: Basic setup code to import TensorFlow and check the number of available GPUs in the system.

LANGUAGE: python
CODE:
import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

----------------------------------------

TITLE: Reshaping Tensors in TensorFlow
DESCRIPTION: Shows how to reshape tensors using tf.reshape and explains when reshaping is appropriate.

LANGUAGE: Python
CODE:
# You can reshape a tensor to a new shape.
# Note that you're passing in a list
reshaped = tf.reshape(x, [1, 3])

print(x.shape)
print(reshaped.shape)

print(tf.reshape(rank_3_tensor, [-1]))

print(tf.reshape(rank_3_tensor, [3*2, 5]), "\n")
print(tf.reshape(rank_3_tensor, [3, -1]))

----------------------------------------

TITLE: Applying Various Operations to Tensors in TensorFlow
DESCRIPTION: Demonstrates operations like finding the maximum value, argmax, and softmax on a tensor.

LANGUAGE: Python
CODE:
c = tf.constant([[4.0, 5.0], [10.0, 1.0]])

# Find the largest value
print(tf.reduce_max(c))
# Find the index of the largest value
print(tf.math.argmax(c))
# Compute the softmax
print(tf.nn.softmax(c))

----------------------------------------

TITLE: Running Object Detection Inference
DESCRIPTION: Performs object detection inference on the input image using the loaded TensorFlow Hub model.

LANGUAGE: python
CODE:
# running inference
results = hub_model(image_np)

# different object detection models have additional results
# all of them are explained in the documentation
result = {key:value.numpy() for key,value in results.items()}
print(result.keys())

----------------------------------------

TITLE: Training and Evaluating the Final Model
DESCRIPTION: Train the model with the best hyperparameters and evaluate it on the test dataset.

LANGUAGE: python
CODE:
# Build the model with the optimal hyperparameters and train it on the data for 50 epochs
model = tuner.hypermodel.build(best_hps)
history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)

val_acc_per_epoch = history.history['val_accuracy']
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
print('Best epoch: %d' % (best_epoch,))

hypermodel = tuner.hypermodel.build(best_hps)

# Retrain the model
hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)

eval_result = hypermodel.evaluate(img_test, label_test)
print("[test loss, test accuracy]:", eval_result)

----------------------------------------

TITLE: Generating Next Note Prediction
DESCRIPTION: Implements a function to predict the next note (pitch, step, duration) using the trained model, with temperature-based sampling for pitch.

LANGUAGE: Python
CODE:
def predict_next_note(
    notes: np.ndarray, 
    model: tf.keras.Model, 
    temperature: float = 1.0) -> tuple[int, float, float]:
  """Generates a note as a tuple of (pitch, step, duration), using a trained sequence model."""

  assert temperature > 0

  # Add batch dimension
  inputs = tf.expand_dims(notes, 0)

  predictions = model.predict(inputs)
  pitch_logits = predictions['pitch']
  step = predictions['step']
  duration = predictions['duration']
 
  pitch_logits /= temperature
  pitch = tf.random.categorical(pitch_logits, num_samples=1)
  pitch = tf.squeeze(pitch, axis=-1)
  duration = tf.squeeze(duration, axis=-1)
  step = tf.squeeze(step, axis=-1)

  # `step` and `duration` values should be non-negative
  step = tf.maximum(0, step)
  duration = tf.maximum(0, duration)

  return int(pitch), float(step), float(duration)

----------------------------------------

TITLE: Defining Model Creation Function
DESCRIPTION: Define a function to create a convolutional neural network model using Keras Sequential API.

LANGUAGE: Python
CODE:
def create_model():
  regularizer = tf.keras.regularizers.L2(1e-5)
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3,
                             activation='relu',
                             kernel_regularizer=regularizer),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Conv2D(64, 3,
                             activation='relu',
                             kernel_regularizer=regularizer),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64,
                            activation='relu',
                            kernel_regularizer=regularizer),
      tf.keras.layers.Dense(10, kernel_regularizer=regularizer)
    ])

  return model

----------------------------------------

TITLE: Importing Required Libraries for Text Processing
DESCRIPTION: Imports necessary Python libraries and TensorFlow modules for text processing and machine learning tasks.

LANGUAGE: Python
CODE:
import collections
import pathlib

import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import utils
from tensorflow.keras.layers import TextVectorization

import tensorflow_datasets as tfds
import tensorflow_text as tf_text

----------------------------------------

TITLE: Loading and Preprocessing Fashion MNIST Dataset
DESCRIPTION: Load the Fashion MNIST dataset, add a dimension to the image arrays, and scale the pixel values to the [0, 1] range.

LANGUAGE: Python
CODE:
fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Add a dimension to the array -> new shape == (28, 28, 1)
# This is done because the first layer in our model is a convolutional
# layer and it requires a 4D input (batch_size, height, width, channels).
# batch_size dimension will be added later on.
train_images = train_images[..., None]
test_images = test_images[..., None]

# Scale the images to the [0, 1] range.
train_images = train_images / np.float32(255)
test_images = test_images / np.float32(255)

----------------------------------------

TITLE: Implementing a Custom Dense Layer in TensorFlow
DESCRIPTION: This snippet shows how to implement a custom Dense layer by extending tf.keras.Layer.

LANGUAGE: Python
CODE:
class MyDenseLayer(tf.keras.layers.Layer):
  def __init__(self, num_outputs):
    super(MyDenseLayer, self).__init__()
    self.num_outputs = num_outputs

  def build(self, input_shape):
    self.kernel = self.add_weight("kernel",
                                  shape=[int(input_shape[-1]),
                                         self.num_outputs])

  def call(self, inputs):
    return tf.matmul(inputs, self.kernel)

layer = MyDenseLayer(10)
_ = layer(tf.zeros([10, 5])) # Calling the layer `.builds` it.

print([var.name for var in layer.trainable_variables])

----------------------------------------

TITLE: Defining Loss Function and Metrics
DESCRIPTION: Define the loss function and metrics for tracking training and testing performance.

LANGUAGE: Python
CODE:
with strategy.scope():
  # Set reduction to `NONE` so you can do the reduction yourself.
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True,
      reduction=tf.keras.losses.Reduction.NONE)
  def compute_loss(labels, predictions, model_losses):
    per_example_loss = loss_object(labels, predictions)
    loss = tf.nn.compute_average_loss(per_example_loss)
    if model_losses:
      loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))
    return loss

  test_loss = tf.keras.metrics.Mean(name='test_loss')

  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
      name='train_accuracy')
  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
      name='test_accuracy')

----------------------------------------

TITLE: Exporting Text Classification Model for Inference
DESCRIPTION: This code creates an export model that includes the text vectorization layer, allowing it to process raw strings for inference.

LANGUAGE: Python
CODE:
export_model = tf.keras.Sequential([
  vectorize_layer,
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.BinaryCrossentropy(from_logits=False), optimizer="adam", metrics=['accuracy']
)

# Test it with `raw_test_ds`, which yields raw strings
metrics = export_model.evaluate(raw_test_ds, return_dict=True)
print(metrics)

----------------------------------------

TITLE: Training the Model
DESCRIPTION: Compile and train the RNN model on the prepared dataset.

LANGUAGE: Python
CODE:
model.compile(
    optimizer = tf.train.AdamOptimizer(),
    loss = loss)

history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])

----------------------------------------

TITLE: Importing TensorFlow and Preparing Data
DESCRIPTION: Import TensorFlow libraries and set up sample data for demonstration purposes.

LANGUAGE: Python
CODE:
import tensorflow as tf
import tensorflow.compat.v1 as tf1

features = [[1., 1.5]]
labels = [[0.3]]
eval_features = [[4., 4.5]]
eval_labels = [[0.8]]

----------------------------------------

TITLE: Defining and Training the Image Classification Model
DESCRIPTION: Creates a Keras model using the selected TensorFlow Hub feature extractor, compiles it, and trains it on the Flowers dataset.

LANGUAGE: Python
CODE:
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),
    hub.KerasLayer(model_handle, trainable=do_fine_tuning),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(len(class_names),
                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))
])
model.build((None,)+IMAGE_SIZE+(3,))
model.summary()

model.compile(
  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), 
  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),
  metrics=['accuracy'])

hist = model.fit(
    train_ds,
    epochs=5, steps_per_epoch=steps_per_epoch,
    validation_data=val_ds,
    validation_steps=validation_steps).history

----------------------------------------

TITLE: Creating Training Sequences from Note Data
DESCRIPTION: Defines a function to create training sequences from the extracted note data, including normalization and label splitting.

LANGUAGE: Python
CODE:
def create_sequences(
    dataset: tf.data.Dataset, 
    seq_length: int,
    vocab_size = 128,
) -> tf.data.Dataset:
  """Returns TF Dataset of sequence and label examples."""
  seq_length = seq_length+1

  # Take 1 extra for the labels
  windows = dataset.window(seq_length, shift=1, stride=1,
                              drop_remainder=True)

  # `flat_map` flattens the" dataset of datasets" into a dataset of tensors
  flatten = lambda x: x.batch(seq_length, drop_remainder=True)
  sequences = windows.flat_map(flatten)
  
  # Normalize note pitch
  def scale_pitch(x):
    x = x/[vocab_size,1.0,1.0]
    return x

  # Split the labels
  def split_labels(sequences):
    inputs = sequences[:-1]
    labels_dense = sequences[-1]
    labels = {key:labels_dense[i] for i,key in enumerate(key_order)}

    return scale_pitch(inputs), labels

  return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)

----------------------------------------

TITLE: Accessing TensorFlow Variable Properties and Converting to NumPy
DESCRIPTION: This snippet shows how to access properties of a TensorFlow variable, such as shape and data type, and how to convert the variable to a NumPy array.

LANGUAGE: Python
CODE:
print("Shape: ", my_variable.shape)
print("DType: ", my_variable.dtype)
print("As NumPy: ", my_variable.numpy())

----------------------------------------

TITLE: Iterating Over Dataset with LSTM State in TensorFlow
DESCRIPTION: Example of how to iterate over an entire dataset, maintaining the LSTM state between batches and accumulating the total loss.

LANGUAGE: python
CODE:
# A numpy array holding the state of LSTM after each batch of words.
numpy_state = initial_state.eval()
total_loss = 0.0
for current_batch_of_words in words_in_dataset:
    numpy_state, current_loss = session.run([final_state, loss],
        # Initialize the LSTM state from the previous iteration.
        feed_dict={initial_state: numpy_state, words: current_batch_of_words})
    total_loss += current_loss

----------------------------------------

TITLE: TensorFlow 2 Keras Multiple GPU Implementation
DESCRIPTION: Implementation of multi-GPU training using Keras with MirroredStrategy in TensorFlow 2

LANGUAGE: python
CODE:
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
  model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])
  optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)

model.compile(optimizer=optimizer, loss='mse')
model.fit(dataset)
model.evaluate(eval_dataset, return_dict=True)

----------------------------------------

TITLE: Implementing CVAE Loss Function and Training Step
DESCRIPTION: Defines the loss function for the CVAE model and implements a single training step using TensorFlow's automatic differentiation.

LANGUAGE: Python
CODE:
optimizer = tf.keras.optimizers.Adam(1e-4)

def log_normal_pdf(sample, mean, logvar, raxis=1):
  log2pi = tf.math.log(2. * np.pi)
  return tf.reduce_sum(
      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),
      axis=raxis)

def compute_loss(model, x):
  mean, logvar = model.encode(x)
  z = model.reparameterize(mean, logvar)
  x_logit = model.decode(z)
  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)
  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])
  logpz = log_normal_pdf(z, 0., 0.)
  logqz_x = log_normal_pdf(z, mean, logvar)
  return -tf.reduce_mean(logpx_z + logpz - logqz_x)

@tf.function
def train_step(model, x, optimizer):
  with tf.GradientTape() as tape:
    loss = compute_loss(model, x)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

----------------------------------------

TITLE: Running Inference on an Input Image
DESCRIPTION: Loads an input image, runs the selected classification model on it, and displays the top 5 predicted classes with their probabilities.

LANGUAGE: Python
CODE:
# Load and preprocess the image
image, original_image = load_image(img_url, image_size, dynamic_size, max_dynamic_size)
show_image(image, 'Scaled image')

# Run model on image
probabilities = tf.nn.softmax(classifier(image)).numpy()

top_5 = tf.argsort(probabilities, axis=-1, direction="DESCENDING")[0][:5].numpy()
np_classes = np.array(classes)

# Some models include an additional 'background' class in the predictions
includes_background_class = probabilities.shape[1] == 1001

for i, item in enumerate(top_5):
  class_index = item if includes_background_class else item + 1
  line = f'({i+1}) {class_index:4} - {classes[class_index]}: {probabilities[0][top_5][i]}'
  print(line)

show_image(image, '')

----------------------------------------

TITLE: Generating and Displaying Adversarial Examples with Different Epsilon Values
DESCRIPTION: Creates adversarial examples by adding perturbations to the original image with different epsilon values. Displays the resulting images along with their predicted labels and confidence scores.

LANGUAGE: Python
CODE:
def display_images(image, description):
  _, label, confidence = get_imagenet_label(pretrained_model.predict(image))
  plt.figure()
  plt.imshow(image[0]*0.5+0.5)
  plt.title('{} \n {} : {:.2f}% Confidence'.format(description,
                                                   label, confidence*100))
  plt.show()

epsilons = [0, 0.01, 0.1, 0.15]
descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')
                for eps in epsilons]

for i, eps in enumerate(epsilons):
  adv_x = image + eps*perturbations
  adv_x = tf.clip_by_value(adv_x, -1, 1)
  display_images(adv_x, descriptions[i])

----------------------------------------

TITLE: Custom Training Loop with MirroredStrategy in TensorFlow
DESCRIPTION: Demonstrates how to implement a custom training loop using MirroredStrategy. This includes creating the model and optimizer, distributing the dataset, and defining the training step.

LANGUAGE: Python
CODE:
with mirrored_strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
  optimizer = tf.train.GradientDescentOptimizer(0.1)

dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(
    global_batch_size)
dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)

def train_step(dist_inputs):
  def step_fn(inputs):
    features, labels = inputs
    logits = model(features)
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(
        logits=logits, labels=labels)
    loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)
    train_op = optimizer.minimize(loss)
    with tf.control_dependencies([train_op]):
      return tf.identity(loss)

  per_replica_losses = mirrored_strategy.run(
      step_fn, args=(dist_inputs,))
  mean_loss = mirrored_strategy.reduce(
      tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)
  return mean_loss

with mirrored_strategy.scope():
  input_iterator = dist_dataset.make_initializable_iterator()
  iterator_init = input_iterator.initializer
  var_init = tf.global_variables_initializer()
  loss = train_step(input_iterator.get_next())
  with tf.Session() as sess:
    sess.run([var_init, iterator_init])
    for _ in range(10):
      print(sess.run(loss))

----------------------------------------

TITLE: Defining TensorFlow Feature Columns
DESCRIPTION: Creates various types of feature columns including numeric, bucketized, categorical, and crossed columns for use in the model.

LANGUAGE: Python
CODE:
feature_columns = []

# Numeric columns
for header in ['PhotoAmt', 'Fee', 'Age']:
  feature_columns.append(feature_column.numeric_column(header))

# Bucketized columns
age = feature_column.numeric_column('Age')
age_buckets = feature_column.bucketized_column(age, boundaries=[1, 2, 3, 4, 5])
feature_columns.append(age_buckets)

# Indicator columns
indicator_column_names = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',
                          'FurLength', 'Vaccinated', 'Sterilized', 'Health']
for col_name in indicator_column_names:
  categorical_column = feature_column.categorical_column_with_vocabulary_list(
      col_name, dataframe[col_name].unique())
  indicator_column = feature_column.indicator_column(categorical_column)
  feature_columns.append(indicator_column)

# Embedding columns
breed1 = feature_column.categorical_column_with_vocabulary_list(
      'Breed1', dataframe.Breed1.unique())
breed1_embedding = feature_column.embedding_column(breed1, dimension=8)
feature_columns.append(breed1_embedding)

# Crossed columns
age_type_feature = feature_column.crossed_column([age_buckets, animal_type], hash_bucket_size=100)
feature_columns.append(feature_column.indicator_column(age_type_feature))

----------------------------------------

TITLE: Loading MNIST Data from NPZ File
DESCRIPTION: Downloads and loads the MNIST dataset from a .npz file using TensorFlow's utilities and NumPy's load function.

LANGUAGE: python
CODE:
DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'

path = tf.keras.utils.get_file('mnist.npz', DATA_URL)
with np.load(path) as data:
  train_examples = data['x_train']
  train_labels = data['y_train']
  test_examples = data['x_test']
  test_labels = data['y_test']

----------------------------------------

TITLE: Linear Model Class Implementation
DESCRIPTION: Defines a simple linear model class with two trainable variables W and b that computes f(x) = W*x + b

LANGUAGE: python
CODE:
class Model(object):
  def __init__(self):
    # Initialize variable to (5.0, 0.0)
    # In practice, these should be initialized to random values.
    self.W = tf.Variable(5.0)
    self.b = tf.Variable(0.0)

  def __call__(self, x):
    return self.W * x + self.b

model = Model()

assert model(3.0).numpy() == 15.0

----------------------------------------

TITLE: Creating and Training Keras Model with TPUEmbedding
DESCRIPTION: Defines a Keras model with TPUEmbedding layer, compiles it with TPUStrategy, and trains/evaluates on TPUs.

LANGUAGE: python
CODE:
strategy = tf.distribute.TPUStrategy(cluster_resolver)
with strategy.scope():
  if hasattr(tf.keras.optimizers, "legacy"):
    optimizer = tf.keras.optimizers.legacy.Adagrad(learning_rate=0.05)
  else:
    optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)
  dense_input = tf.keras.Input(shape=(2,), dtype=tf.float32, batch_size=global_batch_size)
  sparse_input = tf.keras.Input(shape=(), dtype=tf.int32, batch_size=global_batch_size)
  embedded_input = tfrs.layers.embedding.TPUEmbedding(
      feature_config=tf.tpu.experimental.embedding.FeatureConfig(
          table=tf.tpu.experimental.embedding.TableConfig(
              vocabulary_size=10,
              dim=5,
              initializer=tf.initializers.TruncatedNormal(mean=0.0, stddev=1)),
          name="sparse_input"),
      optimizer=optimizer)(sparse_input)
  input = tf.keras.layers.Concatenate(axis=1)([dense_input, embedded_input])
  result = tf.keras.layers.Dense(1)(input)
  model = tf.keras.Model(inputs={"dense_feature": dense_input, "sparse_feature": sparse_input}, outputs=result)
  model.compile(optimizer, "mse", steps_per_execution=10)

model.fit(input_dataset, epochs=5, steps_per_epoch=10)
model.evaluate(eval_dataset, steps=1, return_dict=True)

----------------------------------------

TITLE: Building TensorFlow model for image classification
DESCRIPTION: Constructs a TensorFlow graph for the image classification model using a pre-trained TF-Hub module for feature extraction and a linear classifier on top.

LANGUAGE: Python
CODE:
LEARNING_RATE = 0.01

tf.reset_default_graph()

# Load a pre-trained TF-Hub module for extracting features from images.
image_module = hub.Module('https://tfhub.dev/google/imagenet/mobilenet_v2_035_128/feature_vector/2')

# Preprocessing images into tensors with size expected by the image module.
encoded_images = tf.placeholder(tf.string, shape=[None])
image_size = hub.get_expected_image_size(image_module)

def decode_and_resize_image(encoded):
    decoded = tf.image.decode_jpeg(encoded, channels=3)
    decoded = tf.image.convert_image_dtype(decoded, tf.float32)
    return tf.image.resize_images(decoded, image_size)

batch_images = tf.map_fn(decode_and_resize_image, encoded_images, dtype=tf.float32)

# The image module can be applied as a function to extract feature vectors for a
# batch of images.
features = image_module(batch_images)

def create_model(features):
    """Build a model for classification from extracted features."""
    layer = tf.layers.dense(inputs=features, units=NUM_CLASSES, activation=None)
    return layer

# For each class (kind of flower), the model outputs some real number as a score
# how much the input resembles this class. This vector of numbers is often
# called the "logits".
logits = create_model(features)
labels = tf.placeholder(tf.float32, [None, NUM_CLASSES])

# Mathematically, a good way to measure how much the predicted probabilities
# diverge from the truth is the "cross-entropy" between the two probability
# distributions. For numerical stability, this is best done directly from the
# logits, not the probabilities extracted from them.
cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)
cross_entropy_mean = tf.reduce_mean(cross_entropy)

# Let's add an optimizer so we can train the network.
optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)
train_op = optimizer.minimize(loss=cross_entropy_mean)

# The "softmax" function transforms the logits vector into a vector of
# probabilities: non-negative numbers that sum up to one, and the i-th number
# says how likely the input comes from class i.
probabilities = tf.nn.softmax(logits)

# We choose the highest one as the predicted class.
prediction = tf.argmax(probabilities, 1)
correct_prediction = tf.equal(prediction, tf.argmax(labels, 1))

# The accuracy will allow us to eval on our test set. 
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

----------------------------------------

TITLE: Implementing MNIST Compressor and Decompressor
DESCRIPTION: Defines separate models for compressing MNIST images to strings and decompressing strings back to images.

LANGUAGE: Python
CODE:
class MNISTCompressor(tf.keras.Model):
  """Compresses MNIST images to strings."""

  def __init__(self, analysis_transform, entropy_model):
    super().__init__()
    self.analysis_transform = analysis_transform
    self.entropy_model = entropy_model

  def call(self, x):
    # Ensure inputs are floats in the range (0, 1).
    x = tf.cast(x, self.compute_dtype) / 255.
    y = self.analysis_transform(x)
    # Also return the exact information content of each digit.
    _, bits = self.entropy_model(y, training=False)
    return self.entropy_model.compress(y), bits

class MNISTDecompressor(tf.keras.Model):
  """Decompresses MNIST images from strings."""

  def __init__(self, entropy_model, synthesis_transform):
    super().__init__()
    self.entropy_model = entropy_model
    self.synthesis_transform = synthesis_transform

  def call(self, string):
    y_hat = self.entropy_model.decompress(string, ())
    x_hat = self.synthesis_transform(y_hat)
    # Scale and cast back to 8-bit integer.
    return tf.saturate_cast(tf.round(x_hat * 255.), tf.uint8)


----------------------------------------

TITLE: Implementing SNGP Model Architecture
DESCRIPTION: Defines the SNGP model class by modifying a base ResNet architecture with spectral normalization and a Gaussian process layer.

LANGUAGE: Python
CODE:
class DeepResNetSNGP(DeepResNet):
  def __init__(self, spec_norm_bound=0.9, **kwargs):
    self.spec_norm_bound = spec_norm_bound
    super().__init__(**kwargs)

  def make_dense_layer(self):
    """Applies spectral normalization to the hidden layer."""
    dense_layer = super().make_dense_layer()
    return nlp_layers.SpectralNormalization(
        dense_layer, norm_multiplier=self.spec_norm_bound)

  def make_output_layer(self, num_classes):
    """Uses Gaussian process as the output layer."""
    return nlp_layers.RandomFeatureGaussianProcess(
        num_classes,
        gp_cov_momentum=-1,
        **self.classifier_kwargs)

  def call(self, inputs, training=False, return_covmat=False):
    # Gets logits and a covariance matrix from the GP layer.
    logits, covmat = super().call(inputs)

    # Returns only logits during training.
    if not training and return_covmat:
      return logits, covmat

    return logits

----------------------------------------

TITLE: Assigning New Values to TensorFlow Variables
DESCRIPTION: This snippet shows how to assign new values to existing TensorFlow variables using the assign method, and demonstrates the limitations of reshaping variables.

LANGUAGE: Python
CODE:
a = tf.Variable([2.0, 3.0])
# This will keep the same dtype, float32
a.assign([1, 2]) 
# Not allowed as it resizes the variable: 
try:
  a.assign([1.0, 2.0, 3.0])
except Exception as e:
  print(f"{type(e).__name__}: {e}")

----------------------------------------

TITLE: Computing Higher-Order Gradients in TensorFlow
DESCRIPTION: Demonstrate how to compute second-order derivatives using nested gradient tapes.

LANGUAGE: Python
CODE:
x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0

with tf.GradientTape() as t2:
  with tf.GradientTape() as t1:
    y = x * x * x

  # Compute the gradient inside the outer `t2` context manager
  # which means the gradient computation is differentiable as well.
  dy_dx = t1.gradient(y, x)
d2y_dx2 = t2.gradient(dy_dx, x)

print('dy_dx:', dy_dx.numpy())  # 3 * x**2 => 3.0
print('d2y_dx2:', d2y_dx2.numpy())  # 6 * x => 6.0

----------------------------------------

TITLE: Importing TensorFlow and Checking GPU Availability
DESCRIPTION: This snippet imports TensorFlow and checks for available GPUs.

LANGUAGE: Python
CODE:
import tensorflow as tf

print(tf.config.list_physical_devices('GPU'))

----------------------------------------

TITLE: Defining a Benchmark Function
DESCRIPTION: Creates a function to measure the execution time of iterating over a dataset for a specified number of epochs.

LANGUAGE: Python
CODE:
def benchmark(dataset, num_epochs=2):
    start_time = time.perf_counter()
    for epoch_num in range(num_epochs):
        for sample in dataset:
            # Performing a training step
            time.sleep(0.01)
    print("Execution time:", time.perf_counter() - start_time)

----------------------------------------

TITLE: Adding Variable to Local Collection
DESCRIPTION: Shows how to create a variable and add it to the LOCAL_VARIABLES collection.

LANGUAGE: python
CODE:
my_local = tf.get_variable("my_local", shape=(),
collections=[tf.GraphKeys.LOCAL_VARIABLES])

----------------------------------------

TITLE: Custom Training Loop for TPU in TensorFlow
DESCRIPTION: This code defines a custom training loop for TPU using tf.function and tf.distribute APIs.

LANGUAGE: Python
CODE:
@tf.function
def train_step(iterator):
  """The step function for one training step."""

  def step_fn(inputs):
    """The computation to run on each TPU device."""
    images, labels = inputs
    with tf.GradientTape() as tape:
      logits = model(images, training=True)
      per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(
          labels, logits, from_logits=True)
      loss = tf.nn.compute_average_loss(per_example_loss)
      model_losses = model.losses
      if model_losses:
        loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))

    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))
    training_loss.update_state(loss * strategy.num_replicas_in_sync)
    training_accuracy.update_state(labels, logits)

  strategy.run(step_fn, args=(next(iterator),))

----------------------------------------

TITLE: Compiling and Training MNIST Compression Model
DESCRIPTION: Sets up the model compilation with appropriate loss functions and trains the model on the MNIST dataset.

LANGUAGE: Python
CODE:
def pass_through_loss(_, x):
  # Since rate and distortion are unsupervised, the loss doesn't need a target.
  return x

def make_mnist_compression_trainer(lmbda, latent_dims=50):
  trainer = MNISTCompressionTrainer(latent_dims)
  trainer.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    # Just pass through rate and distortion as losses/metrics.
    loss=dict(rate=pass_through_loss, distortion=pass_through_loss),
    metrics=dict(rate=pass_through_loss, distortion=pass_through_loss),
    loss_weights=dict(rate=1., distortion=lmbda),
  )
  return trainer

def train_mnist_model(lmbda):
  trainer = make_mnist_compression_trainer(lmbda)
  trainer.fit(
      training_dataset.map(add_rd_targets).batch(128).prefetch(8),
      epochs=15,
      validation_data=validation_dataset.map(add_rd_targets).batch(128).cache(),
      validation_freq=1,
      verbose=1,
  )
  return trainer

trainer = train_mnist_model(lmbda=2000)


----------------------------------------

TITLE: Implementing Adam Optimizer in TensorFlow
DESCRIPTION: Creates an Adaptive Moment Estimation (Adam) optimizer, which combines ideas from momentum and RMSprop. Includes bias correction and configurable hyperparameters.

LANGUAGE: Python
CODE:
class Adam(tf.Module):
  
    def __init__(self, learning_rate=1e-3, beta_1=0.9, beta_2=0.999, ep=1e-7):
      # Initialize the Adam parameters
      self.beta_1 = beta_1
      self.beta_2 = beta_2
      self.learning_rate = learning_rate
      self.ep = ep
      self.t = 1.
      self.v_dvar, self.s_dvar = [], []
      self.title = f"Adam: learning rate={self.learning_rate}"
      self.built = False

    def apply_gradients(self, grads, vars):
      # Set up moment and RMSprop slots for each variable on the first call
      if not self.built:
        for var in vars:
          v = tf.Variable(tf.zeros(shape=var.shape))
          s = tf.Variable(tf.zeros(shape=var.shape))
          self.v_dvar.append(v)
          self.s_dvar.append(s)
        self.built = True
      # Perform Adam updates
      for i, (d_var, var) in enumerate(zip(grads, vars)):
        # Moment calculation
        self.v_dvar[i] = self.beta_1*self.v_dvar[i] + (1-self.beta_1)*d_var
        # RMSprop calculation
        self.s_dvar[i] = self.beta_2*self.s_dvar[i] + (1-self.beta_2)*tf.square(d_var)
        # Bias correction
        v_dvar_bc = self.v_dvar[i]/(1-(self.beta_1**self.t))
        s_dvar_bc = self.s_dvar[i]/(1-(self.beta_2**self.t))
        # Update model variables
        var.assign_sub(self.learning_rate*(v_dvar_bc/(tf.sqrt(s_dvar_bc) + self.ep)))
      # Increment the iteration counter
      self.t += 1.

----------------------------------------

TITLE: Setting Up TensorFlow and Required Dependencies
DESCRIPTION: Imports necessary Python libraries including TensorFlow, NumPy, timeit, traceback, and contextlib for demonstration purposes.

LANGUAGE: python
CODE:
import numpy as np
import timeit
import traceback
import contextlib

import tensorflow as tf

----------------------------------------

TITLE: Implementing Episode Run and Return Calculation
DESCRIPTION: Defines functions to run a single episode, collect training data, and compute expected returns for each timestep.

LANGUAGE: Python
CODE:
def run_episode(
    initial_state: tf.Tensor,
    model: tf.keras.Model,
    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
  """Runs a single episode to collect training data."""

  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)

  initial_state_shape = initial_state.shape
  state = initial_state

  for t in tf.range(max_steps):
    # Convert state into a batched tensor (batch size = 1)
    state = tf.expand_dims(state, 0)

    # Run the model and to get action probabilities and critic value
    action_logits_t, value = model(state)

    # Sample next action from the action probability distribution
    action = tf.random.categorical(action_logits_t, 1)[0, 0]
    action_probs_t = tf.nn.softmax(action_logits_t)

    # Store critic values
    values = values.write(t, tf.squeeze(value))

    # Store log probability of the action chosen
    action_probs = action_probs.write(t, action_probs_t[0, action])

    # Apply action to the environment to get next state and reward
    state, reward, done = env_step(action)
    state.set_shape(initial_state_shape)

    # Store reward
    rewards = rewards.write(t, reward)

    if tf.cast(done, tf.bool):
      break

  action_probs = action_probs.stack()
  values = values.stack()
  rewards = rewards.stack()

  return action_probs, values, rewards

def get_expected_return(
    rewards: tf.Tensor,
    gamma: float,
    standardize: bool = True) -> tf.Tensor:
  """Compute expected returns per timestep."""

  n = tf.shape(rewards)[0]
  returns = tf.TensorArray(dtype=tf.float32, size=n)

  # Start from the end of `rewards` and accumulate reward sums
  # into the `returns` array
  rewards = tf.cast(rewards[::-1], dtype=tf.float32)
  discounted_sum = tf.constant(0.0)
  discounted_sum_shape = discounted_sum.shape
  for i in tf.range(n):
    reward = rewards[i]
    discounted_sum = reward + gamma * discounted_sum
    discounted_sum.set_shape(discounted_sum_shape)
    returns = returns.write(i, discounted_sum)
  returns = returns.stack()[::-1]

  if standardize:
    returns = ((returns - tf.math.reduce_mean(returns)) /
               (tf.math.reduce_std(returns) + eps))

  return returns

----------------------------------------

TITLE: Creating Generator Model Architecture
DESCRIPTION: Defines the generator network using convolutional transpose layers to transform random noise into 28x28 pixel images.

LANGUAGE: Python
CODE:
def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256)

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    return model

----------------------------------------

TITLE: Performing Inference on New Data
DESCRIPTION: Save the trained model, reload it, and use it to make predictions on new data samples.

LANGUAGE: Python
CODE:
model.save('my_pet_classifier.keras')
reloaded_model = tf.keras.models.load_model('my_pet_classifier.keras')

sample = {
    'Type': 'Cat',
    'Age': 3,
    'Breed1': 'Tabby',
    'Gender': 'Male',
    'Color1': 'Black',
    'Color2': 'White',
    'MaturitySize': 'Small',
    'FurLength': 'Short',
    'Vaccinated': 'No',
    'Sterilized': 'No',
    'Health': 'Healthy',
    'Fee': 100,
    'PhotoAmt': 2,
}

input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}
predictions = reloaded_model.predict(input_dict)
prob = tf.nn.sigmoid(predictions[0])

print(
    "This particular pet had a %.1f percent probability "
    "of getting adopted." % (100 * prob)
)

----------------------------------------

TITLE: Configuring Distribution Strategy
DESCRIPTION: Creates MirroredStrategy instance for distributed training across multiple GPUs

LANGUAGE: python
CODE:
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

----------------------------------------

TITLE: Starting Profiler Server
DESCRIPTION: Start a gRPC server for on-demand profiling of a TensorFlow model.

LANGUAGE: Python
CODE:
# Start a profiler server before your model runs.
tf.profiler.experimental.server.start(6009)
# (Model code goes here).
#  Send a request to the profiler server to collect a trace of your model.
tf.profiler.experimental.client.trace('grpc://localhost:6009',
                                          'gs://your_tb_logdir', 2000)

----------------------------------------

TITLE: Implementing Custom Training Loop
DESCRIPTION: Defines the training loop with loss calculation, gradient computation and model optimization

LANGUAGE: Python
CODE:
def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value = loss(model, inputs, targets)
  return loss_value, tape.gradient(loss_value, model.trainable_variables)

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
global_step = tf.Variable(0)

----------------------------------------

TITLE: Creating Basic SparseTensor in TensorFlow
DESCRIPTION: Demonstrates how to create a sparse tensor by directly specifying indices, values and dense shape.

LANGUAGE: python
CODE:
st1 = tf.sparse.SparseTensor(indices=[[0, 3], [2, 4]],
                      values=[10, 20],
                      dense_shape=[3, 10])

----------------------------------------

TITLE: Implementing PatchGAN Discriminator
DESCRIPTION: Discriminator model implementation using a PatchGAN architecture to classify real vs generated image patches.

LANGUAGE: Python
CODE:
def Discriminator():
  initializer = tf.random_normal_initializer(0., 0.02)

  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')
  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')

  x = tf.keras.layers.concatenate([inp, tar])

  down1 = downsample(64, 4, False)(x)
  down2 = downsample(128, 4)(down1)
  down3 = downsample(256, 4)(down2)

  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)
  conv = tf.keras.layers.Conv2D(512, 4, strides=1,
                                kernel_initializer=initializer,
                                use_bias=False)(zero_pad1)

  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)

  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)

  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)

  last = tf.keras.layers.Conv2D(1, 4, strides=1,
                                kernel_initializer=initializer)(zero_pad2)

  return tf.keras.Model(inputs=[inp, tar], outputs=last)

----------------------------------------

TITLE: Defining Keras Model Architecture
DESCRIPTION: Creates a CNN model for MNIST classification using Keras Sequential API within distribution strategy scope

LANGUAGE: python
CODE:
with strategy.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10)
  ])

  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=['accuracy'])

----------------------------------------

TITLE: Exporting Trained Model as SavedModel
DESCRIPTION: Saves the trained model in the SavedModel format for platform-agnostic deployment and later use.

LANGUAGE: Python
CODE:
path = 'saved_model/'
model.save(path)

----------------------------------------

TITLE: Creating a Binary Target Variable
DESCRIPTION: Transform the 'AdoptionSpeed' column into a binary classification problem and drop unused features.

LANGUAGE: Python
CODE:
dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)

# Drop unused features.
dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])

----------------------------------------

TITLE: Custom Module Definition - Python
DESCRIPTION: Defines a custom TensorFlow module with variable state and function decorators for demonstrating SavedModel functionality.

LANGUAGE: python
CODE:
class CustomModule(tf.Module):

  def __init__(self):
    super(CustomModule, self).__init__()
    self.v = tf.Variable(1.)

  @tf.function
  def __call__(self, x):
    print('Tracing with', x)
    return x * self.v

  @tf.function(input_signature=[tf.TensorSpec([], tf.float32)])
  def mutate(self, new_v):
    self.v.assign(new_v)

module = CustomModule()

----------------------------------------

TITLE: Loading and Preprocessing Image for Object Detection
DESCRIPTION: Defines a function to load an image from a file or URL and preprocess it into a numpy array suitable for input to the object detection model.

LANGUAGE: python
CODE:
def load_image_into_numpy_array(path):
  """Load an image from file into a numpy array.

  Puts image into numpy array to feed into tensorflow graph.
  Note that by convention we put it into a numpy array with shape
  (height, width, channels), where channels=3 for RGB.

  Args:
    path: the file path to the image

  Returns:
    uint8 numpy array with shape (img_height, img_width, 3)
  """
  image = None
  if(path.startswith('http')):
    response = urlopen(path)
    image_data = response.read()
    image_data = BytesIO(image_data)
    image = Image.open(image_data)
  else:
    image_data = tf.io.gfile.GFile(path, 'rb').read()
    image = Image.open(BytesIO(image_data))

  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (1, im_height, im_width, 3)).astype(np.uint8)

----------------------------------------

TITLE: Installing Required Dependencies for TensorFlow Model Saving
DESCRIPTION: Installs pyyaml and h5py packages, which are required to save models in HDF5 format.

LANGUAGE: bash
CODE:
!pip install pyyaml h5py  # Required to save models in HDF5 format.

----------------------------------------

TITLE: Parallelizing Data Transformation with Map
DESCRIPTION: Demonstrates parallelizing data preprocessing using the map transformation with multiple parallel calls.

LANGUAGE: Python
CODE:
def mapped_function(s):
    # Do some hard pre-processing
    tf.py_function(lambda: time.sleep(0.03), [], ())
    return s

benchmark(
    ArtificialDataset()
    .map(
        mapped_function,
        num_parallel_calls=tf.data.AUTOTUNE
    )
)

----------------------------------------

TITLE: Training Loop for CVAE Model
DESCRIPTION: Implements the main training loop for the CVAE model, including periodic image generation and loss reporting.

LANGUAGE: Python
CODE:
for epoch in range(1, epochs + 1):
  start_time = time.time()
  for train_x in train_dataset:
    train_step(model, train_x, optimizer)
  end_time = time.time()

  loss = tf.keras.metrics.Mean()
  for test_x in test_dataset:
    loss(compute_loss(model, test_x))
  elbo = -loss.result()
  display.clear_output(wait=False)
  print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'
        .format(epoch, elbo, end_time - start_time))
  generate_and_save_images(model, epoch, test_sample)

----------------------------------------

TITLE: Training Loop Implementation
DESCRIPTION: Executes the main training loop over specified epochs, performing training and evaluation steps.

LANGUAGE: python
CODE:
EPOCHS = 5

for epoch in range(EPOCHS):
  train_loss.reset_state()
  train_accuracy.reset_state()
  test_loss.reset_state()
  test_accuracy.reset_state()

  for images, labels in train_ds:
    train_step(images, labels)

  for test_images, test_labels in test_ds:
    test_step(test_images, test_labels)

  print(
    f'Epoch {epoch + 1}, '
    f'Loss: {train_loss.result():0.2f}, '
    f'Accuracy: {train_accuracy.result() * 100:0.2f}, '
    f'Test Loss: {test_loss.result():0.2f}, '
    f'Test Accuracy: {test_accuracy.result() * 100:0.2f}'
  )

----------------------------------------

TITLE: Setting Up TensorFlow Dataset for Wav2Vec2 Fine-tuning
DESCRIPTION: Creates a TensorFlow Dataset object from the preprocessed LibriSpeech data for efficient training.

LANGUAGE: Python
CODE:
output_signature = (
    tf.TensorSpec(shape=(None),  dtype=tf.float32),
    tf.TensorSpec(shape=(None), dtype=tf.int32),
)

dataset = tf.data.Dataset.from_generator(inputs_generator, output_signature=output_signature)

BUFFER_SIZE = len(flac_files)
SEED = 42

dataset = dataset.shuffle(BUFFER_SIZE, seed=SEED)
dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(AUDIO_MAXLEN, LABEL_MAXLEN), padding_values=(0.0, 0))
dataset = dataset.prefetch(tf.data.AUTOTUNE)

----------------------------------------

TITLE: Creating Basic Image Preprocessing Pipeline
DESCRIPTION: Defines a sequential model for resizing images to a standard size and rescaling pixel values to [0,1] range

LANGUAGE: Python
CODE:
IMG_SIZE = 180

resize_and_rescale = tf.keras.Sequential([
  layers.Resizing(IMG_SIZE, IMG_SIZE),
  layers.Rescaling(1./255)
])

----------------------------------------

TITLE: Loading Pre-trained Object Detection Model from TensorFlow Hub
DESCRIPTION: Loads a selected pre-trained object detection model from TensorFlow Hub using the model handle.

LANGUAGE: python
CODE:
print('loading model...')
hub_model = hub.load(model_handle)
print('model loaded!')

----------------------------------------

TITLE: Creating Neural Network Model with Keras
DESCRIPTION: Defines a sequential neural network model with three dense layers for Iris classification

LANGUAGE: Python
CODE:
model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),
  tf.keras.layers.Dense(10, activation=tf.nn.relu),
  tf.keras.layers.Dense(3)
])

----------------------------------------

TITLE: Frame Extraction from Video Files
DESCRIPTION: Function to extract frames from video files using OpenCV, with options for number of frames and output size

LANGUAGE: Python
CODE:
def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):
  result = []
  src = cv2.VideoCapture(str(video_path))  
  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)
  need_length = 1 + (n_frames - 1) * frame_step
  if need_length > video_length:
    start = 0
  else:
    max_start = video_length - need_length
    start = random.randint(0, max_start + 1)
  src.set(cv2.CAP_PROP_POS_FRAMES, start)
  ret, frame = src.read()
  result.append(format_frames(frame, output_size))
  for _ in range(n_frames - 1):
    for _ in range(frame_step):
      ret, frame = src.read()
    if ret:
      frame = format_frames(frame, output_size)
      result.append(frame)
    else:
      result.append(np.zeros_like(result[0]))
  src.release()
  result = np.array(result)[..., [2, 1, 0]]
  return result

----------------------------------------

TITLE: Object Detection Model Loading and Inference
DESCRIPTION: Functions to load pre-trained object detection models from TF Hub and run inference on images. Includes timing measurements and visualization of detection results.

LANGUAGE: python
CODE:
module_handle = "https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1"
detector = hub.load(module_handle).signatures['default']

def load_img(path):
  img = tf.io.read_file(path)
  img = tf.image.decode_jpeg(img, channels=3)
  return img

def run_detector(detector, path):
  img = load_img(path)
  converted_img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]
  result = detector(converted_img)
  result = {key:value.numpy() for key,value in result.items()}
  image_with_boxes = draw_boxes(
      img.numpy(), result["detection_boxes"],
      result["detection_class_entities"], result["detection_scores"])
  display_image(image_with_boxes)

----------------------------------------

TITLE: Pulling TensorFlow Docker Images
DESCRIPTION: Commands to download various TensorFlow Docker images, including the latest stable release, nightly dev release with GPU support, and the latest release with GPU support and Jupyter.

LANGUAGE: bash
CODE:
docker pull tensorflow/tensorflow                     # latest stable release
docker pull tensorflow/tensorflow:devel-gpu           # nightly dev release w/ GPU support
docker pull tensorflow/tensorflow:latest-gpu-jupyter  # latest release w/ GPU support and Jupyter

----------------------------------------

TITLE: Loading a pre-trained ImageNet classifier from TensorFlow Hub
DESCRIPTION: Downloads a MobileNetV2 model pre-trained on ImageNet from TensorFlow Hub and wraps it as a Keras layer.

LANGUAGE: Python
CODE:
mobilenet_v2 ="https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4"
inception_v3 = "https://tfhub.dev/google/imagenet/inception_v3/classification/5"

classifier_model = mobilenet_v2 #@param ["mobilenet_v2", "inception_v3"] {type:"raw"}

IMAGE_SHAPE = (224, 224)

classifier = tf.keras.Sequential([
    hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))
])

----------------------------------------

TITLE: Distributed Training Step Implementation
DESCRIPTION: Training step function that handles forward pass, gradient calculation and parameter updates in distributed setting

LANGUAGE: Python
CODE:
@tf.function
def train_step(model, x, y, learning_rate=tf.constant(1e-4)):
  with tf.GradientTape() as tape:
    logits = model(x)
    loss = tf.reduce_sum(
        tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=logits, labels=y))
  parameters = model.trainable_variables
  gradients = tape.gradient(loss, parameters)
  for parameter, parameter_gradient in zip(parameters, gradients):
    parameter.assign_sub(learning_rate * parameter_gradient)
  accuracy = 1.0 - tf.reduce_sum(tf.cast(tf.argmax(logits, axis=-1, output_type=tf.int64) != y, tf.float32)) / x.shape[0]
  loss_per_sample = loss / len(x)
  return {'loss': loss_per_sample, 'accuracy': accuracy}

----------------------------------------

TITLE: SavedModel Save and Load Operations - Python
DESCRIPTION: Demonstrates saving and loading models in SavedModel format, including signature specification and verification.

LANGUAGE: python
CODE:
module_no_signatures_path = os.path.join(tmpdir, 'module_no_signatures')
module(tf.constant(0.))
print('Saving model...')
tf.saved_model.save(module, module_no_signatures_path)

LANGUAGE: python
CODE:
imported = tf.saved_model.load(module_no_signatures_path)
assert imported(tf.constant(3.)).numpy() == 3
imported.mutate(tf.constant(2.))
assert imported(tf.constant(3.)).numpy() == 6

----------------------------------------

TITLE: TensorFlow 2.x Checkpoint Saving with Keras Callbacks
DESCRIPTION: Shows how to implement checkpoint saving using tf.keras.callbacks.ModelCheckpoint in TensorFlow 2.x with a Keras Sequential model.

LANGUAGE: python
CODE:
def create_model():
  return tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
  ])

model = create_model()
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'],
              steps_per_execution=10)

log_dir = tempfile.mkdtemp()

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=log_dir)

model.fit(x=x_train,
          y=y_train,
          epochs=10,
          validation_data=(x_test, y_test),
          callbacks=[model_checkpoint_callback])

----------------------------------------

TITLE: SavedModel Save and Load Operations - Python
DESCRIPTION: Demonstrates saving and loading models in SavedModel format, including signature specification and verification.

LANGUAGE: python
CODE:
module_no_signatures_path = os.path.join(tmpdir, 'module_no_signatures')
module(tf.constant(0.))
print('Saving model...')
tf.saved_model.save(module, module_no_signatures_path)

LANGUAGE: python
CODE:
imported = tf.saved_model.load(module_no_signatures_path)
assert imported(tf.constant(3.)).numpy() == 3
imported.mutate(tf.constant(2.))
assert imported(tf.constant(3.)).numpy() == 6

----------------------------------------

TITLE: Using tf.function as a decorator
DESCRIPTION: Shows how to use tf.function as a decorator to convert a Python function into a TensorFlow graph function. The example includes nested function calls.

LANGUAGE: Python
CODE:
def inner_function(x, y, b):
  x = tf.matmul(x, y)
  x = x + b
  return x

# Using the `tf.function` decorator makes `outer_function` into a
# `PolymorphicFunction`.
@tf.function
def outer_function(x):
  y = tf.constant([[2.0], [3.0]])
  b = tf.constant(4.0)

  return inner_function(x, y, b)

# Note that the callable will create a graph that
# includes `inner_function` as well as `outer_function`.
outer_function(tf.constant([[1.0, 2.0]])).numpy()

----------------------------------------

TITLE: Loading and Using Image Classification Model
DESCRIPTION: Example showing how to load a SavedModel for image classification and process a batch of images to obtain class logits.

LANGUAGE: python
CODE:
obj = hub.load("path/to/model")  # That's tf.saved_model.load() after download.
images = ...  # A batch of images with shape [batch_size, height, width, 3].
logits = obj(images)   # A batch with shape [batch_size, num_classes].

----------------------------------------

TITLE: Importing TensorFlow and Helper Libraries
DESCRIPTION: Sets up the required dependencies including TensorFlow, NumPy and Matplotlib for the image classification project.

LANGUAGE: python
CODE:
# TensorFlow and tf.keras
import tensorflow as tf

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

----------------------------------------

TITLE: Parallel Data Processing with map_and_batch
DESCRIPTION: Demonstrates how to parallelize data transformation using map with num_parallel_calls and the fused map_and_batch operation.

LANGUAGE: python
CODE:
dataset = dataset.apply(tf.contrib.data.map_and_batch(
    map_func=parse_fn, batch_size=FLAGS.batch_size))

----------------------------------------

TITLE: Configuring Training Components
DESCRIPTION: Sets up loss function, optimizer, and metrics for model training and evaluation.

LANGUAGE: python
CODE:
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

optimizer = tf.keras.optimizers.Adam()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

----------------------------------------

TITLE: Loading GIF Processing Function
DESCRIPTION: Function to load and preprocess a GIF file into a TensorFlow tensor with proper formatting

LANGUAGE: python
CODE:
def load_gif(file_path, image_size=(224, 224)):
  raw = tf.io.read_file(file_path)
  video = tf.io.decode_gif(raw)
  video = tf.image.resize(video, image_size)
  video = tf.cast(video, tf.float32) / 255.
  return video

----------------------------------------

TITLE: Preprocessing Images
DESCRIPTION: Defines functions to load and preprocess images, including decoding, resizing, and normalizing.

LANGUAGE: Python
CODE:
def preprocess_image(image):
  image = tf.image.decode_jpeg(image, channels=3)
  image = tf.image.resize(image, [192, 192])
  image /= 255.0  # normalize to [0,1] range

  return image

def load_and_preprocess_image(path):
  image = tf.read_file(path)
  return preprocess_image(image)

----------------------------------------

TITLE: Using Ragged Tensors with Keras Models
DESCRIPTION: Shows how to use ragged tensors in a Keras model by converting to dense tensors and using masking.

LANGUAGE: python
CODE:
# Build the Keras model.
keras_model = tf.keras.Sequential([
    tf.keras.layers.Embedding(hash_buckets, 16, mask_zero=True),
    tf.keras.layers.LSTM(32, return_sequences=True, use_bias=False),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(32),
    tf.keras.layers.Activation(tf.nn.relu),
    tf.keras.layers.Dense(1)
])

keras_model.compile(loss='binary_crossentropy', optimizer='rmsprop')
keras_model.fit(hashed_words.to_tensor(), is_question, epochs=5)

----------------------------------------

TITLE: Implementing Logistic Regression Model
DESCRIPTION: Class definition for logistic regression model using TensorFlow variables and operations

LANGUAGE: Python
CODE:
class LogisticRegression(tf.Module):

  def __init__(self):
    self.built = False
    
  def __call__(self, x, train=True):
    if not self.built:
      rand_w = tf.random.uniform(shape=[x.shape[-1], 1], seed=22)
      rand_b = tf.random.uniform(shape=[], seed=22)
      self.w = tf.Variable(rand_w)
      self.b = tf.Variable(rand_b)
      self.built = True
    z = tf.add(tf.matmul(x, self.w), self.b)
    z = tf.squeeze(z, axis=1)
    if train:
      return z
    return tf.sigmoid(z)

----------------------------------------

TITLE: Creating Adversarial Pattern using Fast Gradient Sign Method
DESCRIPTION: Implements the FGSM attack by creating a function that generates an adversarial pattern. It uses GradientTape to compute gradients of the loss with respect to the input image.

LANGUAGE: Python
CODE:
loss_object = tf.keras.losses.CategoricalCrossentropy()

def create_adversarial_pattern(input_image, input_label):
  with tf.GradientTape() as tape:
    tape.watch(input_image)
    prediction = pretrained_model(input_image)
    loss = loss_object(input_label, prediction)

  # Get the gradients of the loss w.r.t to the input image.
  gradient = tape.gradient(loss, input_image)
  # Get the sign of the gradients to create the perturbation
  signed_grad = tf.sign(gradient)
  return signed_grad

----------------------------------------

TITLE: Installing and Importing Keras Tuner
DESCRIPTION: Install the Keras Tuner library using pip and import it for use in the notebook.

LANGUAGE: python
CODE:
!pip install -q -U keras-tuner
import keras_tuner as kt

----------------------------------------

TITLE: Ensuring Correct Sample Rate for Audio Input
DESCRIPTION: This function resamples the input audio to ensure it matches the required sample rate of 16kHz for the YAMNet model.

LANGUAGE: python
CODE:
def ensure_sample_rate(original_sample_rate, waveform,
                       desired_sample_rate=16000):
  """Resample waveform if required."""
  if original_sample_rate != desired_sample_rate:
    desired_length = int(round(float(len(waveform)) /
                               original_sample_rate * desired_sample_rate))
    waveform = scipy.signal.resample(waveform, desired_length)
  return desired_sample_rate, waveform

----------------------------------------

TITLE: Implementing Truncated Backpropagation in TensorFlow
DESCRIPTION: Code snippet showing how to create a TensorFlow graph for truncated backpropagation through time, processing a fixed number of steps and updating the LSTM state.

LANGUAGE: python
CODE:
# Placeholder for the inputs in a given iteration.
words = tf.placeholder(tf.int32, [batch_size, num_steps])

lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
initial_state = state = lstm.zero_state(batch_size, dtype=tf.float32)

for i in range(num_steps):
    # The value of state is updated after processing each batch of words.
    output, state = lstm(words[:, i], state)

    # The rest of the code.
    # ...

final_state = state

----------------------------------------

TITLE: Parallel Data Extraction Implementation
DESCRIPTION: Shows how to implement parallel data extraction using parallel_interleave for improved I/O performance.

LANGUAGE: python
CODE:
dataset = files.apply(tf.contrib.data.parallel_interleave(
    tf.data.TFRecordDataset, cycle_length=FLAGS.num_parallel_readers))

----------------------------------------

TITLE: Creating and Running TensorFlow Graph Operations
DESCRIPTION: Demonstrates creating constant tensors and performing basic operations in a TensorFlow graph.

LANGUAGE: python
CODE:
a = tf.constant(3.0, dtype=tf.float32)
b = tf.constant(4.0) # also tf.float32 implicitly
total = a + b
print(a)
print(b)
print(total)

----------------------------------------

TITLE: Calling a Reusable SavedModel in TensorFlow
DESCRIPTION: Shows the general syntax for calling a Reusable SavedModel object, including the optional 'trainable' parameter and additional kwargs.

LANGUAGE: Python
CODE:
outputs = obj(inputs, trainable=..., **kwargs)

----------------------------------------

TITLE: Implementing Data Packing for DTensor
DESCRIPTION: Defines helper functions to pack training data batches into DTensors sharded along the batch axis. This ensures even distribution of training data across the mesh dimension.

LANGUAGE: Python
CODE:
def repack_local_tensor(x, layout):
  # Repacks a local Tensor-like to a DTensor with layout
  # This function assumes a single-client application
  x = tf.convert_to_tensor(x)
  sharded_dims = []

  # For every sharded dimension, use tf.split to split the along the dimension.
  # The result is a nested list of split-tensors in queue[0].
  queue = [x]
  for axis, dim in enumerate(layout.sharding_specs):
    if dim == dtensor.UNSHARDED:
      continue
    num_splits = layout.shape[axis]
    queue = tf.nest.map_structure(lambda x: tf.split(x, num_splits, axis=axis), queue)
    sharded_dims.append(dim)

  # Now you can build the list of component tensors by looking up the location in
  # the nested list of split-tensors created in queue[0].
  components = []
  for locations in layout.mesh.local_device_locations():
    t = queue[0]
    for dim in sharded_dims:
      split_index = locations[dim]  # Only valid on single-client mesh.
      t = t[split_index]
    components.append(t)

  return dtensor.pack(components, layout)

def repack_batch(x, y, mesh):
  # Pack training data batches into DTensors along the batch axis
  x = repack_local_tensor(x, layout=dtensor.Layout(['batch', dtensor.UNSHARDED], mesh))
  y = repack_local_tensor(y, layout=dtensor.Layout(['batch'], mesh))
  return x, y

----------------------------------------

TITLE: Importing TensorFlow and Keras in Python
DESCRIPTION: Imports TensorFlow, Keras, and specific Keras layers. Prints the TensorFlow version.

LANGUAGE: Python
CODE:
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

print(tf.__version__)

----------------------------------------

TITLE: Compressing Classifier Model
DESCRIPTION: Defines a function to convert a compressible model into a compressed model using clone_model.

LANGUAGE: python
CODE:
def compress_layer(layer):
  if isinstance(layer, CompressibleDense):
    return CompressedDense.copy(layer)
  if isinstance(layer, CompressibleConv2D):
    return CompressedConv2D.copy(layer)
  return type(layer).from_config(layer.get_config())

compressed_classifier = tf.keras.models.clone_model(
    compressible_classifier, clone_function=compress_layer)

----------------------------------------

TITLE: Creating an Input Pipeline with tf.data
DESCRIPTION: Define a utility function to convert DataFrames into tf.data.Dataset objects with shuffling and batching.

LANGUAGE: Python
CODE:
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  df = dataframe.copy()
  labels = df.pop('target')
  df = {key: value.to_numpy()[:,tf.newaxis] for key, value in dataframe.items()}
  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  ds = ds.prefetch(batch_size)
  return ds

----------------------------------------

TITLE: Parallel TensorFlow Record Processing
DESCRIPTION: Demonstrates parallel processing of TFRecord files using parallel map operations for better performance.

LANGUAGE: python
CODE:
dataset = tf.data.TFRecordDataset(filename)
dataset = dataset.map(parse_record, num_parallel_calls=2)
dataset = dataset.batch(32)
dataset = dataset.repeat()

----------------------------------------

TITLE: Computing Gradients with tf.GradientTape
DESCRIPTION: Demonstrates the basic usage of tf.GradientTape to compute gradients of a simple calculation.

LANGUAGE: Python
CODE:
x = tf.Variable(3.0)

with tf.GradientTape() as tape:
  y = x**2

# dy = 2x * dx
dy_dx = tape.gradient(y, x)
dy_dx.numpy()

----------------------------------------

TITLE: Defining Keras Model for MNIST Classification on TPU
DESCRIPTION: This function creates a Keras Sequential model for MNIST image classification with L2 regularization on each layer.

LANGUAGE: Python
CODE:
def create_model():
  regularizer = tf.keras.regularizers.L2(1e-5)
  return tf.keras.Sequential(
      [tf.keras.layers.Conv2D(256, 3, input_shape=(28, 28, 1),
                              activation='relu',
                              kernel_regularizer=regularizer),
       tf.keras.layers.Conv2D(256, 3,
                              activation='relu',
                              kernel_regularizer=regularizer),
       tf.keras.layers.Flatten(),
       tf.keras.layers.Dense(256,
                             activation='relu',
                             kernel_regularizer=regularizer),
       tf.keras.layers.Dense(128,
                             activation='relu',
                             kernel_regularizer=regularizer),
       tf.keras.layers.Dense(10,
                             kernel_regularizer=regularizer)])

----------------------------------------

TITLE: Measuring performance difference between eager and graph execution
DESCRIPTION: Compares the execution time of a matrix power function in eager mode versus graph mode using tf.function.

LANGUAGE: Python
CODE:
x = tf.random.uniform(shape=[10, 10], minval=-1, maxval=2, dtype=tf.dtypes.int32)

def power(x, y):
  result = tf.eye(10, dtype=tf.dtypes.int32)
  for _ in range(y):
    result = tf.matmul(x, result)
  return result

print("Eager execution:", timeit.timeit(lambda: power(x, 100), number=1000), "seconds")

power_as_graph = tf.function(power)
print("Graph execution:", timeit.timeit(lambda: power_as_graph(x, 100), number=1000), "seconds")

----------------------------------------

TITLE: Importing TensorFlow and utility libraries
DESCRIPTION: Imports the necessary libraries for the examples in this guide, including TensorFlow, timeit for performance measurements, and datetime.

LANGUAGE: Python
CODE:
import tensorflow as tf
import timeit
from datetime import datetime

----------------------------------------

TITLE: Creating Custom TensorFlow Estimator
DESCRIPTION: Python code to create a custom TensorFlow Estimator with specified model function and parameters.

LANGUAGE: python
CODE:
classifier = tf.estimator.Estimator(
    model_fn=my_model_fn,
    params={
        'feature_columns': my_feature_columns,
        # Two hidden layers of 10 nodes each.
        'hidden_units': [10, 10],
        # The model must choose between 3 classes.
        'n_classes': 3,
    })

----------------------------------------

TITLE: Distributed Training with Keras and TensorFlow Estimators
DESCRIPTION: Shows how to use tf.distribute.MirroredStrategy for distributed training of a Keras model converted to a TensorFlow Estimator.

LANGUAGE: Python
CODE:
def input_fn():
  x = np.random.random((1024, 10))
  y = np.random.randint(2, size=(1024, 1))
  x = tf.cast(x, tf.float32)
  dataset = tf.data.Dataset.from_tensor_slices((x, y))
  dataset = dataset.repeat(10)
  dataset = dataset.batch(32)
  return dataset

strategy = tf.distribute.MirroredStrategy()
config = tf.estimator.RunConfig(train_distribute=strategy)

keras_estimator = tf.keras.estimator.model_to_estimator(
  keras_model=model,
  config=config,
  model_dir='/tmp/model_dir')

keras_estimator.train(input_fn=input_fn, steps=10)

----------------------------------------

TITLE: Creating a tf.train.Example Message in TensorFlow
DESCRIPTION: Demonstrates how to create a tf.train.Example message from existing data using the helper functions.

LANGUAGE: Python
CODE:
@tf.py_function(Tout=tf.string)
def serialize_example(feature0, feature1, feature2, feature3):
  """
  Creates a tf.train.Example message ready to be written to a file.
  """
  # Create a dictionary mapping the feature name to the tf.train.Example-compatible
  # data type.
  feature = {
      'feature0': _int64_feature(feature0),
      'feature1': _int64_feature(feature1),
      'feature2': _bytes_feature(feature2),
      'feature3': _float_feature(feature3),
  }

  # Create a Features message using tf.train.Example.

  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
  return example_proto.SerializeToString()

----------------------------------------

TITLE: Loading and Processing UCF101 Dataset
DESCRIPTION: Lists videos in the UCF101 dataset, categorizes them, and loads a sample video for processing.

LANGUAGE: Python
CODE:
ucf_videos = list_ucf_videos()

categories = {}
for video in ucf_videos:
    category = video[2:-12]
    if category not in categories:
        categories[category] = []
    categories[category].append(video)
print("Found %d videos in %d categories." % (len(ucf_videos), len(categories)))

for category, sequences in categories.items():
    summary = ", ".join(sequences[:2])
    print("%-20s %4d videos (%s, ...)" % (category, len(sequences), summary))

# Get a sample cricket video.
video_path = fetch_ucf_video("v_CricketShot_g04_c02.avi")
sample_video = load_video(video_path)

----------------------------------------

TITLE: Loading and Preprocessing Dataset
DESCRIPTION: Loads the horse2zebra dataset and applies preprocessing steps including random jittering and normalization.

LANGUAGE: Python
CODE:
dataset, metadata = tfds.load('cycle_gan/horse2zebra',
                              with_info=True, as_supervised=True)

train_horses, train_zebras = dataset['trainA'], dataset['trainB']
test_horses, test_zebras = dataset['testA'], dataset['testB']

# Preprocessing functions
def normalize(image):
  image = tf.cast(image, tf.float32)
  image = (image / 127.5) - 1
  return image

def random_jitter(image):
  # resizing to 286 x 286 x 3
  image = tf.image.resize(image, [286, 286],
                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)

  # randomly cropping to 256 x 256 x 3
  image = random_crop(image)

  # random mirroring
  image = tf.image.random_flip_left_right(image)

  return image

# Apply preprocessing
train_horses = train_horses.map(
    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(
    BUFFER_SIZE).batch(BATCH_SIZE)

train_zebras = train_zebras.map(
    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(
    BUFFER_SIZE).batch(BATCH_SIZE)

----------------------------------------

TITLE: Defining Analysis Transform for MNIST Compression
DESCRIPTION: Creates the analysis (encoder) transform using a series of convolutional and dense layers.

LANGUAGE: Python
CODE:
def make_analysis_transform(latent_dims):
  """Creates the analysis (encoder) transform."""
  return tf.keras.Sequential([
      tf.keras.layers.Conv2D(
          20, 5, use_bias=True, strides=2, padding="same",
          activation="leaky_relu", name="conv_1"),
      tf.keras.layers.Conv2D(
          50, 5, use_bias=True, strides=2, padding="same",
          activation="leaky_relu", name="conv_2"),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(
          500, use_bias=True, activation="leaky_relu", name="fc_1"),
      tf.keras.layers.Dense(
          latent_dims, use_bias=True, activation=None, name="fc_2"),
  ], name="analysis_transform")


----------------------------------------

TITLE: Loading a Reusable SavedModel in TensorFlow Hub
DESCRIPTION: Demonstrates how to load a Reusable SavedModel from TensorFlow Hub using the hub.load() function.

LANGUAGE: Python
CODE:
obj = hub.load(url)

----------------------------------------

TITLE: Implementing DTensor-Aware Dense Layer
DESCRIPTION: Creates a dense layer module that supports DTensor. It uses dtensor.call_with_layout to initialize DTensor variables and implements the forward pass.

LANGUAGE: Python
CODE:
class DenseLayer(tf.Module):

  def __init__(self, in_dim, out_dim, weight_layout, activation=tf.identity):
    super().__init__()
    # Initialize dimensions and the activation function
    self.in_dim, self.out_dim = in_dim, out_dim
    self.activation = activation

    # Initialize the DTensor weights using the Xavier scheme
    uniform_initializer = tf.function(tf.random.stateless_uniform)
    xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(self.in_dim + self.out_dim, tf.float32))
    self.w = dtensor.DVariable(
      dtensor.call_with_layout(
          uniform_initializer, weight_layout,
          shape=(self.in_dim, self.out_dim), seed=(22, 23),
          minval=-xavier_lim, maxval=xavier_lim))
        
    # Initialize the bias with the zeros
    bias_layout = weight_layout.delete([0])
    self.b = dtensor.DVariable(
      dtensor.call_with_layout(tf.zeros, bias_layout, shape=[out_dim]))

  def __call__(self, x):
    # Compute the forward pass
    z = tf.add(tf.matmul(x, self.w), self.b)
    return self.activation(z)

----------------------------------------

TITLE: Evaluating CNN Model Performance
DESCRIPTION: This snippet plots the training and validation accuracy over epochs and evaluates the model on the test set.

LANGUAGE: Python
CODE:
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

----------------------------------------

TITLE: Evaluating CNN Model Performance
DESCRIPTION: This snippet plots the training and validation accuracy over epochs and evaluates the model on the test set.

LANGUAGE: Python
CODE:
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

----------------------------------------

TITLE: Fine-tuning CropNet Model with Model Maker
DESCRIPTION: Uses TensorFlow Model Maker to fine-tune a pre-trained CropNet model on the prepared dataset, including training configuration and model creation.

LANGUAGE: Python
CODE:
model_name = 'mobilenet_v3_large_100_224'  #@param ['cropnet_cassava', 'cropnet_concat', 'cropnet_imagenet', 'mobilenet_v3_large_100_224']

map_model_name = {
    'cropnet_cassava': 'https://tfhub.dev/google/cropnet/feature_vector/cassava_disease_V1/1',
    'cropnet_concat': 'https://tfhub.dev/google/cropnet/feature_vector/concat/1',
    'cropnet_imagenet': 'https://tfhub.dev/google/cropnet/feature_vector/imagenet/1',
    'mobilenet_v3_large_100_224': 'https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5',
}

model_handle = map_model_name[model_name]
image_model_spec = ModelSpec(uri=model_handle)

model = image_classifier.create(
    train_data,
    model_spec=image_model_spec,
    batch_size=128,
    learning_rate=0.03,
    epochs=5,
    shuffle=True,
    train_whole_model=True,
    validation_data=validation_data)

----------------------------------------

TITLE: Creating and Training Text Classification Model
DESCRIPTION: Defines a text classification model using Keras layers, compiles it, and trains on the preprocessed dataset.

LANGUAGE: Python
CODE:
model = create_model(vocab_size=VOCAB_SIZE+2, num_labels=3)

model.compile(
    optimizer='adam',
    loss=losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'])

history = model.fit(train_data, validation_data=validation_data, epochs=3)

----------------------------------------

TITLE: Building and Compiling the RNN Model
DESCRIPTION: Constructs the RNN model architecture using LSTM layers and compiles it with custom loss functions and loss weights.

LANGUAGE: Python
CODE:
input_shape = (seq_length, 3)
learning_rate = 0.005

inputs = tf.keras.Input(input_shape)
x = tf.keras.layers.LSTM(128)(inputs)

outputs = {
  'pitch': tf.keras.layers.Dense(128, name='pitch')(x),
  'step': tf.keras.layers.Dense(1, name='step')(x),
  'duration': tf.keras.layers.Dense(1, name='duration')(x),
}

model = tf.keras.Model(inputs, outputs)

loss = {
      'pitch': tf.keras.losses.SparseCategoricalCrossentropy(
          from_logits=True),
      'step': mse_with_positive_pressure,
      'duration': mse_with_positive_pressure,
}

optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

model.compile(
    loss=loss,
    loss_weights={
        'pitch': 0.05,
        'step': 1.0,
        'duration':1.0,
    },
    optimizer=optimizer,
)

----------------------------------------

TITLE: Setting up TensorFlow and TensorBoard Imports
DESCRIPTION: Imports required TensorFlow libraries and enables TensorBoard extension

LANGUAGE: python
CODE:
import tensorflow_datasets as tfds
import tensorflow as tf

import os

# Load the TensorBoard notebook extension.
%load_ext tensorboard

----------------------------------------

TITLE: Loading and Configuring S3D MIL-NCE Model
DESCRIPTION: Loads the S3D MIL-NCE model from TensorFlow Hub and defines a function to generate embeddings from video frames and text input.

LANGUAGE: python
CODE:
hub_handle = 'https://tfhub.dev/deepmind/mil-nce/s3d/1'
hub_model = hub.load(hub_handle)

def generate_embeddings(model, input_frames, input_words):
  """Generate embeddings from the model from video frames and input words."""
  vision_output = model.signatures['video'](tf.constant(tf.cast(input_frames, dtype=tf.float32)))
  text_output = model.signatures['text'](tf.constant(input_words))
  return vision_output['video_embedding'], text_output['text_embedding']

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Installing necessary Python packages including remotezip for ZIP handling, tqdm for progress bars, OpenCV for video processing, and TensorFlow models

LANGUAGE: Shell
CODE:
!pip install remotezip tqdm opencv-python==4.5.2.52 opencv-python-headless==4.5.2.52 tf-models-official

----------------------------------------

TITLE: Converting Between Ragged, Dense, and Sparse Tensors
DESCRIPTION: Shows methods for converting between ragged tensors and other tensor types like dense and sparse tensors.

LANGUAGE: python
CODE:
ragged_sentences = tf.ragged.constant([
    ['Hi'], ['Welcome', 'to', 'the', 'fair'], ['Have', 'fun']])

# RaggedTensor -> Tensor
print(ragged_sentences.to_tensor(default_value='', shape=[None, 10]))

# Tensor -> RaggedTensor
x = [[1, 3, -1, -1], [2, -1, -1, -1], [4, 5, 8, 9]]
print(tf.RaggedTensor.from_tensor(x, padding=-1))

#RaggedTensor -> SparseTensor
print(ragged_sentences.to_sparse())

# SparseTensor -> RaggedTensor
st = tf.SparseTensor(indices=[[0, 0], [2, 0], [2, 1]],
                     values=['a', 'b', 'c'],
                     dense_shape=[3, 3])
print(tf.RaggedTensor.from_sparse(st))

----------------------------------------

TITLE: Training Custom TensorFlow Estimator
DESCRIPTION: Python code to train a custom TensorFlow Estimator using the specified input function and number of steps.

LANGUAGE: python
CODE:
# Train the Model.
classifier.train(
    input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),
    steps=args.train_steps)

----------------------------------------

TITLE: Compiling Keras Model
DESCRIPTION: Configures the model for training by specifying loss function and optimizer

LANGUAGE: python
CODE:
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam')
model.summary()

----------------------------------------

TITLE: Using Distribution Strategies with RNG
DESCRIPTION: Shows how to use random number generators with TensorFlow distribution strategies.

LANGUAGE: python
CODE:
strat = tf.distribute.MirroredStrategy(devices=["cpu:0", "cpu:1"])
with strat.scope():
  g = tf.random.Generator.from_seed(1)
  print(strat.run(lambda: g.normal([])))
  print(strat.run(lambda: g.normal([])))

----------------------------------------

TITLE: Building Sequential Neural Network Model
DESCRIPTION: Create a sequential model with flatten, dense and dropout layers for image classification.

LANGUAGE: python
CODE:
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

----------------------------------------

TITLE: Building the RNN Model
DESCRIPTION: Define a function to build the RNN model using TensorFlow Keras API.

LANGUAGE: Python
CODE:
def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim,
                              batch_input_shape=[batch_size, None]),
    rnn(rnn_units,
        return_sequences=True,
        recurrent_initializer='glorot_uniform',
        stateful=True),
    tf.keras.layers.Dense(vocab_size)
  ])
  return model

----------------------------------------

TITLE: Importing Required Libraries for TensorFlow Model Creation
DESCRIPTION: Imports essential TensorFlow and NumPy libraries for model development

LANGUAGE: python
CODE:
import tensorflow as tf

import numpy as np
import tensorflow_datasets as tfds

----------------------------------------

TITLE: Writing Image Data to TFRecord in TensorFlow
DESCRIPTION: Shows how to write image data to a TFRecord file, including encoding the image and its metadata as tf.train.Example messages.

LANGUAGE: Python
CODE:
def image_example(image_string, label):
  image_shape = tf.io.decode_jpeg(image_string).shape

  feature = {
      'height': _int64_feature(image_shape[0]),
      'width': _int64_feature(image_shape[1]),
      'depth': _int64_feature(image_shape[2]),
      'label': _int64_feature(label),
      'image_raw': _bytes_feature(image_string),
  }

  return tf.train.Example(features=tf.train.Features(feature=feature))

record_file = 'images.tfrecords'
with tf.io.TFRecordWriter(record_file) as writer:
  for filename, label in image_labels.items():
    image_string = open(filename, 'rb').read()
    tf_example = image_example(image_string, label)
    writer.write(tf_example.SerializeToString())

----------------------------------------

TITLE: Downloading and Extracting IMDB Dataset with TensorFlow
DESCRIPTION: This code snippet downloads the IMDB dataset using TensorFlow's get_file utility and extracts it to a local directory.

LANGUAGE: Python
CODE:
url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"

dataset = tf.keras.utils.get_file("aclImdb_v1", url,
                                    untar=True, cache_dir='.',
                                    cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')

----------------------------------------

TITLE: Building Approximate Nearest Neighbors Index with Annoy
DESCRIPTION: Implements a function to build an Approximate Nearest Neighbors index using the Annoy library based on the generated text embeddings.

LANGUAGE: Python
CODE:
def build_index(embedding_files_pattern, index_filename, vector_length, 
    metric='angular', num_trees=100):
  annoy_index = annoy.AnnoyIndex(vector_length, metric=metric)
  mapping = {}

  embed_files = tf.gfile.Glob(embedding_files_pattern)
  print('Found {} embedding file(s).'.format(len(embed_files)))

  item_counter = 0
  for f, embed_file in enumerate(embed_files):
    print('Loading embeddings in file {} of {}...'.format(
      f+1, len(embed_files)))
    record_iterator = tf.io.tf_record_iterator(
      path=embed_file)

    for string_record in record_iterator:
      example = tf.train.Example()
      example.ParseFromString(string_record)
      text = example.features.feature['text'].bytes_list.value[0].decode("utf-8")
      mapping[item_counter] = text
      embedding = np.array(
        example.features.feature['embedding'].float_list.value)
      annoy_index.add_item(item_counter, embedding)
      item_counter += 1
      if item_counter % 100000 == 0:
        print('{} items loaded to the index'.format(item_counter))

  print('A total of {} items added to the index'.format(item_counter))

  print('Building the index with {} trees...'.format(num_trees))
  annoy_index.build(n_trees=num_trees)
  print('Index is successfully built.')
  
  print('Saving index to disk...')
  annoy_index.save(index_filename)
  print('Index is saved to disk.')
  print("Index file size: {} GB".format(
    round(os.path.getsize(index_filename) / float(1024 ** 3), 2)))
  annoy_index.unload()

  print('Saving mapping to disk...')
  with open(index_filename + '.mapping', 'wb') as handle:
    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)
  print('Mapping is saved to disk.')
  print("Mapping file size: {} MB".format(
    round(os.path.getsize(index_filename + '.mapping') / float(1024 ** 2), 2)))

----------------------------------------

TITLE: Saving Model Checkpoints During Training in TensorFlow
DESCRIPTION: Sets up a ModelCheckpoint callback to save model weights during training, and trains the model with this callback.

LANGUAGE: python
CODE:
checkpoint_path = "training_1/cp.weights.h5"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

# Train the model with the new callback
model.fit(train_images,
          train_labels,
          epochs=10,
          validation_data=(test_images, test_labels),
          callbacks=[cp_callback])  # Pass callback to training

----------------------------------------

TITLE: Cleaning and preprocessing Auto MPG dataset in Python
DESCRIPTION: Removes rows with missing values, converts 'Origin' column to categorical, and applies one-hot encoding.

LANGUAGE: Python
CODE:
dataset = dataset.dropna()

dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})

dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='', dtype=float)

----------------------------------------

TITLE: Building MoViNet Classifier Model
DESCRIPTION: Function to build a classifier model using a MoViNet backbone with specified parameters

LANGUAGE: Python
CODE:
def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):
  """Builds a classifier on top of a backbone model."""
  model = movinet_model.MovinetClassifier(
      backbone=backbone,
      num_classes=num_classes)
  model.build([batch_size, num_frames, resolution, resolution, 3])

  return model

----------------------------------------

TITLE: Dynamic RNN Implementation
DESCRIPTION: Shows how to use TensorArray for accumulating values in a loop within tf.function

LANGUAGE: Python
CODE:
@tf.function
def dynamic_rnn(rnn_step, input_data, initial_state):
  input_data = tf.transpose(input_data, [1, 0, 2])
  max_seq_len = input_data.shape[0]

  states = tf.TensorArray(tf.float32, size=max_seq_len)
  state = initial_state
  for i in tf.range(max_seq_len):
    state = rnn_step(input_data[i], state)
    states = states.write(i, state)
  return tf.transpose(states.stack(), [1, 0, 2])

----------------------------------------

TITLE: Loading and Preparing CIFAR10 Dataset
DESCRIPTION: This code loads the CIFAR10 dataset and normalizes the pixel values to be between 0 and 1.

LANGUAGE: Python
CODE:
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

----------------------------------------

TITLE: Building Approximate Nearest Neighbors Index with Annoy
DESCRIPTION: Implements a function to build an Approximate Nearest Neighbors index using the Annoy library based on the generated text embeddings.

LANGUAGE: Python
CODE:
def build_index(embedding_files_pattern, index_filename, vector_length, 
    metric='angular', num_trees=100):
  annoy_index = annoy.AnnoyIndex(vector_length, metric=metric)
  mapping = {}

  embed_files = tf.gfile.Glob(embedding_files_pattern)
  print('Found {} embedding file(s).'.format(len(embed_files)))

  item_counter = 0
  for f, embed_file in enumerate(embed_files):
    print('Loading embeddings in file {} of {}...'.format(
      f+1, len(embed_files)))
    record_iterator = tf.io.tf_record_iterator(
      path=embed_file)

    for string_record in record_iterator:
      example = tf.train.Example()
      example.ParseFromString(string_record)
      text = example.features.feature['text'].bytes_list.value[0].decode("utf-8")
      mapping[item_counter] = text
      embedding = np.array(
        example.features.feature['embedding'].float_list.value)
      annoy_index.add_item(item_counter, embedding)
      item_counter += 1
      if item_counter % 100000 == 0:
        print('{} items loaded to the index'.format(item_counter))

  print('A total of {} items added to the index'.format(item_counter))

  print('Building the index with {} trees...'.format(num_trees))
  annoy_index.build(n_trees=num_trees)
  print('Index is successfully built.')
  
  print('Saving index to disk...')
  annoy_index.save(index_filename)
  print('Index is saved to disk.')
  print("Index file size: {} GB".format(
    round(os.path.getsize(index_filename) / float(1024 ** 3), 2)))
  annoy_index.unload()

  print('Saving mapping to disk...')
  with open(index_filename + '.mapping', 'wb') as handle:
    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)
  print('Mapping is saved to disk.')
  print("Mapping file size: {} MB".format(
    round(os.path.getsize(index_filename + '.mapping') / float(1024 ** 2), 2)))

----------------------------------------

TITLE: Defining Visualization Macros for SNGP Results
DESCRIPTION: Sets up visualization parameters and color maps for plotting SNGP results.

LANGUAGE: Python
CODE:
plt.rcParams['figure.dpi'] = 140

DEFAULT_X_RANGE = (-3.5, 3.5)
DEFAULT_Y_RANGE = (-2.5, 2.5)
DEFAULT_CMAP = colors.ListedColormap(["#377eb8", "#ff7f00"])
DEFAULT_NORM = colors.Normalize(vmin=0, vmax=1,)
DEFAULT_N_GRID = 100

----------------------------------------

TITLE: Saving and Loading ExtensionTypes with SavedModel in TensorFlow
DESCRIPTION: Shows how to save and load models that use extension types using SavedModel, including both Keras models and custom tf.Module subclasses.

LANGUAGE: Python
CODE:
masked_tensor_model_path = tempfile.mkdtemp()
tf.saved_model.save(masked_tensor_model, masked_tensor_model_path)
imported_model = tf.saved_model.load(masked_tensor_model_path)

class CustomModule(tf.Module):
  def __init__(self, variable_value):
    super().__init__()
    self.v = tf.Variable(variable_value)

  @tf.function
  def grow(self, x: MaskedTensor):
    return MaskedTensor(x.values * self.v, x.mask)

module = CustomModule(100.0)
module.grow.get_concrete_function(MaskedTensor.Spec(shape=None, dtype=tf.float32))
custom_module_path = tempfile.mkdtemp()
tf.saved_model.save(module, custom_module_path)
imported_model = tf.saved_model.load(custom_module_path)

----------------------------------------

TITLE: Creating Style Transfer Model Class
DESCRIPTION: Defines the StyleContentModel class that extracts style and content features using VGG19

LANGUAGE: Python
CODE:
class StyleContentModel(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super(StyleContentModel, self).__init__()
    self.vgg = vgg_layers(style_layers + content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False

  def call(self, inputs):
    "Expects float input in [0,1]"
    inputs = inputs*255.0
    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs, content_outputs = (outputs[:self.num_style_layers],
                                      outputs[self.num_style_layers:])

    style_outputs = [gram_matrix(style_output)
                     for style_output in style_outputs]

    content_dict = {content_name: value
                    for content_name, value
                    in zip(self.content_layers, content_outputs)}

    style_dict = {style_name: value
                  for style_name, value
                  in zip(self.style_layers, style_outputs)}

    return {'content': content_dict, 'style': style_dict}

----------------------------------------

TITLE: Implementing Loss Functions
DESCRIPTION: Defines loss functions for the discriminator, generator, cycle consistency, and identity preservation.

LANGUAGE: Python
CODE:
LAMBDA = 10

loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real, generated):
  real_loss = loss_obj(tf.ones_like(real), real)
  generated_loss = loss_obj(tf.zeros_like(generated), generated)
  total_disc_loss = real_loss + generated_loss
  return total_disc_loss * 0.5

def generator_loss(generated):
  return loss_obj(tf.ones_like(generated), generated)

def calc_cycle_loss(real_image, cycled_image):
  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))
  return LAMBDA * loss1

def identity_loss(real_image, same_image):
  loss = tf.reduce_mean(tf.abs(real_image - same_image))
  return LAMBDA * 0.5 * loss

----------------------------------------

TITLE: Implementing CSV Data Normalization
DESCRIPTION: Adding data normalization layer to preprocess CSV input features

LANGUAGE: Python
CODE:
normalize = layers.Normalization()
normalize.adapt(abalone_features)

norm_abalone_model = tf.keras.Sequential([
  normalize,
  layers.Dense(64, activation='relu'),
  layers.Dense(1)
])

----------------------------------------

TITLE: Implementing MNIST Compression Trainer Model
DESCRIPTION: Defines a Keras model for training a compressor/decompressor for MNIST images, computing rate and distortion losses.

LANGUAGE: Python
CODE:
class MNISTCompressionTrainer(tf.keras.Model):
  """Model that trains a compressor/decompressor for MNIST."""

  def __init__(self, latent_dims):
    super().__init__()
    self.analysis_transform = make_analysis_transform(latent_dims)
    self.synthesis_transform = make_synthesis_transform()
    self.prior_log_scales = tf.Variable(tf.zeros((latent_dims,)))

  @property
  def prior(self):
    return tfc.NoisyLogistic(loc=0., scale=tf.exp(self.prior_log_scales))

  def call(self, x, training):
    """Computes rate and distortion losses."""
    # Ensure inputs are floats in the range (0, 1).
    x = tf.cast(x, self.compute_dtype) / 255.
    x = tf.reshape(x, (-1, 28, 28, 1))

    # Compute latent space representation y, perturb it and model its entropy,
    # then compute the reconstructed pixel-level representation x_hat.
    y = self.analysis_transform(x)
    entropy_model = tfc.ContinuousBatchedEntropyModel(
        self.prior, coding_rank=1, compression=False)
    y_tilde, rate = entropy_model(y, training=training)
    x_tilde = self.synthesis_transform(y_tilde)

    # Average number of bits per MNIST digit.
    rate = tf.reduce_mean(rate)

    # Mean absolute difference across pixels.
    distortion = tf.reduce_mean(abs(x - x_tilde))

    return dict(rate=rate, distortion=distortion)


----------------------------------------

TITLE: Visualizing SNGP Uncertainty Results
DESCRIPTION: Plots the class probabilities and predictive uncertainties of the SNGP model.

LANGUAGE: Python
CODE:
def plot_predictions(pred_probs, model_name=""):
  """Plot normalized class probabilities and predictive uncertainties."""
  # Compute predictive uncertainty.
  uncertainty = pred_probs * (1. - pred_probs)

  # Initialize the plot axes.
  fig, axs = plt.subplots(1, 2, figsize=(14, 5))

  # Plots the class probability.
  pcm_0 = plot_uncertainty_surface(pred_probs, ax=axs[0])
  # Plots the predictive uncertainty.
  pcm_1 = plot_uncertainty_surface(uncertainty, ax=axs[1])

  # Adds color bars and titles.
  fig.colorbar(pcm_0, ax=axs[0])
  fig.colorbar(pcm_1, ax=axs[1])

  axs[0].set_title(f"Class Probability, {model_name}")
  axs[1].set_title(f"(Normalized) Predictive Uncertainty, {model_name}")

  plt.show()

----------------------------------------

TITLE: Using Ragged Tensors with tf.data Datasets
DESCRIPTION: Demonstrates how to create and manipulate datasets containing ragged tensors.

LANGUAGE: python
CODE:
dataset = tf.data.Dataset.from_tensor_slices(feature_tensors)
print_dictionary_dataset(dataset)

batched_dataset = dataset.batch(2)
print_dictionary_dataset(batched_dataset)

unbatched_dataset = batched_dataset.unbatch()
print_dictionary_dataset(unbatched_dataset)

def transform_lengths(features):
  return {
      'mean_length': tf.math.reduce_mean(features['lengths']),
      'length_ranges': tf.ragged.range(features['lengths'])}
transformed_dataset = dataset.map(transform_lengths)
print_dictionary_dataset(transformed_dataset)

----------------------------------------

TITLE: Setting up TensorFlow and Required Libraries
DESCRIPTION: Imports necessary Python libraries including TensorFlow, Keras, NumPy and matplotlib for building the image classification model.

LANGUAGE: Python
CODE:
import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

----------------------------------------

TITLE: Creating a Flexible Keras Layer
DESCRIPTION: Defines a flexible Keras layer that uses the build method to create variables, allowing for dynamic input shapes.

LANGUAGE: Python
CODE:
class FlexibleDense(tf.keras.layers.Layer):
  # Note the added `**kwargs`, as Keras supports many arguments
  def __init__(self, out_features, **kwargs):
    super().__init__(**kwargs)
    self.out_features = out_features

  def build(self, input_shape):  # Create the state of the layer (weights)
    self.w = tf.Variable(
      tf.random.normal([input_shape[-1], self.out_features]), name='w')
    self.b = tf.Variable(tf.zeros([self.out_features]), name='b')

  def call(self, inputs):  # Defines the computation from inputs to outputs
    return tf.matmul(inputs, self.w) + self.b

# Create the instance of the layer
flexible_dense = FlexibleDense(out_features=3)

----------------------------------------

TITLE: Saving and reloading TensorFlow model in Python
DESCRIPTION: Demonstrates how to save a trained TensorFlow model and reload it for later use.

LANGUAGE: Python
CODE:
dnn_model.save('dnn_model.keras')

reloaded = tf.keras.models.load_model('dnn_model.keras')

test_results['reloaded'] = reloaded.evaluate(test_features, test_labels, verbose=0)

----------------------------------------

TITLE: Installing Dependencies
DESCRIPTION: Installs required scikit-image package for image processing and feature matching visualization

LANGUAGE: python
CODE:
!pip install scikit-image

----------------------------------------

TITLE: Defining Actor-Critic Model Architecture
DESCRIPTION: Creates a combined Actor-Critic neural network model using TensorFlow's Keras API. The model takes the state as input and outputs action probabilities and critic value.

LANGUAGE: Python
CODE:
class ActorCritic(tf.keras.Model):
  """Combined actor-critic network."""

  def __init__(
      self,
      num_actions: int,
      num_hidden_units: int):
    """Initialize."""
    super().__init__()

    self.common = layers.Dense(num_hidden_units, activation="relu")
    self.actor = layers.Dense(num_actions)
    self.critic = layers.Dense(1)

  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
    x = self.common(inputs)
    return self.actor(x), self.critic(x)

num_actions = env.action_space.n  # 2
num_hidden_units = 128

model = ActorCritic(num_actions, num_hidden_units)

----------------------------------------

TITLE: Adding Dropout Layers to Model in TensorFlow Keras
DESCRIPTION: Defines a new model with Dropout layers added after each Dense layer to prevent overfitting by randomly dropping out units during training.

LANGUAGE: Python
CODE:
dpt_model = keras.models.Sequential([
    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(16, activation=tf.nn.relu),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1, activation=tf.nn.sigmoid)
])

dpt_model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy','binary_crossentropy'])

----------------------------------------

TITLE: Implementing Image Compression Function
DESCRIPTION: Defines a function to compress an image using SVD and low-rank approximation.

LANGUAGE: Python
CODE:
def compress_image(I, r, verbose=False):
  # Compress an image with the SVD given a rank 
  I_size = tf.size(I)
  print(f"Original size of image: {I_size}")
  # Compute SVD of image
  I = tf.convert_to_tensor(I)/255
  I_batched = tf.transpose(I, [2, 0, 1]) # einops.rearrange(I, 'h w c -> c h w')
  s, U, V = tf.linalg.svd(I_batched)
  # Compute low-rank approximation of image across each RGB channel
  I_r, I_r_size = rank_r_approx(s, U, V, r)
  I_r = tf.transpose(I_r, [1, 2, 0]) # einops.rearrange(I_r, 'c h w -> h w c')
  I_r_prop = (I_r_size / I_size)
  if verbose:
    # Display compressed image and attributes
    print(f"Number of singular values used in compression: {r}")
    print(f"Compressed image size: {I_r_size}")
    print(f"Proportion of original size: {I_r_prop:.3f}")
    ax_1 = plt.subplot(1,2,1)
    show_img(tf.clip_by_value(I_r,0.,1.))
    ax_1.set_title("Approximated image")
    ax_2 = plt.subplot(1,2,2)
    show_img(tf.clip_by_value(0.5+abs(I-I_r),0.,1.))
    ax_2.set_title("Error")
  return I_r, I_r_prop

----------------------------------------

TITLE: Creating WindowGenerator Class for Data Windowing
DESCRIPTION: Defines a WindowGenerator class to handle the creation of input-output windows for time series data, including methods for splitting data and plotting.

LANGUAGE: Python
CODE:
class WindowGenerator():
  def __init__(self, input_width, label_width, shift,
               train_df=train_df, val_df=val_df, test_df=test_df,
               label_columns=None):
    # Store the raw data.
    self.train_df = train_df
    self.val_df = val_df
    self.test_df = test_df

    # Work out the label column indices.
    self.label_columns = label_columns
    if label_columns is not None:
      self.label_columns_indices = {name: i for i, name in
                                    enumerate(label_columns)}
    self.column_indices = {name: i for i, name in
                           enumerate(train_df.columns)}

    # Work out the window parameters.
    self.input_width = input_width
    self.label_width = label_width
    self.shift = shift

    self.total_window_size = input_width + shift

    self.input_slice = slice(0, input_width)
    self.input_indices = np.arange(self.total_window_size)[self.input_slice]

    self.label_start = self.total_window_size - self.label_width
    self.labels_slice = slice(self.label_start, None)
    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

----------------------------------------

TITLE: Setting Up Checkpoint Objects
DESCRIPTION: Creates tf.train.Checkpoint and tf.train.CheckpointManager objects for managing checkpoints.

LANGUAGE: Python
CODE:
opt = tf.keras.optimizers.Adam(0.1)
dataset = toy_dataset()
iterator = iter(dataset)
ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net, iterator=iterator)
manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)

----------------------------------------

TITLE: Creating Two Moon Dataset for SNGP Training
DESCRIPTION: Generates a two moon dataset using scikit-learn for training the SNGP model.

LANGUAGE: Python
CODE:
def make_training_data(sample_size=500):
  """Create two moon training dataset."""
  train_examples, train_labels = sklearn.datasets.make_moons(
      n_samples=2 * sample_size, noise=0.1)

  # Adjust data position slightly.
  train_examples[train_labels == 0] += [-0.1, 0.2]
  train_examples[train_labels == 1] += [0.1, -0.2]

  return train_examples, train_labels

----------------------------------------

TITLE: Creating Input Function for TensorFlow Estimator in Python
DESCRIPTION: This function creates a TensorFlow Dataset from features and labels, with options for shuffling and batching. It's used to feed data to the Estimator for training and evaluation.

LANGUAGE: Python
CODE:
def input_fn(features, labels, training=True, batch_size=256):
    """An input function for training or evaluating"""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle and repeat if you are in training mode.
    if training:
        dataset = dataset.shuffle(1000).repeat()
    
    return dataset.batch(batch_size)

----------------------------------------

TITLE: Compiling the Model
DESCRIPTION: This snippet compiles the model, specifying the optimizer, loss function, and metrics to use during training.

LANGUAGE: Python
CODE:
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

----------------------------------------

TITLE: NumPy-TensorFlow Interoperability
DESCRIPTION: Shows conversion between NumPy arrays and TensorFlow tensors, demonstrating automatic type conversion and explicit conversions.

LANGUAGE: python
CODE:
import numpy as np

ndarray = np.ones([3, 3])

print("TensorFlow operations convert numpy arrays to Tensors automatically")
tensor = tf.multiply(ndarray, 42)
print(tensor)

print("And NumPy operations convert Tensors to numpy arrays automatically")
print(np.add(tensor, 1))

print("The .numpy() method explicitly converts a Tensor to a numpy array")
print(tensor.numpy())

----------------------------------------

TITLE: Importing Libraries for Semantic Search Pipeline
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Hub, Apache Beam, and ANNOY for building the semantic search system.

LANGUAGE: python
CODE:
import os
import sys
import pickle
from collections import namedtuple
from datetime import datetime
import numpy as np
import apache_beam as beam
from apache_beam.transforms import util
import tensorflow as tf
import tensorflow_hub as hub
import annoy
from sklearn.random_projection import gaussian_random_matrix

----------------------------------------

TITLE: Creating TensorFlow Variables with Different Data Types
DESCRIPTION: This code demonstrates how to create TensorFlow variables with various data types, including tensors, boolean values, and complex numbers.

LANGUAGE: Python
CODE:
my_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])
my_variable = tf.Variable(my_tensor)

# Variables can be all kinds of types, just like tensors
bool_variable = tf.Variable([False, False, False, True])
complex_variable = tf.Variable([5 + 4j, 6 + 1j])

----------------------------------------

TITLE: Using Ragged Tensors with tf.function
DESCRIPTION: Shows how to use ragged tensors with tf.function-decorated functions and concrete functions.

LANGUAGE: python
CODE:
@tf.function
def make_palindrome(x, axis):
  return tf.concat([x, tf.reverse(x, [axis])], axis)

make_palindrome(tf.constant([[1, 2], [3, 4], [5, 6]]), axis=1)
make_palindrome(tf.ragged.constant([[1, 2], [3], [4, 5, 6]]), axis=1)

@tf.function(
    input_signature=[tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32)])
def max_and_min(rt):
  return (tf.math.reduce_max(rt, axis=-1), tf.math.reduce_min(rt, axis=-1))

max_and_min(tf.ragged.constant([[1, 2], [3], [4, 5, 6]]))

----------------------------------------

TITLE: Making Predictions
DESCRIPTION: This code uses the trained model to make predictions on the test images.

LANGUAGE: Python
CODE:
predictions = model.predict(test_images)

----------------------------------------

TITLE: Defining a Hypertunable Model Builder Function
DESCRIPTION: Create a model builder function that defines the architecture and hyperparameters to be tuned.

LANGUAGE: python
CODE:
def model_builder(hp):
  model = keras.Sequential()
  model.add(keras.layers.Flatten(input_shape=(28, 28)))

  # Tune the number of units in the first Dense layer
  # Choose an optimal value between 32-512
  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
  model.add(keras.layers.Dense(units=hp_units, activation='relu'))
  model.add(keras.layers.Dense(10))

  # Tune the learning rate for the optimizer
  # Choose an optimal value from 0.01, 0.001, or 0.0001
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

  return model

----------------------------------------

TITLE: Implementing Gradient Descent Optimizer in TensorFlow
DESCRIPTION: Creates a basic gradient descent optimizer class with an initialization method and a function to update variables given gradients. The learning rate is configurable.

LANGUAGE: Python
CODE:
class GradientDescent(tf.Module):

  def __init__(self, learning_rate=1e-3):
    # Initialize parameters
    self.learning_rate = learning_rate
    self.title = f"Gradient descent optimizer: learning rate={self.learning_rate}"

  def apply_gradients(self, grads, vars):
    # Update variables
    for grad, var in zip(grads, vars):
      var.assign_sub(self.learning_rate*grad)

----------------------------------------

TITLE: Basic TensorFlow Function Definition
DESCRIPTION: Demonstrates how to define and use a basic tf.function decorator that adds two tensors

LANGUAGE: Python
CODE:
@tf.function
def add(a, b):
  return a + b

add(tf.ones([2, 2]), tf.ones([2, 2]))

----------------------------------------

TITLE: Importing Libraries in Python
DESCRIPTION: Imports necessary Python libraries including NumPy, Pandas, Matplotlib, and TensorFlow.

LANGUAGE: Python
CODE:
import os
import sys

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import clear_output
from six.moves import urllib

import tensorflow.compat.v2.feature_column as fc

import tensorflow as tf

----------------------------------------

TITLE: Opening TensorFlow Graph File
DESCRIPTION: Opens a TensorFlow graph file in binary read mode for processing.

LANGUAGE: python
CODE:
with open(FLAGS.graph, "rb") as f:

----------------------------------------

TITLE: Evaluating Model Performance and Displaying Confusion Matrix
DESCRIPTION: This code evaluates the trained model on the test dataset and displays a confusion matrix to visualize classification performance.

LANGUAGE: Python
CODE:
model.evaluate(test_spectrogram_ds, return_dict=True)

y_pred = model.predict(test_spectrogram_ds)
y_pred = tf.argmax(y_pred, axis=1)
y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)

confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_mtx,
            xticklabels=label_names,
            yticklabels=label_names,
            annot=True, fmt='g')
plt.xlabel('Prediction')
plt.ylabel('Label')
plt.show()

----------------------------------------

TITLE: Processing Input Tensors for RNN Model
DESCRIPTION: Function to extract and reshape input features, including ink data and sequence lengths, from the TensorFlow feature dictionary.

LANGUAGE: Python
CODE:
shapes = features["shape"]
lengths = tf.squeeze(
    tf.slice(shapes, begin=[0, 0], size=[params["batch_size"], 1]))
inks = tf.reshape(
    tf.sparse_tensor_to_dense(features["ink"]),
    [params["batch_size"], -1, 3])
if targets is not None:
  targets = tf.squeeze(targets)

----------------------------------------

TITLE: Implementing U-Net Generator Architecture
DESCRIPTION: Generator model implementation using a U-Net architecture with skip connections between encoder and decoder layers.

LANGUAGE: Python
CODE:
def Generator():
  inputs = tf.keras.layers.Input(shape=[256, 256, 3])

  down_stack = [
    downsample(64, 4, apply_batchnorm=False),
    downsample(128, 4),
    downsample(256, 4),
    downsample(512, 4),
    downsample(512, 4),
    downsample(512, 4),
    downsample(512, 4),
    downsample(512, 4),
  ]

  up_stack = [
    upsample(512, 4, apply_dropout=True),
    upsample(512, 4, apply_dropout=True),
    upsample(512, 4, apply_dropout=True),
    upsample(512, 4),
    upsample(256, 4),
    upsample(128, 4),
    upsample(64, 4),
  ]

  initializer = tf.random_normal_initializer(0., 0.02)
  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                         strides=2,
                                         padding='same',
                                         kernel_initializer=initializer,
                                         activation='tanh')

  x = inputs

  # Downsampling through the model
  skips = []
  for down in down_stack:
    x = down(x)
    skips.append(x)

  skips = reversed(skips[:-1])

  # Upsampling and establishing the skip connections
  for up, skip in zip(up_stack, skips):
    x = up(x)
    x = tf.keras.layers.Concatenate()([x, skip])

  x = last(x)

  return tf.keras.Model(inputs=inputs, outputs=x)

----------------------------------------

TITLE: Calculating Jacobian Matrix in TensorFlow
DESCRIPTION: Show how to compute the Jacobian matrix using tf.GradientTape.jacobian method for vector-valued functions.

LANGUAGE: Python
CODE:
x = tf.linspace(-10.0, 10.0, 200+1)
delta = tf.Variable(0.0)

with tf.GradientTape() as tape:
  y = tf.nn.sigmoid(x+delta)

dy_dx = tape.jacobian(y, delta)

print(y.shape)
print(dy_dx.shape)

plt.plot(x.numpy(), y, label='y')
plt.plot(x.numpy(), dy_dx, label='dy/dx')
plt.legend()
_ = plt.xlabel('x')

----------------------------------------

TITLE: Creating TensorFlow Datasets from NumPy Arrays
DESCRIPTION: Converts NumPy arrays into TensorFlow Dataset objects using from_tensor_slices().

LANGUAGE: python
CODE:
train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))

----------------------------------------

TITLE: Importing Required Libraries for YAMNet Sound Classification
DESCRIPTION: This snippet imports the necessary Python libraries for working with TensorFlow, TensorFlow Hub, audio processing, and visualization.

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import csv

import matplotlib.pyplot as plt
from IPython.display import Audio
from scipy.io import wavfile

----------------------------------------

TITLE: Installing TensorFlow on MacOS
DESCRIPTION: Commands to install CPU-only TensorFlow on MacOS systems using pip, including verification steps.

LANGUAGE: bash
CODE:
python3 -m pip install tensorflow
python3 -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"

----------------------------------------

TITLE: Building Neural Network Model Architecture
DESCRIPTION: Creates a sequential neural network with flattening layer and two dense layers for image classification.

LANGUAGE: python
CODE:
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])

----------------------------------------

TITLE: Function Tracing Demonstration
DESCRIPTION: Illustrates how tf.function traces and caches functions based on input types

LANGUAGE: Python
CODE:
@tf.function
def double(a):
  print("Tracing with", a)
  return a + a

print(double(tf.constant(1)))
print(double(tf.constant(1.1)))
print(double(tf.constant("a")))

----------------------------------------

TITLE: Extracting Encoder Features
DESCRIPTION: Computes and displays the shapes of encoder features used for representation learning evaluations.

LANGUAGE: Python
CODE:
_out_features = sess.run(enc_features, feed_dict={enc_ph: test_images_batch})
print('AvePool features shape:', _out_features['avepool_feat'].shape)
print('BN+CReLU features shape:', _out_features['bn_crelu_feat'].shape)

----------------------------------------

TITLE: Generating and Visualizing CVAE Latent Space
DESCRIPTION: Implements a function to visualize the 2D latent space of the trained CVAE model by generating a grid of interpolated digits.

LANGUAGE: Python
CODE:
def plot_latent_images(model, n, digit_size=28):
  norm = tfp.distributions.Normal(0, 1)
  grid_x = norm.quantile(np.linspace(0.05, 0.95, n))
  grid_y = norm.quantile(np.linspace(0.05, 0.95, n))
  image_width = digit_size*n
  image_height = image_width
  image = np.zeros((image_height, image_width))

  for i, yi in enumerate(grid_x):
    for j, xi in enumerate(grid_y):
      z = np.array([[xi, yi]])
      x_decoded = model.sample(z)
      digit = tf.reshape(x_decoded[0], (digit_size, digit_size))
      image[i * digit_size: (i + 1) * digit_size,
            j * digit_size: (j + 1) * digit_size] = digit.numpy()

  plt.figure(figsize=(10, 10))
  plt.imshow(image, cmap='Greys_r')
  plt.axis('Off')
  plt.show()

plot_latent_images(model, 20)

----------------------------------------

TITLE: Custom Model Implementation with Dictionary Inputs
DESCRIPTION: Implements a custom model class that handles dictionary inputs and performs preprocessing.

LANGUAGE: Python
CODE:
class MyModel(tf.keras.Model):
  def __init__(self):
    super().__init__()
    self.normalizer = tf.keras.layers.Normalization(axis=-1)
    self.seq = tf.keras.Sequential([
      self.normalizer,
      tf.keras.layers.Dense(10, activation='relu'),
      tf.keras.layers.Dense(10, activation='relu'),
      tf.keras.layers.Dense(1)
    ])
    self.concat = tf.keras.layers.Concatenate(axis=1)

  def _stack(self, input_dict):
    values = []
    for key, value in sorted(input_dict.items()):
      values.append(value)
    return self.concat(values)

  def adapt(self, inputs):
    inputs = self._stack(inputs)
    self.normalizer.adapt(inputs)

  def call(self, inputs):
    inputs = self._stack(inputs)
    result = self.seq(inputs)
    return result

----------------------------------------

TITLE: Creating and training linear regression model with TensorFlow
DESCRIPTION: Builds a linear regression model using TensorFlow's Keras API, compiles it, and trains it on the dataset.

LANGUAGE: Python
CODE:
linear_model = tf.keras.Sequential([
    normalizer,
    layers.Dense(units=1)
])

linear_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

history = linear_model.fit(
    train_features,
    train_labels,
    epochs=100,
    verbose=0,
    validation_split = 0.2)

----------------------------------------

TITLE: Creating and training linear regression model with TensorFlow
DESCRIPTION: Builds a linear regression model using TensorFlow's Keras API, compiles it, and trains it on the dataset.

LANGUAGE: Python
CODE:
linear_model = tf.keras.Sequential([
    normalizer,
    layers.Dense(units=1)
])

linear_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

history = linear_model.fit(
    train_features,
    train_labels,
    epochs=100,
    verbose=0,
    validation_split = 0.2)

----------------------------------------

TITLE: Importing Libraries
DESCRIPTION: Importing required Python libraries including TensorFlow, OpenCV, NumPy and other utilities

LANGUAGE: Python
CODE:
import tqdm
import random
import pathlib
import itertools
import collections

import cv2
import numpy as np
import remotezip as rz
import seaborn as sns
import matplotlib.pyplot as plt

import keras
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy

from official.projects.movinet.modeling import movinet
from official.projects.movinet.modeling import movinet_model

----------------------------------------

TITLE: Implementing Training Step and Loop
DESCRIPTION: Defines the training step function and runs the training loop until the success criterion or maximum number of episodes is reached.

LANGUAGE: Python
CODE:
@tf.function
def train_step(
    initial_state: tf.Tensor,
    model: tf.keras.Model,
    optimizer: tf.keras.optimizers.Optimizer,
    gamma: float,
    max_steps_per_episode: int) -> tf.Tensor:
  """Runs a model training step."""

  with tf.GradientTape() as tape:
    # Run the model for one episode to collect training data
    action_probs, values, rewards = run_episode(
        initial_state, model, max_steps_per_episode)

    # Calculate the expected returns
    returns = get_expected_return(rewards, gamma)

    # Convert training data to appropriate TF tensor shapes
    action_probs, values, returns = [
        tf.expand_dims(x, 1) for x in [action_probs, values, returns]]

    # Calculate the loss values to update our network
    loss = compute_loss(action_probs, values, returns)

  # Compute the gradients from the loss
  grads = tape.gradient(loss, model.trainable_variables)

  # Apply the gradients to the model's parameters
  optimizer.apply_gradients(zip(grads, model.trainable_variables))

  episode_reward = tf.math.reduce_sum(rewards)

  return episode_reward

# Training loop
min_episodes_criterion = 100
max_episodes = 10000
max_steps_per_episode = 500
reward_threshold = 475
running_reward = 0
gamma = 0.99
episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)

t = tqdm.trange(max_episodes)
for i in t:
    initial_state, info = env.reset()
    initial_state = tf.constant(initial_state, dtype=tf.float32)
    episode_reward = int(train_step(
        initial_state, model, optimizer, gamma, max_steps_per_episode))

    episodes_reward.append(episode_reward)
    running_reward = statistics.mean(episodes_reward)

    t.set_postfix(
        episode_reward=episode_reward, running_reward=running_reward)

    if running_reward > reward_threshold and i >= min_episodes_criterion:
        break

print(f'\nSolved at episode {i}: average reward: {running_reward:.2f}!')

----------------------------------------

TITLE: Defining and Compiling Keras Model
DESCRIPTION: Creates and compiles a Keras Sequential model for text classification using the embedding layer.

LANGUAGE: Python
CODE:
def create_model():
  model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=[], dtype=tf.string),
    embedding_layer,
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dense(16, activation="relu"),
    tf.keras.layers.Dense(5),
  ])
  model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
      optimizer="adam", metrics=['accuracy'])
  return model

model = create_model()
early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3)

----------------------------------------

TITLE: Optimizing Dataset Performance with Prefetch
DESCRIPTION: Demonstrates using the prefetch transformation to overlap data production and consumption.

LANGUAGE: Python
CODE:
benchmark(
    ArtificialDataset()
    .prefetch(tf.data.AUTOTUNE)
)

----------------------------------------

TITLE: Reading TFRecord File Using tf.data in TensorFlow
DESCRIPTION: Demonstrates how to read a TFRecord file using tf.data.TFRecordDataset and parse the tf.train.Example messages.

LANGUAGE: Python
CODE:
feature_description = {
    'feature0': tf.io.FixedLenFeature([], tf.int64, default_value=0),
    'feature1': tf.io.FixedLenFeature([], tf.int64, default_value=0),
    'feature2': tf.io.FixedLenFeature([], tf.string, default_value=''),
    'feature3': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),
}

def _parse_function(example_proto):
  # Parse the input `tf.train.Example` proto using the dictionary above.
  return tf.io.parse_single_example(example_proto, feature_description)

filenames = [filename]
raw_dataset = tf.data.TFRecordDataset(filenames)
parsed_dataset = raw_dataset.map(_parse_function)

----------------------------------------

TITLE: Basic TensorFlow Data Pipeline Construction
DESCRIPTION: Demonstrates creation of a basic tf.data pipeline with range, map, repeat, and batch transformations.

LANGUAGE: python
CODE:
dataset = tf.data.Dataset.range(10)
dataset = dataset.map(lambda x: x)
dataset = dataset.repeat(2)
dataset = dataset.batch(5)

----------------------------------------

TITLE: Referencing Core TensorFlow Image Modules
DESCRIPTION: Key TensorFlow modules and utilities for image processing, including tf.image for low-level operations and tf.keras.utils for high-level preprocessing.

LANGUAGE: python
CODE:
tf.image.flip_left_right
tf.image.rgb_to_grayscale
tf.image.adjust_brightness
tf.image.central_crop
tf.image.stateless_random*
tf.keras.utils.image_dataset_from_directory

----------------------------------------

TITLE: Interpolating Between Latent Vectors in S3GAN
DESCRIPTION: This code demonstrates interpolation between two latent vectors in the S3GAN model's latent space. It generates a sequence of images showing smooth transitions between two random points.

LANGUAGE: Python
CODE:
num_samples = 1  # @param {type: "slider", min: 1, max: 6, step: 1}
num_interps = 6  # @param {type: "slider", min: 2, max: 10, step: 1}
noise_seed_A = 11  # @param {type: "slider", min: 0, max: 100, step: 1}
noise_seed_B = 0  # @param {type: "slider", min: 0, max: 100, step: 1}
label_str = "1) goldfish, Carassius auratus"  # @param [...]

def interpolate(A, B, num_interps):
    # Function implementation...

def interpolate_and_shape(A, B, num_interps):
    # Function implementation...

label = int(label_str.split(')')[0])
labels = np.asarray([label] * num_samples * num_interps)

z_A = sampler.get_noise(num_samples, seed=noise_seed_A)
z_B = sampler.get_noise(num_samples, seed=noise_seed_B)
z = interpolate_and_shape(z_A, z_B, num_interps)

samples = sampler.get_samples(z, labels)
imshow(imgrid(samples, cols=num_interps))

----------------------------------------

TITLE: Preprocessing Data with Dataset.map()
DESCRIPTION: Example showing how to preprocess data using Dataset.map() transform to parse TFRecord examples

LANGUAGE: python
CODE:
def _parse_function(example_proto):
  features = {"image": tf.FixedLenFeature((), tf.string, default_value=""),
              "label": tf.FixedLenFeature((), tf.int64, default_value=0)}
  parsed_features = tf.parse_single_example(example_proto, features)
  return parsed_features["image"], parsed_features["label"]

filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(_parse_function)

----------------------------------------

TITLE: Training Step Implementation
DESCRIPTION: Defines the training step function that performs one optimization iteration

LANGUAGE: Python
CODE:
@tf.function()
def train_step(image):
  with tf.GradientTape() as tape:
    outputs = extractor(image)
    loss = style_content_loss(outputs)
    loss += total_variation_weight*tf.image.total_variation(image)

  grad = tape.gradient(loss, image)
  opt.apply_gradients([(grad, image)])
  image.assign(clip_0_1(image))

----------------------------------------

TITLE: Loading Multilingual Universal Sentence Encoder Model
DESCRIPTION: Loads the pre-trained multilingual encoder model from TensorFlow Hub and defines a helper function for text embedding.

LANGUAGE: python
CODE:
module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'
model = hub.load(module_url)

def embed_text(input):
  return model(input)

----------------------------------------

TITLE: Defining Input Functions for TensorFlow Estimator
DESCRIPTION: Creates input functions for training, evaluation, and prediction using the SciCite dataset. These functions prepare the data for use with a TensorFlow Estimator.

LANGUAGE: python
CODE:
def preprocessed_input_fn(for_eval):
  data = THE_DATASET.get_data(for_eval=for_eval)
  data = data.map(THE_DATASET.example_fn, num_parallel_calls=1)
  return data


def input_fn_train(params):
  data = preprocessed_input_fn(for_eval=False)
  data = data.repeat(None)
  data = data.shuffle(1024)
  data = data.batch(batch_size=params['batch_size'])
  return data


def input_fn_eval(params):
  data = preprocessed_input_fn(for_eval=True)
  data = data.repeat(1)
  data = data.batch(batch_size=params['batch_size'])
  return data


def input_fn_predict(params):
  data = preprocessed_input_fn(for_eval=True)
  data = data.batch(batch_size=params['batch_size'])
  return data

----------------------------------------

TITLE: Implementing Input Function for TensorFlow Estimator
DESCRIPTION: Python function that creates an input pipeline for training a TensorFlow Estimator, using tf.data.Dataset.

LANGUAGE: python
CODE:
def train_input_fn(features, labels, batch_size):
    """An input function for training"""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)

    # Return the dataset.
    return dataset

----------------------------------------

TITLE: Implementing Model Training Pipeline
DESCRIPTION: Defines reusable functions for model compilation and training with early stopping and TensorBoard logging

LANGUAGE: Python
CODE:
def compile_and_fit(model, name, optimizer=None, max_epochs=10000):
  if optimizer is None:
    optimizer = get_optimizer()
  model.compile(optimizer=optimizer,
                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                metrics=[
                  tf.keras.metrics.BinaryCrossentropy(
                      from_logits=True, name='binary_crossentropy'),
                  'accuracy'])

  model.summary()

  history = model.fit(
    train_ds.map(lambda x, y: (x, tf.expand_dims(y, axis=-1))),
    steps_per_epoch = STEPS_PER_EPOCH,
    epochs=max_epochs,
    validation_data=validate_ds.map(lambda x, y: (x, tf.expand_dims(y, axis=-1))),
    callbacks=get_callbacks(name),
    verbose=0)
  return history

----------------------------------------

TITLE: Creating MNIST Classifier Model
DESCRIPTION: Defines a sequential model architecture for MNIST classification using custom layers.

LANGUAGE: python
CODE:
classifier = tf.keras.Sequential([
    CustomConv2D(20, 5, strides=2, name="conv_1"),
    CustomConv2D(50, 5, strides=2, name="conv_2"),
    tf.keras.layers.Flatten(),
    CustomDense(500, name="fc_1"),
    CustomDense(10, name="fc_2"),
], name="classifier")

----------------------------------------

TITLE: Converting MIDI File to Note Dataframe
DESCRIPTION: Defines a function to extract note information from a MIDI file and return it as a pandas DataFrame.

LANGUAGE: Python
CODE:
def midi_to_notes(midi_file: str) -> pd.DataFrame:
  pm = pretty_midi.PrettyMIDI(midi_file)
  instrument = pm.instruments[0]
  notes = collections.defaultdict(list)

  # Sort the notes by start time
  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)
  prev_start = sorted_notes[0].start

  for note in sorted_notes:
    start = note.start
    end = note.end
    notes['pitch'].append(note.pitch)
    notes['start'].append(start)
    notes['end'].append(end)
    notes['step'].append(start - prev_start)
    notes['duration'].append(end - start)
    prev_start = start

  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})

----------------------------------------

TITLE: Testing Audio Classification
DESCRIPTION: Command to test the frozen model by classifying a specific audio file and outputting confidence scores for each label.

LANGUAGE: bash
CODE:
python tensorflow/examples/speech_commands/label_wav.py \
--graph=/tmp/my_frozen_graph.pb \
--labels=/tmp/speech_commands_train/conv_labels.txt \
--wav=/tmp/speech_dataset/left/a5d485dc_nohash_0.wav

----------------------------------------

TITLE: Converting Pitch Values to Musical Notes
DESCRIPTION: Functions to convert raw pitch values to musical notes with quantization and error correction

LANGUAGE: python
CODE:
def quantize_predictions(group, ideal_offset):
  non_zero_values = [v for v in group if v != 0]
  zero_values_count = len(group) - len(non_zero_values)

  if zero_values_count > 0.8 * len(group):
    return 0.51 * len(non_zero_values), "Rest"
  else:
    h = round(statistics.mean([
      12 * math.log2(freq / C0) - ideal_offset for freq in non_zero_values
    ]))
    octave = h // 12
    n = h % 12
    note = note_names[n] + str(octave)
    error = sum([
      abs(12 * math.log2(freq / C0) - ideal_offset - h)
      for freq in non_zero_values
    ])
    return error, note

----------------------------------------

TITLE: Implementing Single-Step Prediction Models
DESCRIPTION: Creates and trains various models for single-step predictions, including baseline, linear, dense, and LSTM models.

LANGUAGE: Python
CODE:
baseline = Baseline(label_index=column_indices['T (degC)'])

baseline.compile(loss=tf.keras.losses.MeanSquaredError(),
                 metrics=[tf.keras.metrics.MeanAbsoluteError()])

val_performance = {}
performance = {}
val_performance['Baseline'] = baseline.evaluate(single_step_window.val, return_dict=True)
performance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0, return_dict=True)

# Similar code blocks for linear, dense, and LSTM models

----------------------------------------

TITLE: Loading and Preprocessing Input Image for HRNet in Python
DESCRIPTION: This code downloads a sample image file and loads it using PIL. The image is then converted to a numpy array and normalized by dividing by 255.0 to prepare it for input to the HRNet model.

LANGUAGE: python
CODE:
img_file = tf.keras.utils.get_file(origin="https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg")
img = np.array(Image.open(img_file))/255.0

----------------------------------------

TITLE: Defining Actor-Critic Loss Function
DESCRIPTION: Implements the combined Actor-Critic loss function using Huber loss for the Critic and policy gradient loss for the Actor.

LANGUAGE: Python
CODE:
huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)

def compute_loss(
    action_probs: tf.Tensor,
    values: tf.Tensor,
    returns: tf.Tensor) -> tf.Tensor:
  """Computes the combined Actor-Critic loss."""

  advantage = returns - values

  action_log_probs = tf.math.log(action_probs)
  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)

  critic_loss = huber_loss(values, returns)

  return actor_loss + critic_loss

----------------------------------------

TITLE: Plotting Semantic Similarity Heatmap in Python
DESCRIPTION: This code defines functions to plot a semantic similarity heatmap using seaborn. It computes the inner product of sentence embeddings to measure similarity and visualizes the results.

LANGUAGE: Python
CODE:
def plot_similarity(labels, features, rotation):
  corr = np.inner(features, features)
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap="YlOrRd")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Textual Similarity")

def run_and_plot(messages_):
  message_embeddings_ = embed(messages_)
  plot_similarity(messages_, message_embeddings_, 90)

----------------------------------------

TITLE: Working with Ragged Tensors in TensorFlow
DESCRIPTION: Demonstrates how to create and work with ragged tensors using tf.ragged.constant.

LANGUAGE: Python
CODE:
ragged_list = [
    [0, 1, 2, 3],
    [4, 5],
    [6, 7, 8],
    [9]]

ragged_tensor = tf.ragged.constant(ragged_list)
print(ragged_tensor)

print(ragged_tensor.shape)

----------------------------------------

TITLE: Setting Up Basic Dependencies for Image Processing
DESCRIPTION: Imports essential TensorFlow and image processing libraries needed for the tutorial

LANGUAGE: Python
CODE:
import numpy as np
import os
import PIL
import PIL.Image
import tensorflow as tf
import tensorflow_datasets as tfds

----------------------------------------

TITLE: Evaluating Semantic Textual Similarity Benchmark in Python
DESCRIPTION: This code downloads the STS Benchmark dataset, processes it, and evaluates the performance of the Universal Sentence Encoder on semantic textual similarity tasks using Pearson correlation coefficient.

LANGUAGE: Python
CODE:
import pandas
import scipy
import math
import csv

sts_dataset = tf.keras.utils.get_file(
    fname="Stsbenchmark.tar.gz",
    origin="http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz",
    extract=True)
sts_dev = pandas.read_table(
    os.path.join(os.path.dirname(sts_dataset), "stsbenchmark", "sts-dev.csv"),
    skip_blank_lines=True,
    usecols=[4, 5, 6],
    names=["sim", "sent_1", "sent_2"])
sts_test = pandas.read_table(
    os.path.join(
        os.path.dirname(sts_dataset), "stsbenchmark", "sts-test.csv"),
    quoting=csv.QUOTE_NONE,
    skip_blank_lines=True,
    usecols=[4, 5, 6],
    names=["sim", "sent_1", "sent_2"])
# cleanup some NaN values in sts_dev
sts_dev = sts_dev[[isinstance(s, str) for s in sts_dev['sent_2']]]

sts_data = sts_dev #@param ["sts_dev", "sts_test"] {type:"raw"}

def run_sts_benchmark(batch):
  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)
  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)
  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)
  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)
  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi
  """Returns the similarity scores"""
  return scores

dev_scores = sts_data['sim'].tolist()
scores = []
for batch in np.array_split(sts_data, 10):
  scores.extend(run_sts_benchmark(batch))

pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)
print('Pearson correlation coefficient = {0}\np-value = {1}'.format(
    pearson_correlation[0], pearson_correlation[1]))

----------------------------------------

TITLE: Creating Hidden Layers in TensorFlow Neural Network
DESCRIPTION: Python code to create hidden layers in a TensorFlow neural network using tf.layers.dense.

LANGUAGE: python
CODE:
# Build the hidden layers, sized according to the 'hidden_units' param.
for units in params['hidden_units']:
    net = tf.layers.dense(net, units=units, activation=tf.nn.relu)

----------------------------------------

TITLE: Importing Data Processing Libraries
DESCRIPTION: Import required libraries for data visualization and image processing

LANGUAGE: Python
CODE:
import IPython.display as display
import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (12, 12)
mpl.rcParams['axes.grid'] = False
import numpy as np
import PIL.Image
import time
import functools

----------------------------------------

TITLE: Loading HRNet Model from TensorFlow Hub in Python
DESCRIPTION: This snippet loads the selected HRNet model from TensorFlow Hub using the constructed model name. It then prints a confirmation message with the loaded model's name.

LANGUAGE: python
CODE:
hrnet_model = hub.load(tfhub_model_name)

print('HRNet model loaded           :', tfhub_model_name)

----------------------------------------

TITLE: Creating MNIST Codec and Visualizing Results
DESCRIPTION: Creates compressor and decompressor models, compresses and decompresses MNIST images, and visualizes the results.

LANGUAGE: Python
CODE:
def make_mnist_codec(trainer, **kwargs):
  # The entropy model must be created with `compression=True` and the same
  # instance must be shared between compressor and decompressor.
  entropy_model = tfc.ContinuousBatchedEntropyModel(
      trainer.prior, coding_rank=1, compression=True, **kwargs)
  compressor = MNISTCompressor(trainer.analysis_transform, entropy_model)
  decompressor = MNISTDecompressor(entropy_model, trainer.synthesis_transform)
  return compressor, decompressor

compressor, decompressor = make_mnist_codec(trainer)

(originals, _), = validation_dataset.batch(16).skip(3).take(1)
strings, entropies = compressor(originals)
reconstructions = decompressor(strings)

default_display_digits(originals, strings, entropies, reconstructions)


----------------------------------------

TITLE: Implementing Helper Functions for Image Preprocessing and Label Extraction
DESCRIPTION: Defines two helper functions: one for preprocessing images to be compatible with MobileNetV2 input requirements, and another for extracting labels from the model's probability output.

LANGUAGE: Python
CODE:
def preprocess(image):
  image = tf.cast(image, tf.float32)
  image = tf.image.resize(image, (224, 224))
  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)
  image = image[None, ...]
  return image

def get_imagenet_label(probs):
  return decode_predictions(probs, top=1)[0][0]

----------------------------------------

TITLE: Performing Operations on TensorFlow Variables
DESCRIPTION: This code demonstrates various operations that can be performed on TensorFlow variables, including conversion to tensors, finding the index of the highest value, and reshaping.

LANGUAGE: Python
CODE:
print("A variable:", my_variable)
print("\nViewed as a tensor:", tf.convert_to_tensor(my_variable))
print("\nIndex of highest value:", tf.math.argmax(my_variable))

# This creates a new tensor; it does not reshape the variable.
print("\nCopying and reshaping: ", tf.reshape(my_variable, [1,4]))

----------------------------------------

TITLE: Downloading and Extracting Mini Speech Commands Dataset
DESCRIPTION: This code downloads and extracts a smaller version of the Speech Commands dataset using TensorFlow's utility function.

LANGUAGE: Python
CODE:
DATASET_PATH = 'data/mini_speech_commands'

data_dir = pathlib.Path(DATASET_PATH)
if not data_dir.exists():
  tf.keras.utils.get_file(
      'mini_speech_commands.zip',
      origin="http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip",
      extract=True,
      cache_dir='.', cache_subdir='data')

----------------------------------------

TITLE: Creating Image Dataset from Directory
DESCRIPTION: Uses Keras utility to load images from directory with specified parameters for training and validation splits

LANGUAGE: Python
CODE:
train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

----------------------------------------

TITLE: Creating Input Function for TensorFlow Estimator
DESCRIPTION: Defines a function to create input pipelines for training and evaluation data.

LANGUAGE: Python
CODE:
def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):
  def input_function():
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))
    if shuffle:
      ds = ds.shuffle(1000)
    ds = ds.batch(batch_size).repeat(num_epochs)
    return ds
  return input_function

train_input_fn = make_input_fn(dftrain, y_train)
eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)

----------------------------------------

TITLE: Preprocessed Text Embedding Implementation
DESCRIPTION: Demonstrates the two-step process of text embedding using separate preprocessor and encoder models.

LANGUAGE: python
CODE:
text_input = tf.constant(["A long sentence.",
                          "single-word",
                          "http://example.com"])
preprocessor = hub.load("path/to/preprocessor")  # Must match `encoder`.
encoder_inputs = preprocessor(text_input)

encoder = hub.load("path/to/encoder")
encoder_outputs = encoder(encoder_inputs)
embeddings = encoder_outputs["default"]

----------------------------------------

TITLE: Creating Image Dataset from Directory
DESCRIPTION: Uses Keras utility to load images from directory with specified parameters for training and validation splits

LANGUAGE: Python
CODE:
train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

----------------------------------------

TITLE: Using Keras Metrics Directly in Python
DESCRIPTION: This code demonstrates how to use tf.keras.metrics.Accuracy directly with numpy data or eager tensors in TensorFlow 2.x. It shows how to update the metric state and retrieve results.

LANGUAGE: Python
CODE:
accuracy = tf.keras.metrics.Accuracy()

accuracy.update_state(y_true=[0, 0, 1, 1], y_pred=[0, 0, 0, 1])
accuracy.result().numpy()

accuracy.update_state(y_true=[0, 0, 1, 1], y_pred=[0, 0, 0, 0])
accuracy.update_state(y_true=[0, 0, 1, 1], y_pred=[1, 1, 0, 0])
accuracy.result().numpy()

----------------------------------------

TITLE: Downloading Sample News Headlines Dataset
DESCRIPTION: Downloads a dataset of news headlines from Harvard Dataverse to use as sample data for semantic search.

LANGUAGE: Bash
CODE:
!wget 'https://dataverse.harvard.edu/api/access/datafile/3450625?format=tab&gbrecs=true' -O raw.tsv
!wc -l raw.tsv
!head raw.tsv

----------------------------------------

TITLE: Downloading Sample News Headlines Dataset
DESCRIPTION: Downloads a dataset of news headlines from Harvard Dataverse to use as sample data for semantic search.

LANGUAGE: Bash
CODE:
!wget 'https://dataverse.harvard.edu/api/access/datafile/3450625?format=tab&gbrecs=true' -O raw.tsv
!wc -l raw.tsv
!head raw.tsv

----------------------------------------

TITLE: Installing Dependencies for TensorFlow Object Detection
DESCRIPTION: Installs the required packages numpy and protobuf for running TensorFlow object detection models.

LANGUAGE: bash
CODE:
pip install numpy==1.24.3
pip install protobuf==3.20.3

----------------------------------------

TITLE: TensorFlow Record Dataset Processing
DESCRIPTION: Example of processing TFRecord files with sequential mapping and batching operations.

LANGUAGE: python
CODE:
dataset = tf.data.TFRecordDataset(filename)
dataset = dataset.map(parse_record)
dataset = dataset.batch(32)
dataset = dataset.repeat()

----------------------------------------

TITLE: Initializing Environment and Dependencies - Python
DESCRIPTION: Sets up the required imports and temporary directory for SavedModel examples. Includes configuration of GPU memory growth.

LANGUAGE: python
CODE:
import os
import tempfile

from matplotlib import pyplot as plt
import numpy as np
import tensorflow as tf

tmpdir = tempfile.mkdtemp()

LANGUAGE: python
CODE:
physical_devices = tf.config.list_physical_devices('GPU')
for device in physical_devices:
  tf.config.experimental.set_memory_growth(device, True)

----------------------------------------

TITLE: Visualizing Semantic Similarity of Sentences
DESCRIPTION: Calls the plot_similarity function to create a heatmap visualization of the semantic similarity between the processed sentences based on their BERT embeddings.

LANGUAGE: python
CODE:
plot_similarity(outputs["pooled_output"], sentences)

----------------------------------------

TITLE: Importing Required Libraries for HRNet Semantic Segmentation in Python
DESCRIPTION: This snippet imports the necessary libraries for working with TensorFlow, TensorFlow Hub, matplotlib for visualization, PIL for image processing, and numpy for numerical operations.

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

----------------------------------------

TITLE: Using Decoder as Generative Model
DESCRIPTION: Demonstrates how to use the trained decoder as a generative model by feeding random bits and visualizing the resulting digit samples.

LANGUAGE: Python
CODE:
compressor, decompressor = make_mnist_codec(trainer, decode_sanity_check=False)

import os

strings = tf.constant([os.urandom(8) for _ in range(16)])
samples = decompressor(strings)

fig, axes = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(5, 5))
axes = axes.ravel()
for i in range(len(axes)):
  axes[i].imshow(tf.squeeze(samples[i]))
  axes[i].axis("off")
plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)


----------------------------------------

TITLE: Training Speech Recognition Model
DESCRIPTION: Command to start training the speech recognition model using the Speech Commands dataset. Downloads training data and begins the training process.

LANGUAGE: bash
CODE:
python tensorflow/examples/speech_commands/train.py

----------------------------------------

TITLE: Importing TensorFlow Dependencies
DESCRIPTION: Imports core TensorFlow libraries and Keras components needed for building neural networks.

LANGUAGE: python
CODE:
import tensorflow as tf
print("TensorFlow version:", tf.__version__)

from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model

----------------------------------------

TITLE: Making Predictions with the Trained Model
DESCRIPTION: Uses the trained model to make predictions on new, unlabeled data.

LANGUAGE: Python
CODE:
predict_dataset = tf.convert_to_tensor([
    [0.3, 0.8, 0.4, 0.5,],
    [0.4, 0.1, 0.8, 0.5,],
    [0.7, 0.9, 0.8, 0.4]
])

predictions = model(predict_dataset, training=False)

for i, logits in enumerate(predictions):
  class_idx = tf.math.argmax(logits).numpy()
  p = tf.nn.softmax(logits)[class_idx]
  name = class_names[class_idx]
  print("Example {} prediction: {} ({:4.1f}%)".format(i, name, 100*p))

----------------------------------------

TITLE: Working with String Tensors in TensorFlow
DESCRIPTION: Shows how to create and manipulate string tensors, including Unicode handling and string splitting.

LANGUAGE: Python
CODE:
scalar_string_tensor = tf.constant("Gray wolf")
print(scalar_string_tensor)

tensor_of_strings = tf.constant(["Gray wolf",
                                 "Quick brown fox",
                                 "Lazy dog"])
print(tensor_of_strings)

tf.constant("")

print(tf.strings.split(scalar_string_tensor, sep=" "))

print(tf.strings.split(tensor_of_strings))

text = tf.constant("1 10 100")
print(tf.strings.to_number(tf.strings.split(text, " ")))

byte_strings = tf.strings.bytes_split(tf.constant("Duck"))
byte_ints = tf.io.decode_raw(tf.constant("Duck"), tf.uint8)
print("Byte strings:", byte_strings)
print("Bytes:", byte_ints)

unicode_bytes = tf.constant(" ")
unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, "UTF-8")
unicode_values = tf.strings.unicode_decode(unicode_bytes, "UTF-8")

print("\nUnicode bytes:", unicode_bytes)
print("\nUnicode chars:", unicode_char_bytes)
print("\nUnicode values:", unicode_values)

----------------------------------------

TITLE: Performing Basic Math Operations on Tensors in TensorFlow
DESCRIPTION: Shows how to perform addition, element-wise multiplication, and matrix multiplication on tensors.

LANGUAGE: Python
CODE:
a = tf.constant([[1, 2],
                 [3, 4]])
b = tf.constant([[1, 1],
                 [1, 1]]) # Could have also said `tf.ones([2,2], dtype=tf.int32)`

print(tf.add(a, b), "\n")
print(tf.multiply(a, b), "\n")
print(tf.matmul(a, b), "\n")

print(a + b, "\n") # element-wise addition
print(a * b, "\n") # element-wise multiplication
print(a @ b, "\n") # matrix multiplication

----------------------------------------

TITLE: Vectorizing Text Data
DESCRIPTION: Create mappings between characters and integers, and convert the text to a numerical representation.

LANGUAGE: Python
CODE:
# Creating a mapping from unique characters to indices
char2idx = {u:i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)

text_as_int = np.array([char2idx[c] for c in text])

----------------------------------------

TITLE: Installing TensorFlow Text
DESCRIPTION: Installs the TensorFlow Text library using pip, which is required for text preprocessing operations.

LANGUAGE: bash
CODE:
!pip install --quiet "tensorflow-text==2.11.*"

----------------------------------------

TITLE: Displaying BERT Inputs and Outputs
DESCRIPTION: Prints the original sentences, BERT inputs (tokenized and encoded), pooled embeddings, and per-token embeddings to demonstrate the BERT processing pipeline.

LANGUAGE: python
CODE:
print("Sentences:")
print(sentences)

print("\nBERT inputs:")
print(inputs)

print("\nPooled embeddings:")
print(outputs["pooled_output"])

print("\nPer token embeddings:")
print(outputs["sequence_output"])

----------------------------------------

TITLE: Processing Mini-Batches with LSTM in TensorFlow
DESCRIPTION: Pseudocode demonstrating how to process mini-batches of words through an LSTM cell, updating the state and calculating probabilities and loss at each step.

LANGUAGE: python
CODE:
words_in_dataset = tf.placeholder(tf.float32, [time_steps, batch_size, num_features])
lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
state = lstm.zero_state(batch_size, dtype=tf.float32)
probabilities = []
loss = 0.0
for current_batch_of_words in words_in_dataset:
    # The value of state is updated after processing each batch of words.
    output, state = lstm(current_batch_of_words, state)

    # The LSTM output can be used to make next word predictions
    logits = tf.matmul(output, softmax_w) + softmax_b
    probabilities.append(tf.nn.softmax(logits))
    loss += loss_function(probabilities, target_words)

----------------------------------------

TITLE: Variable Device Placement
DESCRIPTION: Shows how to place a TensorFlow variable on a specific device using a device context.

LANGUAGE: python
CODE:
with tf.device("/device:GPU:1"):
  v = tf.get_variable("v", [1])

----------------------------------------

TITLE: Implementing Metrics with TF2 Keras in Python
DESCRIPTION: This snippet shows how to use tf.keras.metrics.Accuracy with a TensorFlow 2.x Keras model. It creates a dataset, defines a model, and compiles it with the accuracy metric.

LANGUAGE: Python
CODE:
dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(1)
eval_dataset = tf.data.Dataset.from_tensor_slices(
      (eval_features, eval_labels)).batch(1)

inputs = tf.keras.Input((2,))
logits = tf.keras.layers.Dense(2)(inputs)
predictions = tf.math.argmax(input=logits, axis=1)
model = tf.keras.models.Model(inputs, predictions)
optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)

model.compile(optimizer, loss='mse', metrics=[tf.keras.metrics.Accuracy()])

model.evaluate(eval_dataset, return_dict=True)

----------------------------------------

TITLE: Loading TensorFlow Hub BigGAN Module Configuration
DESCRIPTION: Sets up the module path for different BigGAN model variants supporting different image resolutions (128x128, 256x256, 512x512)

LANGUAGE: Python
CODE:
# BigGAN-deep models
# module_path = 'https://tfhub.dev/deepmind/biggan-deep-128/1'  # 128x128 BigGAN-deep
module_path = 'https://tfhub.dev/deepmind/biggan-deep-256/1'  # 256x256 BigGAN-deep
# module_path = 'https://tfhub.dev/deepmind/biggan-deep-512/1'  # 512x512 BigGAN-deep

----------------------------------------

TITLE: Downloading the Maestro Dataset
DESCRIPTION: Downloads the Maestro dataset of MIDI files if not already present in the specified directory.

LANGUAGE: Python
CODE:
data_dir = pathlib.Path('data/maestro-v2.0.0')
if not data_dir.exists():
  tf.keras.utils.get_file(
      'maestro-v2.0.0-midi.zip',
      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',
      extract=True,
      cache_dir='.', cache_subdir='data',
  )

----------------------------------------

TITLE: Loading MoveNet Model from TensorFlow Hub
DESCRIPTION: This code snippet loads the specified MoveNet model variant from TensorFlow Hub or as a TFLite model. It defines a movenet() function that runs inference on an input image and returns keypoint coordinates and scores.

LANGUAGE: Python
CODE:
model_name = "movenet_lightning" #@param ["movenet_lightning", "movenet_thunder", "movenet_lightning_f16.tflite", "movenet_thunder_f16.tflite", "movenet_lightning_int8.tflite", "movenet_thunder_int8.tflite"]

if "tflite" in model_name:
  if "movenet_lightning_f16" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite
    input_size = 192
  elif "movenet_thunder_f16" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite
    input_size = 256
  elif "movenet_lightning_int8" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite
    input_size = 192
  elif "movenet_thunder_int8" in model_name:
    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite
    input_size = 256
  else:
    raise ValueError("Unsupported model name: %s" % model_name)

  # Initialize the TFLite interpreter
  interpreter = tf.lite.Interpreter(model_path="model.tflite")
  interpreter.allocate_tensors()

  def movenet(input_image):
    """Runs detection on an input image.

    Args:
      input_image: A [1, height, width, 3] tensor represents the input image
        pixels. Note that the height/width should already be resized and match the
        expected input resolution of the model before passing into this function.

    Returns:
      A [1, 1, 17, 3] float numpy array representing the predicted keypoint
      coordinates and scores.
    """
    # TF Lite format expects tensor type of uint8.
    input_image = tf.cast(input_image, dtype=tf.uint8)
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())
    # Invoke inference.
    interpreter.invoke()
    # Get the model prediction.
    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])
    return keypoints_with_scores

else:
  if "movenet_lightning" in model_name:
    module = hub.load("https://tfhub.dev/google/movenet/singlepose/lightning/4")
    input_size = 192
  elif "movenet_thunder" in model_name:
    module = hub.load("https://tfhub.dev/google/movenet/singlepose/thunder/4")
    input_size = 256
  else:
    raise ValueError("Unsupported model name: %s" % model_name)

  def movenet(input_image):
    """Runs detection on an input image.

    Args:
      input_image: A [1, height, width, 3] tensor represents the input image
        pixels. Note that the height/width should already be resized and match the
        expected input resolution of the model before passing into this function.

    Returns:
      A [1, 1, 17, 3] float numpy array representing the predicted keypoint
      coordinates and scores.
    """
    model = module.signatures['serving_default']

    # SavedModel format expects tensor type of int32.
    input_image = tf.cast(input_image, dtype=tf.int32)
    # Run model inference.
    outputs = model(input_image)
    # Output is a [1, 1, 17, 3] tensor.
    keypoints_with_scores = outputs['output_0'].numpy()
    return keypoints_with_scores

----------------------------------------

TITLE: Importing TensorFlow and Keras Libraries
DESCRIPTION: Imports the necessary TensorFlow and Keras libraries, and prints the TensorFlow version.

LANGUAGE: python
CODE:
import os

import tensorflow as tf
from tensorflow import keras

print(tf.version.VERSION)

----------------------------------------

TITLE: Configuring BERT Model and Preprocessing
DESCRIPTION: Sets up the BERT model and preprocessing model URLs from TensorFlow Hub. This allows for easy switching between different BERT variants.

LANGUAGE: python
CODE:
BERT_MODEL = "https://tfhub.dev/google/experts/bert/wiki_books/2"
PREPROCESS_MODEL = "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"

----------------------------------------

TITLE: Training and Evaluating TensorFlow RNN Model
DESCRIPTION: Code to set up and run training and evaluation of the RNN model using TensorFlow Estimator and Experiment APIs.

LANGUAGE: Python
CODE:
estimator = tf.estimator.Estimator(
    model_fn=model_fn,
    model_dir=output_dir,
    config=config,
    params=model_params)
# Train the model.
tf.contrib.learn.Experiment(
    estimator=estimator,
    train_input_fn=get_input_fn(
        mode=tf.contrib.learn.ModeKeys.TRAIN,
        tfrecord_pattern=FLAGS.training_data,
        batch_size=FLAGS.batch_size),
    train_steps=FLAGS.steps,
    eval_input_fn=get_input_fn(
        mode=tf.contrib.learn.ModeKeys.EVAL,
        tfrecord_pattern=FLAGS.eval_data,
        batch_size=FLAGS.batch_size),
    min_eval_frequency=1000)

----------------------------------------

TITLE: Calculating Loss for TensorFlow Neural Network
DESCRIPTION: Python code to calculate the loss for a TensorFlow neural network using sparse softmax cross entropy.

LANGUAGE: python
CODE:
# Compute loss.
loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

----------------------------------------

TITLE: Custom Training with Preprocessing Layers
DESCRIPTION: Implementation of custom training with Keras preprocessing layers in a distributed setting

LANGUAGE: python
CODE:
strategy = tf.distribute.MirroredStrategy()
vocab = ["a", "b", "c", "d", "f"]

with strategy.scope():
  # Create the layer(s) under scope.
  layer = tf.keras.layers.StringLookup(vocabulary=vocab)

def dataset_fn(input_context):
  # a tf.data.Dataset
  dataset = tf.data.Dataset.from_tensor_slices(["a", "c", "e"]).repeat()

  # Custom your batching, sharding, prefetching, etc.
  global_batch_size = 4
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  dataset = dataset.batch(batch_size)
  dataset = dataset.shard(
      input_context.num_input_pipelines,
      input_context.input_pipeline_id)

  # Apply the preprocessing layer(s) to the tf.data.Dataset
  def preprocess_with_kpl(input):
    return layer(input)

  processed_ds = dataset.map(preprocess_with_kpl)
  return processed_ds

distributed_dataset = strategy.distribute_datasets_from_function(dataset_fn)

# Print out a few example batches.
distributed_dataset_iterator = iter(distributed_dataset)
for _ in range(3):
  print(next(distributed_dataset_iterator))

----------------------------------------

TITLE: Loading IMDB Reviews Dataset from TensorFlow Datasets
DESCRIPTION: Demonstrates how to load the IMDB Large Movie Review dataset from TensorFlow Datasets for sentiment classification.

LANGUAGE: Python
CODE:
train_ds = tfds.load(
    'imdb_reviews',
    split='train[:80%]',
    batch_size=BATCH_SIZE,
    shuffle_files=True,
    as_supervised=True)

----------------------------------------

TITLE: Accessing Module Variables
DESCRIPTION: Demonstrates how to access trainable and all variables of a TensorFlow module.

LANGUAGE: Python
CODE:
# All trainable variables
print("trainable variables:", simple_module.trainable_variables)
# Every variable
print("all variables:", simple_module.variables)

----------------------------------------

TITLE: Applying Keras Preprocessing Layers
DESCRIPTION: Define utility functions for creating normalization layers for numerical features and category encoding layers for categorical features.

LANGUAGE: Python
CODE:
def get_normalization_layer(name, dataset):
  normalizer = layers.Normalization(axis=None)
  feature_ds = dataset.map(lambda x, y: x[name])
  normalizer.adapt(feature_ds)
  return normalizer

def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):
  if dtype == 'string':
    index = layers.StringLookup(max_tokens=max_tokens)
  else:
    index = layers.IntegerLookup(max_tokens=max_tokens)
  feature_ds = dataset.map(lambda x, y: x[name])
  index.adapt(feature_ds)
  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())
  return lambda feature: encoder(index(feature))

----------------------------------------

TITLE: Loading and Preparing TensorFlow Flowers Dataset
DESCRIPTION: Loads the tf_flowers dataset using TensorFlow Datasets API and splits it into training, validation and test sets

LANGUAGE: Python
CODE:
(train_ds, val_ds, test_ds), metadata = tfds.load(
    'tf_flowers',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)

----------------------------------------

TITLE: Importing TensorFlow and Preparing Data in Python
DESCRIPTION: This snippet imports TensorFlow, sets up compatibility with TF1, and prepares sample data for demonstration purposes.

LANGUAGE: Python
CODE:
import tensorflow as tf
import tensorflow.compat.v1 as tf1

features = [[1., 1.5], [2., 2.5], [3., 3.5]]
labels = [0, 0, 1]
eval_features = [[4., 4.5], [5., 5.5], [6., 6.5]]
eval_labels = [0, 1, 1]

----------------------------------------

TITLE: Creating Word Embeddings in TensorFlow
DESCRIPTION: This snippet demonstrates how to create word embeddings in TensorFlow. It assumes that words have been converted to integer IDs and shows how to create an embedding variable and use tf.nn.embedding_lookup to map word IDs to dense vectors.

LANGUAGE: Python
CODE:
word_embeddings = tf.get_variable("word_embeddings",
    [vocabulary_size, embedding_size])
embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, word_ids)

----------------------------------------

TITLE: Reshaping Tensors in TensorFlow
DESCRIPTION: Illustrates how to use tf.reshape to change the shape of tensors while preserving their elements.

LANGUAGE: python
CODE:
rank_three_tensor = tf.ones([3, 4, 5])
matrix = tf.reshape(rank_three_tensor, [6, 10])  # Reshape existing content into
                                                 # a 6x10 matrix
matrixB = tf.reshape(matrix, [3, -1])  #  Reshape existing content into a 3x20
                                       # matrix. -1 tells reshape to calculate
                                       # the size of this dimension.
matrixAlt = tf.reshape(matrixB, [4, 3, -1])  # Reshape existing content into a
                                             #4x3x5 tensor

# Note that the number of elements of the reshaped Tensors has to match the
# original number of elements. Therefore, the following example generates an
# error because no possible value for the last dimension will match the number
# of elements.
yet_another = tf.reshape(matrixAlt, [13, 2, -1])  # ERROR!

----------------------------------------

TITLE: Making Predictions with DNNClassifier Estimator in TensorFlow
DESCRIPTION: This code uses the trained DNNClassifier Estimator to make predictions on new data and prints the results with probabilities.

LANGUAGE: Python
CODE:
# Generate predictions from the model
expected = ['Setosa', 'Versicolor', 'Virginica']
predict_x = {
    'SepalLength': [5.1, 5.9, 6.9],
    'SepalWidth': [3.3, 3.0, 3.1],
    'PetalLength': [1.7, 4.2, 5.4],
    'PetalWidth': [0.5, 1.5, 2.1],
}

def input_fn(features, batch_size=256):
    """An input function for prediction."""
    # Convert the inputs to a Dataset without labels.
    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)

predictions = classifier.predict(
    input_fn=lambda: input_fn(predict_x))

for pred_dict, expec in zip(predictions, expected):
    class_id = pred_dict['class_ids'][0]
    probability = pred_dict['probabilities'][class_id]

    print('Prediction is "{}" ({:.1f}%), expected "{}"'.format(
        SPECIES[class_id], 100 * probability, expec))

----------------------------------------

TITLE: Configuring Virtual CPUs for DTensor
DESCRIPTION: Sets up 8 virtual CPUs to simulate a distributed environment for DTensor. This allows testing distributed training functionality on a single machine.

LANGUAGE: Python
CODE:
def configure_virtual_cpus(ncpu):
  phy_devices = tf.config.list_physical_devices('CPU')
  tf.config.set_logical_device_configuration(phy_devices[0], [
        tf.config.LogicalDeviceConfiguration(),
    ] * ncpu)

configure_virtual_cpus(8)

DEVICES = [f'CPU:{i}' for i in range(8)]
devices = tf.config.list_logical_devices('CPU')
device_names = [d.name for d in devices]
device_names

----------------------------------------

TITLE: Incremental Migration to Native TF2 APIs
DESCRIPTION: Demonstrates the process of incrementally migrating a TF1.x model to native TF2 APIs while maintaining checkpoint compatibility.

LANGUAGE: Python
CODE:
class FunctionalStyleCompatModel(tf.keras.layers.Layer):

  @tf.compat.v1.keras.utils.track_tf1_style_variables
  def call(self, inputs, training=None):
    with tf.compat.v1.variable_scope('model'):
      out = tf.compat.v1.layers.conv2d(
          inputs, 3, 3,
          kernel_regularizer="l2")
      out = tf.compat.v1.layers.conv2d(
          out, 4, 4,
          kernel_regularizer="l2")
      out = tf.compat.v1.layers.conv2d(
          out, 5, 5,
          kernel_regularizer="l2")
      return out

layer = FunctionalStyleCompatModel()
layer(tf.ones(shape=(10, 10, 10, 10)))
[v.name for v in layer.weights]

LANGUAGE: Python
CODE:
class OOStyleCompatModel(tf.keras.layers.Layer):

  def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.conv_1 = tf.compat.v1.layers.Conv2D(
          3, 3,
          kernel_regularizer="l2")
    self.conv_2 = tf.compat.v1.layers.Conv2D(
          4, 4,
          kernel_regularizer="l2")

  @tf.compat.v1.keras.utils.track_tf1_style_variables
  def call(self, inputs, training=None):
    with tf.compat.v1.variable_scope('model'):
      out = self.conv_1(inputs)
      out = self.conv_2(out)
      out = tf.compat.v1.layers.conv2d(
          out, 5, 5,
          kernel_regularizer="l2")
      return out

layer = OOStyleCompatModel()
layer(tf.ones(shape=(10, 10, 10, 10)))
[v.name for v in layer.weights]

LANGUAGE: Python
CODE:
def record_scope(scope_name):
  """Record a variable_scope to make sure future ones get incremented."""
  with tf.compat.v1.variable_scope(scope_name):
    pass

class PartiallyNativeKerasLayersModel(tf.keras.layers.Layer):

  def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.conv_1 = tf.keras.layers.Conv2D(
          3, 3,
          kernel_regularizer="l2")
    self.conv_2 = tf.keras.layers.Conv2D(
          4, 4,
          kernel_regularizer="l2")

  @tf.compat.v1.keras.utils.track_tf1_style_variables
  def call(self, inputs, training=None):
    with tf.compat.v1.variable_scope('model'):
      out = self.conv_1(inputs)
      record_scope('conv2d')
      out = self.conv_2(out)
      record_scope('conv2d_1')
      out = tf.compat.v1.layers.conv2d(
          out, 5, 5,
          kernel_regularizer="l2")
      return out

layer = PartiallyNativeKerasLayersModel()
layer(tf.ones(shape=(10, 10, 10, 10)))
[v.name for v in layer.weights]

LANGUAGE: Python
CODE:
class FullyNativeKerasLayersModel(tf.keras.layers.Layer):

  def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.conv_1 = tf.keras.layers.Conv2D(
          3, 3,
          kernel_regularizer="l2")
    self.conv_2 = tf.keras.layers.Conv2D(
          4, 4,
          kernel_regularizer="l2")
    self.conv_3 = tf.keras.layers.Conv2D(
          5, 5,
          kernel_regularizer="l2")


  def call(self, inputs, training=None):
    with tf.compat.v1.variable_scope('model'):
      out = self.conv_1(inputs)
      out = self.conv_2(out)
      out = self.conv_3(out)
      return out

layer = FullyNativeKerasLayersModel()
layer(tf.ones(shape=(10, 10, 10, 10)))
[v.name for v in layer.weights]

----------------------------------------

TITLE: Loading Pre-trained MobileNetV2 Model for Image Classification
DESCRIPTION: Loads a pre-trained MobileNetV2 model with ImageNet weights and sets it as non-trainable. Also imports the decode_predictions function for interpreting model outputs.

LANGUAGE: Python
CODE:
pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,
                                                     weights='imagenet')
pretrained_model.trainable = False

# ImageNet labels
decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions

----------------------------------------

TITLE: Implementing Training Logic in TensorFlow Estimator
DESCRIPTION: Python code to implement training logic in a TensorFlow Estimator, including optimizer setup and minimization operation.

LANGUAGE: python
CODE:
optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)
train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

----------------------------------------

TITLE: Loading and Preprocessing MNIST Dataset
DESCRIPTION: Loads the MNIST dataset, preprocesses images by reshaping and binarizing, and creates TensorFlow datasets for training and testing.

LANGUAGE: Python
CODE:
def preprocess_images(images):
  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.
  return np.where(images > .5, 1.0, 0.0).astype('float32')

train_images = preprocess_images(train_images)
test_images = preprocess_images(test_images)

train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)
                 .shuffle(train_size).batch(batch_size))
test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)
                .shuffle(test_size).batch(batch_size))

----------------------------------------

TITLE: Writing Testable Docstrings for TensorFlow Functions in Python
DESCRIPTION: This snippet demonstrates how to write a testable docstring for the tf.concat function using DocTest. It includes an example usage, argument descriptions, and return value information.

LANGUAGE: Python
CODE:
def concat(values, axis, name="concat"):
  """Concatenates tensors along one dimension.
  ...

  >>> t1 = [[1, 2, 3], [4, 5, 6]]
  >>> t2 = [[7, 8, 9], [10, 11, 12]]
  >>> concat([t1, t2], 0)
  <tf.Tensor: shape=(4, 3), dtype=int32, numpy=
  array([[ 1,  2,  3],
         [ 4,  5,  6],
         [ 7,  8,  9],
         [10, 11, 12]], dtype=int32)>

  <... more description or code snippets ...>

  Args:
    values: A list of `tf.Tensor` objects or a single `tf.Tensor`.
    axis: 0-D `int32` `Tensor`.  Dimension along which to concatenate. Must be
      in the range `[-rank(values), rank(values))`. As in Python, indexing for
      axis is 0-based. Positive axis in the rage of `[0, rank(values))` refers
      to `axis`-th dimension. And negative axis refers to `axis +
      rank(values)`-th dimension.
    name: A name for the operation (optional).

    Returns:
      A `tf.Tensor` resulting from concatenation of the input tensors.
  """

  <code here>

----------------------------------------

TITLE: Creating Rank 0 Tensors in TensorFlow
DESCRIPTION: Demonstrates how to create scalar (rank 0) tensors of different data types using tf.Variable.

LANGUAGE: python
CODE:
mammal = tf.Variable("Elephant", tf.string)
ignition = tf.Variable(451, tf.int16)
floating = tf.Variable(3.14159265359, tf.float64)
its_complicated = tf.Variable(12.3 - 4.85j, tf.complex64)

----------------------------------------

TITLE: Image Loading Utility Function
DESCRIPTION: Defines a function to load and preprocess images from URLs or local files for use with the FILM model

LANGUAGE: python
CODE:
_UINT8_MAX_F = float(np.iinfo(np.uint8).max)

def load_image(img_url: str):
  """Returns an image with shape [height, width, num_channels], with pixels in [0..1] range, and type np.float32."""

  if (img_url.startswith("https")):
    user_agent = {'User-agent': 'Colab Sample (https://tensorflow.org)'}
    response = requests.get(img_url, headers=user_agent)
    image_data = response.content
  else:
    image_data = tf.io.read_file(img_url)

  image = tf.io.decode_image(image_data, channels=3)
  image_numpy = tf.cast(image, dtype=tf.float32).numpy()
  return image_numpy / _UINT8_MAX_F

----------------------------------------

TITLE: Importing Required Libraries for TensorBoard Setup
DESCRIPTION: Import necessary TensorFlow packages and utilities needed for TensorBoard visualization, including compatibility imports for TF1.x code.

LANGUAGE: python
CODE:
import tensorflow.compat.v1 as tf1
import tensorflow as tf
import tempfile
import numpy as np
import datetime
%load_ext tensorboard

----------------------------------------

TITLE: Parsing Quick Draw JSON Data in Python
DESCRIPTION: Function to parse a single line of ndjson data from the Quick Draw dataset, extracting the drawing strokes and class name.

LANGUAGE: Python
CODE:
def parse_line(ndjson_line):
  """Parse an ndjson line and return ink (as np array) and classname."""
  sample = json.loads(ndjson_line)
  class_name = sample["word"]
  inkarray = sample["drawing"]
  stroke_lengths = [len(stroke[0]) for stroke in inkarray]
  total_points = sum(stroke_lengths)
  np_ink = np.zeros((total_points, 3), dtype=np.float32)
  current_t = 0
  for stroke in inkarray:
    for i in [0, 1]:
      np_ink[current_t:(current_t + len(stroke[0])), i] = stroke[i]
    current_t += len(stroke[0])
    np_ink[current_t - 1, 2] = 1  # stroke_end
  # Preprocessing.
  # 1. Size normalization.
  lower = np.min(np_ink[:, 0:2], axis=0)
  upper = np.max(np_ink[:, 0:2], axis=0)
  scale = upper - lower
  scale[scale == 0] = 1
  np_ink[:, 0:2] = (np_ink[:, 0:2] - lower) / scale
  # 2. Compute deltas.
  np_ink[1:, 0:2] -= np_ink[0:-1, 0:2]
  np_ink = np_ink[1:, :]
  return np_ink, class_name

----------------------------------------

TITLE: Loading and Preprocessing Weather Dataset
DESCRIPTION: Downloads a weather dataset, loads it into a pandas DataFrame, and performs initial preprocessing like subsampling and datetime conversion.

LANGUAGE: Python
CODE:
zip_path = tf.keras.utils.get_file(
    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',
    fname='jena_climate_2009_2016.csv.zip',
    extract=True)
csv_path, _ = os.path.splitext(zip_path)

df = pd.read_csv(csv_path)
df = df[5::6]

date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')

----------------------------------------

TITLE: Creating TensorFlow Variable with Custom Type and Initializer
DESCRIPTION: Shows how to create a TensorFlow variable with explicit dtype and initializer specifications.

LANGUAGE: python
CODE:
my_int_variable = tf.get_variable("my_int_variable", [1, 2, 3], dtype=tf.int32,
  initializer=tf.zeros_initializer)

----------------------------------------

TITLE: Training DNNClassifier Estimator in TensorFlow
DESCRIPTION: This code trains the DNNClassifier Estimator using the input function and training data for 5000 steps.

LANGUAGE: Python
CODE:
# Train the Model.
classifier.train(
    input_fn=lambda: input_fn(train, train_y, training=True),
    steps=5000)

----------------------------------------

TITLE: Using Multiple GPUs in TensorFlow
DESCRIPTION: Demonstrates how to construct a multi-tower model that distributes computations across multiple GPUs.

LANGUAGE: python
CODE:
# Creates a graph.
c = []
for d in ['/device:GPU:2', '/device:GPU:3']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
    c.append(tf.matmul(a, b))
with tf.device('/cpu:0'):
  sum = tf.add_n(c)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(sum))

----------------------------------------

TITLE: Exploring Rate-Distortion Tradeoff
DESCRIPTION: Trains and visualizes models with different lambda values to demonstrate the tradeoff between compression rate and reconstruction quality.

LANGUAGE: Python
CODE:
def train_and_visualize_model(lmbda):
  trainer = train_mnist_model(lmbda=lmbda)
  compressor, decompressor = make_mnist_codec(trainer)
  strings, entropies = compressor(originals)
  reconstructions = decompressor(strings)
  display_digits(originals, strings, entropies, reconstructions)

train_and_visualize_model(lmbda=500)
train_and_visualize_model(lmbda=300)


----------------------------------------

TITLE: Converting TF1 frozen GraphDef to TFLite with TF2
DESCRIPTION: Illustrates the process of converting a TensorFlow 1.x frozen GraphDef to TFLite by first converting to a TF1 SavedModel.

LANGUAGE: Python
CODE:
## Convert TF1 frozen Graph to TF1 SavedModel.

# Load the graph as a v1.GraphDef
import pathlib
gdef = tf.compat.v1.GraphDef()
gdef.ParseFromString(pathlib.Path(GRAPH_DEF_MODEL_PATH).read_bytes())

# Convert the GraphDef to a tf.Graph
with tf.Graph().as_default() as g:
  tf.graph_util.import_graph_def(gdef, name="")

# Look up the input and output tensors.
input_tensor = g.get_tensor_by_name('input:0') 
output_tensor = g.get_tensor_by_name('MobilenetV1/Predictions/Softmax:0')

# Save the graph as a TF1 Savedmodel
remove_dir('saved_model_3/')
with tf.compat.v1.Session(graph=g) as s:
  tf.compat.v1.saved_model.simple_save(
      session=s,
      export_dir='saved_model_3/',
      inputs={'input':input_tensor},
      outputs={'output':output_tensor})

# Convert TF1 SavedModel to a TFLite model.
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir='saved_model_3/')
converter.optimizations = {tf.lite.Optimize.DEFAULT}
tflite_model = converter.convert()

----------------------------------------

TITLE: Training and Evaluating Estimator
DESCRIPTION: Executes training and evaluation of the converted estimator using the input function

LANGUAGE: python
CODE:
keras_estimator.train(input_fn=input_fn, steps=500)
eval_result = keras_estimator.evaluate(input_fn=input_fn, steps=10)
print('Eval result: {}'.format(eval_result))

----------------------------------------

TITLE: Running TensorFlow with GPU Support in Docker
DESCRIPTION: Example of running a TensorFlow program with GPU support in a Docker container. This command uses the latest GPU-enabled TensorFlow image.

LANGUAGE: bash
CODE:
docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu \
   python -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"

----------------------------------------

TITLE: Generating Multiple Distribution Types in Python
DESCRIPTION: Creates and combines multiple types of distributions including normal, gamma, poisson, and uniform distributions to demonstrate different histogram patterns in TensorBoard.

LANGUAGE: python
CODE:
import tensorflow as tf

k = tf.placeholder(tf.float32)

# Make a normal distribution, with a shifting mean
mean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)
# Record that distribution into a histogram summary
tf.summary.histogram("normal/moving_mean", mean_moving_normal)

# Make a normal distribution with shrinking variance
variance_shrinking_normal = tf.random_normal(shape=[1000], mean=0, stddev=1-(k))
# Record that distribution too
tf.summary.histogram("normal/shrinking_variance", variance_shrinking_normal)

# Let's combine both of those distributions into one dataset
normal_combined = tf.concat([mean_moving_normal, variance_shrinking_normal], 0)
# We add another histogram summary to record the combined distribution
tf.summary.histogram("normal/bimodal", normal_combined)

# Add a gamma distribution
gamma = tf.random_gamma(shape=[1000], alpha=k)
tf.summary.histogram("gamma", gamma)

# And a poisson distribution
poisson = tf.random_poisson(shape=[1000], lam=k)
tf.summary.histogram("poisson", poisson)

# And a uniform distribution
uniform = tf.random_uniform(shape=[1000], maxval=k*10)
tf.summary.histogram("uniform", uniform)

# Finally, combine everything together!
all_distributions = [mean_moving_normal, variance_shrinking_normal,
                     gamma, poisson, uniform]
all_combined = tf.concat(all_distributions, 0)
tf.summary.histogram("all_combined", all_combined)

summaries = tf.summary.merge_all()

# Setup a session and summary writer
sess = tf.Session()
writer = tf.summary.FileWriter("/tmp/histogram_example")

# Setup a loop and write the summaries to disk
N = 400
for step in range(N):
  k_val = step/float(N)
  summ = sess.run(summaries, feed_dict={k: k_val})
  writer.add_summary(summ, global_step=step)

----------------------------------------

TITLE: Loading Universal Sentence Encoder Model
DESCRIPTION: Loads the Universal Sentence Encoder-Lite model from TensorFlow Hub and sets up the input placeholder and encodings

LANGUAGE: python
CODE:
module = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-lite/2")

input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])
encodings = module(
    inputs=dict(
        values=input_placeholder.values,
        indices=input_placeholder.indices,
        dense_shape=input_placeholder.dense_shape))

----------------------------------------

TITLE: Creating Sequential Keras Model Architecture
DESCRIPTION: Defines a simple sequential neural network with dense layers and dropout for classification

LANGUAGE: python
CODE:
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(16, activation='relu', input_shape=(4,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(3)
])

----------------------------------------

TITLE: Importing TensorFlow and Configuring Device Placement Logging
DESCRIPTION: This snippet imports TensorFlow and provides an option to enable device placement logging for debugging purposes.

LANGUAGE: Python
CODE:
import tensorflow as tf

# Uncomment to see where your variables get placed (see below)
# tf.debugging.set_log_device_placement(True)

----------------------------------------

TITLE: Processing Sentences with BERT Model
DESCRIPTION: Loads the BERT model and preprocessing model from TF-Hub, tokenizes the sentences, and generates BERT embeddings for the input sentences.

LANGUAGE: python
CODE:
preprocess = hub.load(PREPROCESS_MODEL)
bert = hub.load(BERT_MODEL)
inputs = preprocess(sentences)
outputs = bert(inputs)

----------------------------------------

TITLE: Using Custom Callbacks with Model.fit in TensorFlow 2
DESCRIPTION: This snippet demonstrates how to use the custom callbacks with Keras Model.fit in TensorFlow 2. It creates a dataset, defines a simple model, and trains it using the custom callbacks for logging and stopping.

LANGUAGE: Python
CODE:
dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(1)
model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])
optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)
model.compile(optimizer, "mse")

# Begin training.
# The training will stop after 2 steps, and the weights/loss will also be logged.
model.fit(dataset, callbacks=[StopAtStepCallback(stop_step=2),
                              LoggingTensorCallback(every_n_iter=2)])

----------------------------------------

TITLE: Loading Required Python Libraries
DESCRIPTION: Import necessary Python packages for data processing, visualization and machine learning

LANGUAGE: python
CODE:
import os
from IPython import display
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_io as tfio

----------------------------------------

TITLE: Generating Text Embeddings with TensorFlow Hub
DESCRIPTION: Defines a function to generate text embeddings using a Universal Sentence Encoder model from TensorFlow Hub.

LANGUAGE: Python
CODE:
def embed_text(text, module_url, random_projection_matrix):
  global encoder
  if not encoder:
    encoder = hub.Module(module_url)
  embedding = encoder(text)
  if random_projection_matrix is not None:
    embedding = tf.matmul(
        embedding, tf.cast(random_projection_matrix, embedding.dtype))
  return embedding

----------------------------------------

TITLE: Basic Autoencoder Implementation in TensorFlow
DESCRIPTION: Defines a basic autoencoder class using TensorFlow's Keras API that compresses images into a 64-dimensional latent space and reconstructs them. Includes encoder and decoder components using Dense layers.

LANGUAGE: Python
CODE:
class Autoencoder(Model):
  def __init__(self, latent_dim, shape):
    super(Autoencoder, self).__init__()
    self.latent_dim = latent_dim
    self.shape = shape
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(latent_dim, activation='relu'),
    ])
    self.decoder = tf.keras.Sequential([
      layers.Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid'),
      layers.Reshape(shape)
    ])

  def call(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return decoded

----------------------------------------

TITLE: Installing TensorFlow Recommenders and Importing Dependencies
DESCRIPTION: Installs the TensorFlow Recommenders package and imports necessary libraries for TPU embedding.

LANGUAGE: python
CODE:
!pip install tensorflow-recommenders

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow.compat.v1 as tf1

# TPUEmbedding layer is not part of TensorFlow.
import tensorflow_recommenders as tfrs

----------------------------------------

TITLE: Generating Text Embeddings with TensorFlow Hub
DESCRIPTION: Defines a function to generate text embeddings using a Universal Sentence Encoder model from TensorFlow Hub.

LANGUAGE: Python
CODE:
def embed_text(text, module_url, random_projection_matrix):
  global encoder
  if not encoder:
    encoder = hub.Module(module_url)
  embedding = encoder(text)
  if random_projection_matrix is not None:
    embedding = tf.matmul(
        embedding, tf.cast(random_projection_matrix, embedding.dtype))
  return embedding

----------------------------------------

TITLE: Logging Device Placement in TensorFlow
DESCRIPTION: Demonstrates how to log device assignments for operations and tensors by configuring the TensorFlow session with log_device_placement.

LANGUAGE: python
CODE:
# Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))

----------------------------------------

TITLE: Creating Input Placeholders for Skip-gram Model
DESCRIPTION: Defines placeholder tensors for input data (context words) and labels (target words) with specified batch sizes.

LANGUAGE: Python
CODE:
train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])

----------------------------------------

TITLE: Installing System Package unzip
DESCRIPTION: Uses apt-get to install the unzip utility, which is required for extracting the dataset.

LANGUAGE: Bash
CODE:
sudo apt-get install -y unzip

----------------------------------------

TITLE: Converting TF1 Checkpoint to TF2 Format
DESCRIPTION: Function to convert a TensorFlow 1.x checkpoint to TensorFlow 2.x format by loading variables and resaving them

LANGUAGE: Python
CODE:
def convert_tf1_to_tf2(checkpoint_path, output_prefix):
  vars = {}
  reader = tf.train.load_checkpoint(checkpoint_path)
  dtypes = reader.get_variable_to_dtype_map()
  for key in dtypes.keys():
    vars[key] = tf.Variable(reader.get_tensor(key))
  return tf.train.Checkpoint(vars=vars).save(output_prefix)

----------------------------------------

TITLE: Evaluating ESRGAN Performance with PSNR
DESCRIPTION: Calculates the Peak Signal-to-Noise Ratio (PSNR) between the original high-resolution image and the super-resolution image generated by ESRGAN.

LANGUAGE: Python
CODE:
psnr = tf.image.psnr(
    tf.clip_by_value(fake_image, 0, 255),
    tf.clip_by_value(hr_image, 0, 255), max_val=255)
print("PSNR Achieved: %f" % psnr)

----------------------------------------

TITLE: Launching TensorBoard from Command Line
DESCRIPTION: Command to start TensorBoard and view the histogram data from the specified log directory.

LANGUAGE: sh
CODE:
tensorboard --logdir=/tmp/histogram_example

----------------------------------------

TITLE: Installing TensorFlow Text
DESCRIPTION: Installs the TensorFlow Text package using pip.

LANGUAGE: Python
CODE:
!pip install "tensorflow-text==2.13.*"

----------------------------------------

TITLE: Gradient Computation with Intermediate Values
DESCRIPTION: Shows how to compute gradients with respect to intermediate values in a computation graph using GradientTape.

LANGUAGE: Python
CODE:
x = tf.ones((2, 2))

with tf.GradientTape() as t:
  t.watch(x)
  y = tf.reduce_sum(x)
  z = tf.multiply(y, y)

# Use the tape to compute the derivative of z with respect to the
# intermediate value y.
dz_dy = t.gradient(z, y)
assert dz_dy.numpy() == 8.0

----------------------------------------

TITLE: Setting up TensorFlow Environment
DESCRIPTION: Initial setup code to configure TensorFlow and import required dependencies

LANGUAGE: Python
CODE:
import os
import tensorflow as tf
os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'

----------------------------------------

TITLE: Helper Function for Visualization
DESCRIPTION: Defines a plotting function to display image examples with their labels and predictions in a grid layout

LANGUAGE: python
CODE:
def plot(examples, predictions=None):
  # Get the images, labels, and optionally predictions
  images = examples['image']
  labels = examples['label']
  batch_size = len(images)
  if predictions is None:
    predictions = batch_size * [None]

  # Configure the layout of the grid
  x = np.ceil(np.sqrt(batch_size))
  y = np.ceil(batch_size / x)
  fig = plt.figure(figsize=(x * 6, y * 7))

  for i, (image, label, prediction) in enumerate(zip(images, labels, predictions)):
    # Render the image
    ax = fig.add_subplot(x, y, i+1)
    ax.imshow(image, aspect='auto')
    ax.grid(False)
    ax.set_xticks([])
    ax.set_yticks([])

    # Display the label and optionally prediction
    x_label = 'Label: ' + name_map[class_names[label]]
    if prediction is not None:
      x_label = 'Prediction: ' + name_map[class_names[prediction]] + '\n' + x_label
      ax.xaxis.label.set_color('green' if label == prediction else 'red')
    ax.set_xlabel(x_label)

  plt.show()

----------------------------------------

TITLE: Importing Required Libraries for TensorFlow and NumPy
DESCRIPTION: Basic setup to import the required TensorFlow and NumPy libraries.

LANGUAGE: python
CODE:
import numpy as np
import tensorflow as tf

----------------------------------------

TITLE: Defining Helper Functions for Feature Conversion in TensorFlow
DESCRIPTION: Defines helper functions to convert values to tf.train.Feature types compatible with tf.train.Example messages.

LANGUAGE: Python
CODE:
def _bytes_feature(value):
  """Returns a bytes_list from a string / byte."""
  if isinstance(value, type(tf.constant(0))):
    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _float_feature(value):
  """Returns a float_list from a float / double."""
  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))

def _int64_feature(value):
  """Returns an int64_list from a bool / enum / int / uint."""
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

----------------------------------------

TITLE: Resetting Gradient Tape in TensorFlow
DESCRIPTION: Show how to reset gradient recording from scratch using tf.GradientTape.reset() method.

LANGUAGE: Python
CODE:
x = tf.Variable(2.0)
y = tf.Variable(3.0)
reset = True

with tf.GradientTape() as t:
  y_sq = y * y
  if reset:
    # Throw out all the tape recorded so far.
    t.reset()
  z = x * x + y_sq

grad = t.gradient(z, {'x': x, 'y': y})

print('dz/dx:', grad['x'])  # 2*x => 4
print('dz/dy:', grad['y'])

----------------------------------------

TITLE: Loading and Running SPICE Model
DESCRIPTION: Code to load the SPICE model from TensorFlow Hub and process audio samples to get pitch and uncertainty predictions

LANGUAGE: python
CODE:
model = hub.load("https://tfhub.dev/google/spice/2")

model_output = model.signatures["serving_default"](tf.constant(audio_samples, tf.float32))

pitch_outputs = model_output["pitch"]
uncertainty_outputs = model_output["uncertainty"]

confidence_outputs = 1.0 - uncertainty_outputs

----------------------------------------

TITLE: Exporting and reloading the trained model
DESCRIPTION: Saves the trained model as a SavedModel, then reloads it and verifies that it produces the same predictions.

LANGUAGE: Python
CODE:
export_path = "/tmp/saved_models/{}".format(int(time.time()))
model.save(export_path)

reloaded = tf.keras.models.load_model(export_path)

result_batch = model.predict(image_batch)
reloaded_result_batch = reloaded.predict(image_batch)

abs(reloaded_result_batch - result_batch).max()

----------------------------------------

TITLE: Installing Clang Compiler on Linux
DESCRIPTION: Commands to install LLVM/Clang 17 compiler required for building TensorFlow on Linux systems

LANGUAGE: bash
CODE:
sudo apt-get update && sudo apt-get install -y llvm-17 clang-17

----------------------------------------

TITLE: Custom training loop with MirroredStrategy
DESCRIPTION: Implements a custom training loop using MirroredStrategy for distributed training.

LANGUAGE: Python
CODE:
with mirrored_strategy.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(1, input_shape=(1,),
                            kernel_regularizer=tf.keras.regularizers.L2(1e-4))])
  optimizer = tf.keras.optimizers.SGD()

dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(
    global_batch_size)
dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)

loss_object = tf.keras.losses.BinaryCrossentropy(
  from_logits=True,
  reduction=tf.keras.losses.Reduction.NONE)

def train_step(inputs):
  features, labels = inputs

  with tf.GradientTape() as tape:
    predictions = model(features, training=True)
    per_example_loss = loss_object(labels, predictions)
    loss = tf.nn.compute_average_loss(per_example_loss)
    model_losses = model.losses
    if model_losses:
      loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))

  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  return loss

@tf.function
def distributed_train_step(dist_inputs):
  per_replica_losses = mirrored_strategy.run(train_step, args=(dist_inputs,))
  return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)

for dist_inputs in dist_dataset:
  print(distributed_train_step(dist_inputs))

----------------------------------------

TITLE: Resuming Training from Checkpoint in TensorFlow 2 Keras
DESCRIPTION: Recreates the Keras model and resumes training from the last saved checkpoint using the BackupAndRestore callback.

LANGUAGE: python
CODE:
model = create_model()
model.compile(optimizer='adam',
              loss=loss,
              metrics=['accuracy'],
              steps_per_execution=10)
model.fit(x=x_train,
            y=y_train,
            epochs=10,
            steps_per_epoch=100,
            validation_data=(x_test, y_test),
            callbacks=[backup_restore_callback])

----------------------------------------

TITLE: Initializing Word Embeddings in TensorFlow
DESCRIPTION: Creates a Variable tensor for word embeddings initialized with random uniform values between -1.0 and 1.0.

LANGUAGE: Python
CODE:
embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))

----------------------------------------

TITLE: Creating a Neural Network Model
DESCRIPTION: Defines a sequential neural network model with two hidden layers for penguin classification.

LANGUAGE: Python
CODE:
model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required
  tf.keras.layers.Dense(10, activation=tf.nn.relu),
  tf.keras.layers.Dense(3)
])

----------------------------------------

TITLE: Setting up TensorFlow Dependencies and Imports
DESCRIPTION: Imports required TensorFlow modules, utilities and visualization libraries for the tutorial

LANGUAGE: Python
CODE:
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import regularizers

print(tf.__version__)

----------------------------------------

TITLE: Installing Required Libraries for Semantic Search
DESCRIPTION: Installs Apache Beam, scikit-learn, and ANNOY libraries needed for the semantic search pipeline.

LANGUAGE: bash
CODE:
!pip install apache_beam
!pip install 'scikit_learn~=0.23.0'  # For gaussian_random_matrix.
!pip install annoy

----------------------------------------

TITLE: Implementing Gradient in Python for ZeroOut Op
DESCRIPTION: Example of implementing the gradient function for the 'ZeroOut' op in Python using tf.RegisterGradient.

LANGUAGE: Python
CODE:
from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import sparse_ops

@ops.RegisterGradient("ZeroOut")
def _zero_out_grad(op, grad):
  """The gradients for `zero_out`.

  Args:
    op: The `zero_out` `Operation` that we are differentiating, which we can use
      to find the inputs and outputs of the original op.
    grad: Gradient with respect to the output of the `zero_out` op.

  Returns:
    Gradients with respect to the input of `zero_out`.
  """
  to_zero = op.inputs[0]
  shape = array_ops.shape(to_zero)
  index = array_ops.zeros_like(shape)
  first_grad = array_ops.reshape(grad, [-1])[0]
  to_zero_grad = sparse_ops.sparse_to_dense([index], shape, first_grad, 0)
  return [to_zero_grad]  # List of one Tensor, since we have one input

----------------------------------------

TITLE: Complete Model Training Example
DESCRIPTION: Shows a full example of defining preprocessing layers, training a model, and performing inference

LANGUAGE: Python
CODE:
inputs = preprocessing_model.input
outputs = training_model(preprocessing_model(inputs))
inference_model = tf.keras.Model(inputs, outputs)

predict_dataset = tf.data.Dataset.from_tensor_slices(predict_features).batch(1)
inference_model.predict(predict_dataset)

----------------------------------------

TITLE: Defining Wav2Vec2 Model Architecture with Language Modeling Head
DESCRIPTION: Sets up the Wav2Vec2 model architecture by adding a dense layer (LM head) on top of the pre-trained model using Keras Functional API.

LANGUAGE: Python
CODE:
inputs = tf.keras.Input(shape=(AUDIO_MAXLEN,))
hidden_states = pretrained_layer(inputs)
outputs = tf.keras.layers.Dense(config.vocab_size)(hidden_states)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

----------------------------------------

TITLE: Creating a MIDI File from Note Data
DESCRIPTION: Defines a function to generate a MIDI file from a DataFrame of note information.

LANGUAGE: Python
CODE:
def notes_to_midi(
  notes: pd.DataFrame,
  out_file: str, 
  instrument_name: str,
  velocity: int = 100,  # note loudness
) -> pretty_midi.PrettyMIDI:

  pm = pretty_midi.PrettyMIDI()
  instrument = pretty_midi.Instrument(
      program=pretty_midi.instrument_name_to_program(
          instrument_name))

  prev_start = 0
  for i, note in notes.iterrows():
    start = float(prev_start + note['step'])
    end = float(start + note['duration'])
    note = pretty_midi.Note(
        velocity=velocity,
        pitch=int(note['pitch']),
        start=start,
        end=end,
    )
    instrument.notes.append(note)
    prev_start = start

  pm.instruments.append(instrument)
  pm.write(out_file)
  return pm

----------------------------------------

TITLE: Defining Linear Model Class
DESCRIPTION: Creates a custom TensorFlow module that implements a simple linear model with weight and bias variables.

LANGUAGE: Python
CODE:
class MyModel(tf.Module):
  def __init__(self, **kwargs):
    super().__init__(**kwargs)
    # Initialize the weights to `5.0` and the bias to `0.0`
    # In practice, these should be randomly initialized
    self.w = tf.Variable(5.0)
    self.b = tf.Variable(0.0)

  def __call__(self, x):
    return self.w * x + self.b

----------------------------------------

TITLE: Loading Class Labels for Bird Vocalization Classifier
DESCRIPTION: Reads the class labels from a CSV file included with the model assets.

LANGUAGE: python
CODE:
def class_names_from_csv(class_map_csv_text):
  """Returns list of class names corresponding to score vector."""
  with open(labels_path) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    class_names = [mid for mid, desc in csv_reader]
    return class_names[1:]

labels_path = hub.resolve(model_handle) + "/assets/label.csv"
classes = class_names_from_csv(labels_path)
print(classes)

----------------------------------------

TITLE: Basic Training Loop with Distributed Dataset
DESCRIPTION: Example of creating and iterating over a distributed dataset using MirroredStrategy

LANGUAGE: python
CODE:
global_batch_size = 16
mirrored_strategy = tf.distribute.MirroredStrategy()

dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(global_batch_size)
# Distribute input using the `experimental_distribute_dataset`.
dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)
# 1 global batch of data fed to the model in 1 step.
print(next(iter(dist_dataset)))

----------------------------------------

TITLE: Defining Prediction Function for Action Recognition
DESCRIPTION: Defines a function to predict actions in a video using the loaded I3D model, returning top 5 predicted actions.

LANGUAGE: Python
CODE:
def predict(sample_video):
    model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis, ...]

    logits = i3d(model_input)['default'][0]
    probabilities = tf.nn.softmax(logits)

    print("Top 5 actions:")
    for i in np.argsort(probabilities)[::-1][:5]:
        print(f"  {labels[i]:22}: {probabilities[i] * 100:5.2f}%")

----------------------------------------

TITLE: Hashing Tensors and Variables in TensorFlow 2
DESCRIPTION: Shows that tensors and variables are unhashable in TensorFlow 2 with tensor equality enabled, and demonstrates the resulting TypeError when attempting to add them to a set.

LANGUAGE: Python
CODE:
tf.compat.v1.enable_tensor_equality()
x = tf.Variable(0.0)

try:
  set([x, tf.constant(2.0)])
except TypeError as e:
  # TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.
  print(e)

----------------------------------------

TITLE: Loading Pre-trained Wav2Vec2 Model from TensorFlow Hub
DESCRIPTION: Loads the pre-trained Wav2Vec2 model from TensorFlow Hub using hub.KerasLayer.

LANGUAGE: Python
CODE:
pretrained_layer = hub.KerasLayer("https://tfhub.dev/vasudevgupta7/wav2vec2/1", trainable=True)

----------------------------------------

TITLE: Configuring Virtual CPUs for DTensor
DESCRIPTION: Sets up virtual CPU devices for simulating distributed training environment

LANGUAGE: Python
CODE:
def configure_virtual_cpus(ncpu):
  phy_devices = tf.config.list_physical_devices('CPU')
  tf.config.set_logical_device_configuration(phy_devices[0], [
        tf.config.LogicalDeviceConfiguration(),
    ] * ncpu)

configure_virtual_cpus(8)
DEVICES = [f'CPU:{i}' for i in range(8)]

----------------------------------------

TITLE: Programming with Multiple Graphs in TensorFlow
DESCRIPTION: Shows how to create and use multiple tf.Graph objects in the same program, allowing for better control over operation naming and resource management.

LANGUAGE: python
CODE:
g_1 = tf.Graph()
with g_1.as_default():
  # Operations created in this scope will be added to `g_1`.
  c = tf.constant("Node in g_1")

  # Sessions created in this scope will run operations from `g_1`.
  sess_1 = tf.Session()

g_2 = tf.Graph()
with g_2.as_default():
  # Operations created in this scope will be added to `g_2`.
  d = tf.constant("Node in g_2")

# Alternatively, you can pass a graph when constructing a `tf.Session`:
# `sess_2` will run operations from `g_2`.
sess_2 = tf.Session(graph=g_2)

assert c.graph is g_1
assert sess_1.graph is g_1

assert d.graph is g_2
assert sess_2.graph is g_2

----------------------------------------

TITLE: Loading Model with tf.saved_model API
DESCRIPTION: Load the saved model using tf.saved_model.load and perform inference.

LANGUAGE: Python
CODE:
DEFAULT_FUNCTION_KEY = 'serving_default'
loaded = tf.saved_model.load(saved_model_path)
inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]

predict_dataset = eval_dataset.map(lambda image, label: image)
for batch in predict_dataset.take(1):
  print(inference_func(batch))

----------------------------------------

TITLE: Saving and Restoring RNG State
DESCRIPTION: Demonstrates checkpointing and saving/restoring RNG state using tf.train.Checkpoint.

LANGUAGE: python
CODE:
filename = "./checkpoint"
g = tf.random.Generator.from_seed(1)
cp = tf.train.Checkpoint(generator=g)
print(g.normal([]))
cp.write(filename)
cp.restore(filename)

----------------------------------------

TITLE: Loading Graph Definition from File
DESCRIPTION: Loads a graph definition from either binary or text format based on the input flag. Uses ParseFromString for binary files and text_format.Merge for text files.

LANGUAGE: python
CODE:
  if FLAGS.input_binary:
    graph_def.ParseFromString(f.read())
  else:
    text_format.Merge(f.read(), graph_def)

----------------------------------------

TITLE: Creating a New TF1 Hub Module in Python
DESCRIPTION: Illustrates how to define a new TF1 Hub module using hub.create_module_spec() and a module_fn that constructs the graph and defines signatures.

LANGUAGE: Python
CODE:
def module_fn():
  inputs = tf.placeholder(dtype=tf.float32, shape=[None, 50])
  layer1 = tf.layers.dense(inputs, 200)
  layer2 = tf.layers.dense(layer1, 100)
  outputs = dict(default=layer2, hidden_activations=layer1)
  # Add default signature.
  hub.add_signature(inputs=inputs, outputs=outputs)

...
spec = hub.create_module_spec(module_fn)

----------------------------------------

TITLE: Custom Training Loop with AutoGraph
DESCRIPTION: Advanced example showing how to implement a complete training loop using AutoGraph, including batch loading, gradient calculation, and metric tracking.

LANGUAGE: Python
CODE:
def train(train_ds, test_ds, learning_rate, max_steps):
  m = mlp_model((28 * 28,))
  opt = tf.train.AdamOptimizer(learning_rate)

  train_losses = tf.TensorArray(tf.float32, size=0, dynamic_size=True, element_shape=())
  test_losses = tf.TensorArray(tf.float32, size=0, dynamic_size=True, element_shape=())
  train_accuracies = tf.TensorArray(tf.float32, size=0, dynamic_size=True, element_shape=())
  test_accuracies = tf.TensorArray(tf.float32, size=0, dynamic_size=True, element_shape=())

  i = tf.constant(0)
  for (train_x, train_y), (test_x, test_y) in tf.data.Dataset.zip((train_ds, test_ds)):
    step_train_loss, step_train_accuracy = fit(m, train_x, train_y, opt)
    step_test_loss, step_test_accuracy = predict(m, test_x, test_y)
    if i % 50 == 0:
      print('Step', i, 'train loss:', step_train_loss, 'test loss:',
            step_test_loss, 'train accuracy:', step_train_accuracy,
            'test accuracy:', step_test_accuracy)
    train_losses.append(step_train_loss)
    test_losses.append(step_test_loss)
    train_accuracies.append(step_train_accuracy)
    test_accuracies.append(step_test_accuracy)

    i += 1
    if i >= max_steps:
      break

  return (train_losses.stack(), test_losses.stack(),
          train_accuracies.stack(), test_accuracies.stack())

----------------------------------------

TITLE: Configuring Dense Layers in Keras
DESCRIPTION: Demonstrates different ways to configure Dense layers in Keras, including activation functions, regularization, and initialization.

LANGUAGE: Python
CODE:
# Create a sigmoid layer:
layers.Dense(64, activation='sigmoid')
# Or:
layers.Dense(64, activation=tf.sigmoid)

# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:
layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))

# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:
layers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01))

# A linear layer with a kernel initialized to a random orthogonal matrix:
layers.Dense(64, kernel_initializer='orthogonal')

# A linear layer with a bias vector initialized to 2.0s:
layers.Dense(64, bias_initializer=tf.keras.initializers.constant(2.0))

----------------------------------------

TITLE: Profiling with tf.profiler Function API
DESCRIPTION: Use tf.profiler Function API to start and stop profiling programmatically.

LANGUAGE: Python
CODE:
tf.profiler.experimental.start('logdir')
# Train the model here
tf.profiler.experimental.stop()

----------------------------------------

TITLE: TensorFlow Hub Model Loading and Embedding Computation
DESCRIPTION: Loads the Universal Sentence Encoder Multilingual Q&A model and computes embeddings for the SQuAD dataset sentences

LANGUAGE: python
CODE:
#@title Load model from tensorflow hub
module_url = "https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3"
model = hub.load(module_url)

#@title Compute embeddings and build simpleneighbors index
batch_size = 100

encodings = model.signatures['response_encoder'](
  input=tf.constant([sentences[0][0]]),
  context=tf.constant([sentences[0][1]]))
index = simpleneighbors.SimpleNeighbors(
    len(encodings['outputs'][0]), metric='angular')

----------------------------------------

TITLE: Loading and Preprocessing Credit Card Fraud Data
DESCRIPTION: Loads credit card transaction data from CSV, handles class imbalance, and prepares train/test splits with appropriate preprocessing.

LANGUAGE: Python
CODE:
raw_df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')

# Clean data
cleaned_df = raw_df.copy()
cleaned_df.pop('Time')
eps = 0.001
cleaned_df['Log Amount'] = np.log(cleaned_df.pop('Amount')+eps)

# Split dataset
train_df, test_df = train_test_split(cleaned_df, test_size=0.2)
train_df, val_df = train_test_split(train_df, test_size=0.2)

----------------------------------------

TITLE: Setting up TensorFlow and Dependencies
DESCRIPTION: Imports required libraries including TensorFlow, Keras, NumPy, Pandas, and scikit-learn for machine learning tasks.

LANGUAGE: Python
CODE:
import tensorflow as tf
from tensorflow import keras

import os
import tempfile

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

----------------------------------------

TITLE: Setting MKL Environment Variables for TensorFlow Performance
DESCRIPTION: This bash command shows how to set MKL-related environment variables when running a Python script to optimize TensorFlow performance on CPUs.

LANGUAGE: bash
CODE:
KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \
KMP_SETTINGS=1 python your_python_script.py

----------------------------------------

TITLE: Training Wav2Vec2 Model for Automatic Speech Recognition
DESCRIPTION: Compiles and trains the Wav2Vec2 model using the prepared dataset.

LANGUAGE: Python
CODE:
model.compile(optimizer, loss=loss_fn)
history = model.fit(train_dataset, validation_data=val_dataset, epochs=3)
history.history

----------------------------------------

TITLE: Defining Loss and Gradient Functions
DESCRIPTION: Creates functions to calculate the loss and gradients for model training.

LANGUAGE: Python
CODE:
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

def loss(model, x, y, training):
  y_ = model(x, training=training)
  return loss_object(y_true=y, y_pred=y_)

def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value = loss(model, inputs, targets, training=True)
  return loss_value, tape.gradient(loss_value, model.trainable_variables)

----------------------------------------

TITLE: Importing Libraries for Video Classification
DESCRIPTION: Imports required Python libraries for data processing, visualization, and machine learning, including TensorFlow and Keras.

LANGUAGE: Python
CODE:
import tqdm
import random
import pathlib
import itertools
import collections

import cv2
import einops
import numpy as np
import remotezip as rz
import seaborn as sns
import matplotlib.pyplot as plt

import tensorflow as tf
import keras
from keras import layers

----------------------------------------

TITLE: Using TensorFlow Hub Model in Python
DESCRIPTION: Demonstrates how to import a pre-trained text embedding model from TensorFlow Hub and use it to generate embeddings for a list of sentences. The example shows the simplicity of integrating TensorFlow Hub models into a Python program.

LANGUAGE: python
CODE:
import tensorflow_hub as hub

model = hub.KerasLayer("https://tfhub.dev/google/nnlm-en-dim128/2")
embeddings = model(["The rain in Spain.", "falls",
                    "mainly", "In the plain!"])

print(embeddings.shape)  #(4,128)

----------------------------------------

TITLE: Registering a Basic Op in C++
DESCRIPTION: Example of registering a simple 'ZeroOut' op that takes an int32 tensor as input and outputs an int32 tensor.

LANGUAGE: C++
CODE:
#include "tensorflow/core/framework/op.h"
#include "tensorflow/core/framework/shape_inference.h"

using namespace tensorflow;

REGISTER_OP("ZeroOut")
    .Input("to_zero: int32")
    .Output("zeroed: int32")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(0));
      return Status::OK();
    });

----------------------------------------

TITLE: Maximizing L2 Cache Fetch Granularity
DESCRIPTION: Execute this code snippet before the training loop to max out the L2 fetch granularity to 128 bytes on NVIDIA GPUs.

LANGUAGE: Python
CODE:
import ctypes

_libcudart = ctypes.CDLL('libcudart.so')
# Set device limit on the current device
# cudaLimitMaxL2FetchGranularity = 0x05
pValue = ctypes.cast((ctypes.c_int*1)(), ctypes.POINTER(ctypes.c_int))
_libcudart.cudaDeviceSetLimit(ctypes.c_int(0x05), ctypes.c_int(128))
_libcudart.cudaDeviceGetLimit(pValue, ctypes.c_int(0x05))
assert pValue.contents.value == 128

----------------------------------------

TITLE: Computing BigBiGAN Reconstructions
DESCRIPTION: Generates reconstructions of input images using the BigBiGAN encoder and generator.

LANGUAGE: Python
CODE:
test_images_batch = test_images[:16]
_out_recons = sess.run(recon_x, feed_dict={enc_ph: test_images_batch})
print('reconstructions shape:', _out_recons.shape)

inputs_and_recons = interleave(test_images_batch, _out_recons)
print('inputs_and_recons shape:', inputs_and_recons.shape)
imshow(imgrid(image_to_uint8(inputs_and_recons), cols=2))

----------------------------------------

TITLE: Converting Between Dense and Sparse Tensors
DESCRIPTION: Shows how to convert between dense and sparse tensor representations using tf.sparse.from_dense and tf.sparse.to_dense.

LANGUAGE: python
CODE:
st2 = tf.sparse.from_dense([[1, 0, 0, 8], [0, 0, 0, 0], [0, 0, 3, 0]])
st3 = tf.sparse.to_dense(st2)

----------------------------------------

TITLE: Data Preprocessing Function
DESCRIPTION: Defines a preprocessing function that normalizes images to [0,1] range and resizes them to 224x224 pixels as required by the model

LANGUAGE: python
CODE:
def preprocess_fn(data):
  image = data['image']

  # Normalize [0, 255] to [0, 1]
  image = tf.cast(image, tf.float32)
  image = image / 255.

  # Resize the images to 224 x 224
  image = tf.image.resize(image, (224, 224))

  data['image'] = image
  return data

----------------------------------------

TITLE: Profiling Custom Training Loops
DESCRIPTION: Instrument custom training loops with tf.profiler.experimental.Trace API for step-based profiling.

LANGUAGE: Python
CODE:
for step in range(NUM_STEPS):
    with tf.profiler.experimental.Trace('train', step_num=step, _r=1):
        train_data = next(dataset)
        train_step(train_data)

----------------------------------------

TITLE: Defining Conv2Plus1D Layer for 3D CNN
DESCRIPTION: Creates a custom Keras layer that implements (2+1)D convolution, separating spatial and temporal convolutions for video processing.

LANGUAGE: Python
CODE:
class Conv2Plus1D(keras.layers.Layer):
  def __init__(self, filters, kernel_size, padding):
    """
      A sequence of convolutional layers that first apply the convolution operation over the
      spatial dimensions, and then the temporal dimension. 
    """
    super().__init__()
    self.seq = keras.Sequential([  
        # Spatial decomposition
        layers.Conv3D(filters=filters,
                      kernel_size=(1, kernel_size[1], kernel_size[2]),
                      padding=padding),
        # Temporal decomposition
        layers.Conv3D(filters=filters, 
                      kernel_size=(kernel_size[0], 1, 1),
                      padding=padding)
        ])
  
  def call(self, x):
    return self.seq(x)

----------------------------------------

TITLE: Frame Generator Class Implementation
DESCRIPTION: Class that generates frames and labels from video files for use in TensorFlow data pipelines

LANGUAGE: Python
CODE:
class FrameGenerator:
  def __init__(self, path, n_frames, training = False):
    self.path = path
    self.n_frames = n_frames
    self.training = training
    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))
    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))

  def get_files_and_class_names(self):
    video_paths = list(self.path.glob('*/*.avi'))
    classes = [p.parent.name for p in video_paths] 
    return video_paths, classes

  def __call__(self):
    video_paths, classes = self.get_files_and_class_names()
    pairs = list(zip(video_paths, classes))
    if self.training:
      random.shuffle(pairs)
    for path, name in pairs:
      video_frames = frames_from_video_file(path, self.n_frames) 
      label = self.class_ids_for_name[name]
      yield video_frames, label

----------------------------------------

TITLE: Defining a Toy Dataset
DESCRIPTION: Creates a simple dataset for demonstration purposes.

LANGUAGE: Python
CODE:
def toy_dataset():
  inputs = tf.range(10.)[:, None]
  labels = inputs * 5. + tf.range(5.)[None, :]
  return tf.data.Dataset.from_tensor_slices(
    dict(x=inputs, y=labels)).repeat().batch(2)

----------------------------------------

TITLE: Loading Text Dataset from Directory
DESCRIPTION: Uses tf.keras.utils.text_dataset_from_directory to create labeled tf.data.Dataset from text files in directories.

LANGUAGE: Python
CODE:
raw_train_ds = utils.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset='training',
    seed=seed)

----------------------------------------

TITLE: Defining and Compiling Neural Network Model
DESCRIPTION: Creates and compiles a sequential neural network model with flatten and dense layers using Keras.

LANGUAGE: python
CODE:
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])

model.compile(optimizer=tf.keras.optimizers.RMSprop(),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['sparse_categorical_accuracy'])

----------------------------------------

TITLE: Importing TensorFlow
DESCRIPTION: Imports the TensorFlow library for use in the notebook.

LANGUAGE: Python
CODE:
import tensorflow as tf

----------------------------------------

TITLE: Installing Python Dependencies on Ubuntu
DESCRIPTION: Commands to install Python and pip package dependencies required for building TensorFlow on Ubuntu Linux

LANGUAGE: bash
CODE:
sudo apt install python3-dev python3-pip

----------------------------------------

TITLE: Generate Test WAV File
DESCRIPTION: Command to generate synthetic test data for evaluating streaming accuracy.

LANGUAGE: bash
CODE:
bazel run tensorflow/examples/speech_commands:generate_streaming_test_wav

----------------------------------------

TITLE: Training TensorFlow 2 Keras Model with Fault Tolerance
DESCRIPTION: Trains the Keras model using Model.fit with the BackupAndRestore callback and the custom interrupt callback to simulate a failure.

LANGUAGE: python
CODE:
try:
  model.fit(x=x_train,
            y=y_train,
            epochs=10,
            steps_per_epoch=100,
            validation_data=(x_test, y_test),
            callbacks=[backup_restore_callback, InterruptAtEpoch()])
except Exception as e:
  print(f'{type(e).__name__}:{e}')

----------------------------------------

TITLE: Loading and Selecting Boundless Model from TensorFlow Hub in Python
DESCRIPTION: This code snippet allows selection of different Boundless models (Half, Quarter, Three Quarters) from TensorFlow Hub and loads the chosen model.

LANGUAGE: Python
CODE:
#@title Model Selection { display-mode: "form" }
model_name = 'Boundless Quarter' # @param ['Boundless Half', 'Boundless Quarter', 'Boundless Three Quarters']
model_handle_map = {
    'Boundless Half' : 'https://tfhub.dev/google/boundless/half/1',
    'Boundless Quarter' : 'https://tfhub.dev/google/boundless/quarter/1', 
    'Boundless Three Quarters' : 'https://tfhub.dev/google/boundless/three_quarter/1'
}

model_handle = model_handle_map[model_name]

print("Loading model {} ({})".format(model_name, model_handle))
model = hub.load(model_handle)

----------------------------------------

TITLE: Creating Non-Trainable Variable
DESCRIPTION: Demonstrates creating a non-trainable TensorFlow variable using the trainable parameter.

LANGUAGE: python
CODE:
my_non_trainable = tf.get_variable("my_non_trainable",
                                   shape=(),
                                   trainable=False)

----------------------------------------

TITLE: Creating a new branch in Git
DESCRIPTION: Command to create and switch to a new branch in Git. This is used to isolate new work in the TensorFlow contribution process.

LANGUAGE: Bash
CODE:
$ git checkout -b new-branch-name

----------------------------------------

TITLE: Loading Audio Dataset and Creating Spectrogram Datasets
DESCRIPTION: This code loads the audio dataset using TensorFlow's audio_dataset_from_directory function and creates spectrogram datasets for training, validation, and testing.

LANGUAGE: Python
CODE:
train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(
    directory=data_dir,
    batch_size=64,
    validation_split=0.2,
    seed=0,
    output_sequence_length=16000,
    subset='both')

def get_spectrogram(waveform):
  spectrogram = tf.signal.stft(
      waveform, frame_length=255, frame_step=128)
  spectrogram = tf.abs(spectrogram)
  spectrogram = spectrogram[..., tf.newaxis]
  return spectrogram

def make_spec_ds(ds):
  return ds.map(
      map_func=lambda audio,label: (get_spectrogram(audio), label),
      num_parallel_calls=tf.data.AUTOTUNE)

train_spectrogram_ds = make_spec_ds(train_ds)
val_spectrogram_ds = make_spec_ds(val_ds)
test_spectrogram_ds = make_spec_ds(test_ds)

----------------------------------------

TITLE: Configuring TensorFlow Session for CPU Optimization
DESCRIPTION: This code snippet shows how to configure a TensorFlow session to optimize CPU performance by setting intra_op and inter_op parallelism threads.

LANGUAGE: python
CODE:
config = tf.ConfigProto()
config.intra_op_parallelism_threads = 44
config.inter_op_parallelism_threads = 44
tf.Session(config=config)

----------------------------------------

TITLE: Training and Evaluating the Model
DESCRIPTION: Trains the model on the training dataset and evaluates it on the test dataset.

LANGUAGE: python
CODE:
model.fit(train_dataset, epochs=10)
model.evaluate(test_dataset)

----------------------------------------

TITLE: Creating Categorical Identity Columns in TensorFlow
DESCRIPTION: Demonstrates how to create categorical identity columns for integer features with a fixed range of values.

LANGUAGE: python
CODE:
# Create categorical output for an integer feature named "my_feature_b",
# The values of my_feature_b must be >= 0 and < num_buckets
identity_feature_column = tf.feature_column.categorical_column_with_identity(
    key='my_feature_b',
    num_buckets=4) # Values [0, 4)

# In order for the preceding call to work, the input_fn() must return
# a dictionary containing 'my_feature_b' as a key. Furthermore, the values
# assigned to 'my_feature_b' must belong to the set [0, 4).
def input_fn():
    ...
    return ({ 'my_feature_a':[7, 9, 5, 2], 'my_feature_b':[3, 1, 2, 2] },
            [Label_values])

----------------------------------------

TITLE: Image Normalization Function
DESCRIPTION: Function to normalize input images to [0,1] range and adjust mask labels

LANGUAGE: Python
CODE:
def normalize(input_image, input_mask):
  input_image = tf.cast(input_image, tf.float32) / 255.0
  input_mask -= 1
  return input_image, input_mask

----------------------------------------

TITLE: Initializing TensorFlow and Virtual Devices
DESCRIPTION: Sets up TensorFlow imports and creates virtual CPU devices for distribution strategy examples.

LANGUAGE: python
CODE:
import tensorflow as tf

physical_devices = tf.config.list_physical_devices("CPU")
tf.config.experimental.set_virtual_device_configuration(
    physical_devices[0], [
        tf.config.experimental.VirtualDeviceConfiguration(),
        tf.config.experimental.VirtualDeviceConfiguration(),
        tf.config.experimental.VirtualDeviceConfiguration()
    ])

----------------------------------------

TITLE: Custom Training Loop with Loss Scaling
DESCRIPTION: Implements a custom training loop that properly handles loss scaling required for mixed precision training.

LANGUAGE: Python
CODE:
@tf.function
def train_step(x, y):
  with tf.GradientTape() as tape:
    predictions = model(x)
    loss = loss_object(y, predictions)
    scaled_loss = optimizer.get_scaled_loss(loss)
  scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)
  gradients = optimizer.get_unscaled_gradients(scaled_gradients)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  return loss

----------------------------------------

TITLE: Building a Sequential Model in TensorFlow
DESCRIPTION: This code builds a simple sequential model using TensorFlow's Keras API. The model consists of two Dense layers, with the first layer having 10 units and ReLU activation, and the second layer having 3 units.

LANGUAGE: python
CODE:
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(None, 5)),
    tf.keras.layers.Dense(3)
])

----------------------------------------

TITLE: Importing TensorFlow Dependencies
DESCRIPTION: Basic TensorFlow and NumPy imports required for tensor operations

LANGUAGE: Python
CODE:
import tensorflow as tf
import numpy as np

----------------------------------------

TITLE: Using TFDBG with TF Estimator
DESCRIPTION: Code example showing how to use tfdbg with TensorFlow Estimator API

LANGUAGE: python
CODE:
from tensorflow.python import debug as tf_debug

hooks = [tf_debug.LocalCLIDebugHook()]

classifier.train(input_fn,
                steps=1000, 
                hooks=hooks)

----------------------------------------

TITLE: Visualizing Boundless Model Output in Python
DESCRIPTION: This function creates a side-by-side visualization of the original image, the masked image, and the generated (filled) image produced by the Boundless model.

LANGUAGE: Python
CODE:
def visualize_output_comparison(img_original, img_masked, img_filled):
  plt.figure(figsize=(24,12))
  plt.subplot(131)
  plt.imshow((np.squeeze(img_original)))
  plt.title("Original", fontsize=24)
  plt.axis('off')
  plt.subplot(132)
  plt.imshow((np.squeeze(img_masked)))
  plt.title("Masked", fontsize=24)
  plt.axis('off')
  plt.subplot(133)
  plt.imshow((np.squeeze(img_filled)))
  plt.title("Generated", fontsize=24)
  plt.axis('off')
  plt.show()

----------------------------------------

TITLE: Setting up TensorFlow Dependencies
DESCRIPTION: Import required TensorFlow libraries and dependencies for transfer learning

LANGUAGE: Python
CODE:
import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf

----------------------------------------

TITLE: Implementing Text Generation Loop
DESCRIPTION: Defines the token-by-token text generation process using the loaded language model

LANGUAGE: python
CODE:
def feedforward_step(module, inputs, mems):
  inputs = tf.dtypes.cast(inputs, tf.int64)
  generation_input_dict = dict(input_tokens=inputs)
  mems_dict = {"mem_{}".format(i): mems[i] for i in range(n_layer)}
  generation_input_dict.update(mems_dict)

  generation_outputs = module(generation_input_dict, signature="prediction", as_dict=True)

  probs = generation_outputs["probs"]
  new_mems = [generation_outputs["new_mem_{}".format(i)] for i in range(n_layer)]

  return probs, new_mems

----------------------------------------

TITLE: Performing Semantic Similarity Search
DESCRIPTION: Implements a function to find semantically similar items for a given query using the built Annoy index.

LANGUAGE: Python
CODE:
def find_similar_items(embedding, num_matches=5):
  ids = index.get_nns_by_vector(
  embedding, num_matches, search_k=-1, include_distances=False)
  items = [mapping[i] for i in ids]
  return items

----------------------------------------

TITLE: Reading and Preprocessing Images for Boundless Model in Python
DESCRIPTION: This function reads an image from a file or URL, crops it to a square, resizes it to 257x257 pixels, and normalizes the pixel values. It prepares the image for input to the Boundless model.

LANGUAGE: Python
CODE:
def read_image(filename):
  fd = None
  if(filename.startswith('http')):
    fd = urlopen(filename)
  else:
    fd = tf.io.gfile.GFile(filename, 'rb')

  pil_image = PilImage.open(fd)
  width, height = pil_image.size
  # crop to make the image square
  pil_image = pil_image.crop((0, 0, height, height))
  pil_image = pil_image.resize((257,257),PilImage.LANCZOS)
  image_unscaled = np.array(pil_image)
  image_np = np.expand_dims(
      image_unscaled.astype(np.float32) / 255., axis=0)
  return image_np

----------------------------------------

TITLE: Placing Input Pipeline on CPU in TensorFlow
DESCRIPTION: This Python code snippet demonstrates how to explicitly place input pipeline operations on the CPU to improve overall performance by freeing up the GPU for training.

LANGUAGE: python
CODE:
with tf.device('/cpu:0'):
  # function to get and process images or data.
  distorted_inputs = load_and_distort_images()

----------------------------------------

TITLE: Type Promotion Example - ALL Mode
DESCRIPTION: Demonstrates type promotion between int32 and float32 in ALL mode, which allows potentially unsafe conversions.

LANGUAGE: Python
CODE:
a = tf.constant(10, dtype = tf.int32)
b = tf.constant(5.0, dtype = tf.float32)
a + b

----------------------------------------

TITLE: Defining a Simple Sequential Model in TensorFlow
DESCRIPTION: Creates a function to build a simple sequential model with dense layers and dropout, and compiles it with Adam optimizer and sparse categorical crossentropy loss.

LANGUAGE: python
CODE:
def create_model():
  model = tf.keras.Sequential([
    keras.layers.Dense(512, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10)
  ])

  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

  return model

# Create a basic model instance
model = create_model()

# Display the model's architecture
model.summary()

----------------------------------------

TITLE: Creating and Transforming TensorFlow Datasets
DESCRIPTION: Shows how to create and transform datasets using tf.data.Dataset API, including tensor slicing and file reading.

LANGUAGE: python
CODE:
ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])

# Create a CSV file
import tempfile
_, filename = tempfile.mkstemp()

with open(filename, 'w') as f:
  f.write("""Line 1
Line 2
Line 3
  """)

ds_file = tf.data.TextLineDataset(filename)

ds_tensors = ds_tensors.map(tf.math.square).shuffle(2).batch(2)
ds_file = ds_file.batch(2)

----------------------------------------

TITLE: Defining Synthesis Transform for MNIST Compression
DESCRIPTION: Creates the synthesis (decoder) transform using dense layers, reshaping, and transposed convolutions.

LANGUAGE: Python
CODE:
def make_synthesis_transform():
  """Creates the synthesis (decoder) transform."""
  return tf.keras.Sequential([
      tf.keras.layers.Dense(
          500, use_bias=True, activation="leaky_relu", name="fc_1"),
      tf.keras.layers.Dense(
          2450, use_bias=True, activation="leaky_relu", name="fc_2"),
      tf.keras.layers.Reshape((7, 7, 50)),
      tf.keras.layers.Conv2DTranspose(
          20, 5, use_bias=True, strides=2, padding="same",
          activation="leaky_relu", name="conv_1"),
      tf.keras.layers.Conv2DTranspose(
          1, 5, use_bias=True, strides=2, padding="same",
          activation="leaky_relu", name="conv_2"),
  ], name="synthesis_transform")


----------------------------------------

TITLE: Creating Keras Layer from TF-Hub Module
DESCRIPTION: Creates a Keras layer from the exported TensorFlow Hub module for use in the model.

LANGUAGE: Python
CODE:
module_path = "text_module"
embedding_layer = hub.KerasLayer(module_path, trainable=False)

----------------------------------------

TITLE: Importing TensorFlow and Matplotlib
DESCRIPTION: Import the required libraries TensorFlow and Matplotlib for computation and visualization.

LANGUAGE: Python
CODE:
import tensorflow as tf

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams['figure.figsize'] = (8, 6)

----------------------------------------

TITLE: Loading Abalone CSV Dataset
DESCRIPTION: Loading a small CSV dataset into a pandas DataFrame using direct URL access

LANGUAGE: Python
CODE:
abalone_train = pd.read_csv(
    "https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv",
    names=["Length", "Diameter", "Height", "Whole weight", "Shucked weight",
           "Viscera weight", "Shell weight", "Age"])

----------------------------------------

TITLE: Exporting FastText Embeddings to TF-Hub Module
DESCRIPTION: Runs the exporter script to convert FastText embeddings to a TensorFlow Hub module, using the first 100,000 tokens.

LANGUAGE: Bash
CODE:
python export_v2.py --embedding_file=cc.bn.300.vec --export_path=text_module --num_lines_to_ignore=1 --num_lines_to_use=100000

----------------------------------------

TITLE: Creating Bucketized Feature Columns in TensorFlow
DESCRIPTION: Shows how to convert numeric values into categorical buckets by specifying boundary values. Useful for converting continuous data into discrete ranges.

LANGUAGE: python
CODE:
# First, convert the raw input to a numeric column.
numeric_feature_column = tf.feature_column.numeric_column("Year")

# Then, bucketize the numeric column on the years 1960, 1980, and 2000.
bucketized_feature_column = tf.feature_column.bucketized_column(
    source_column = numeric_feature_column,
    boundaries = [1960, 1980, 2000])

----------------------------------------

TITLE: Frame Interpolator Class Implementation
DESCRIPTION: Implements a wrapper class for the FILM model to handle frame interpolation with proper input padding and processing

LANGUAGE: python
CODE:
class Interpolator:
  def __init__(self, align: int = 64) -> None:
    self._model = hub.load("https://tfhub.dev/google/film/1")
    self._align = align

  def __call__(self, x0: np.ndarray, x1: np.ndarray, dt: np.ndarray) -> np.ndarray:
    if self._align is not None:
      x0, bbox_to_crop = _pad_to_align(x0, self._align)
      x1, _ = _pad_to_align(x1, self._align)

    inputs = {'x0': x0, 'x1': x1, 'time': dt[..., np.newaxis]}
    result = self._model(inputs, training=False)
    image = result['image']

    if self._align is not None:
      image = tf.image.crop_to_bounding_box(image, **bbox_to_crop)
    return image.numpy()

----------------------------------------

TITLE: Loading BigBiGAN TensorFlow Hub Module
DESCRIPTION: Loads a pre-trained BigBiGAN model from TensorFlow Hub and displays its available functionality.

LANGUAGE: Python
CODE:
module = hub.Module(module_path)  # inference

for signature in module.get_signature_names():
    print('Signature:', signature)
    print('Inputs:', pformat(module.get_input_info_dict(signature)))
    print('Outputs:', pformat(module.get_output_info_dict(signature)))
    print()

----------------------------------------

TITLE: Running Inference on Video with Cropping Algorithm
DESCRIPTION: This code snippet demonstrates how to run MoveNet inference on a video (image sequence) using an intelligent cropping algorithm. It loads a GIF file, processes each frame, and creates an output GIF with detected keypoints visualized.

LANGUAGE: Python
CODE:
# Load the input image.
num_frames, image_height, image_width, _ = image.shape
crop_region = init_crop_region(image_height, image_width)

output_images = []
bar = display(progress(0, num_frames-1), display_id=True)
for frame_idx in range(num_frames):
  keypoints_with_scores = run_inference(
      movenet, image[frame_idx, :, :, :], crop_region,
      crop_size=[input_size, input_size])
  output_images.append(draw_prediction_on_image(
      image[frame_idx, :, :, :].numpy().astype(np.int32),
      keypoints_with_scores, crop_region=None,
      close_figure=True, output_image_height=300))
  crop_region = determine_crop_region(
      keypoints_with_scores, image_height, image_width)
  bar.update(progress(frame_idx, num_frames-1))

# Prepare gif visualization.
output = np.stack(output_images, axis=0)
to_gif(output, duration=100)

----------------------------------------

TITLE: Basic Control Flow Conversion with AutoGraph
DESCRIPTION: Demonstrates how AutoGraph converts a simple Python function with if/else control flow into TensorFlow graph operations.

LANGUAGE: Python
CODE:
def square_if_positive(x):
  if x > 0:
    x = x * x
  else:
    x = 0.0
  return x

----------------------------------------

TITLE: Implementing Shape Function for ZeroOut Op in C++
DESCRIPTION: Example of implementing a shape function for the 'ZeroOut' op to provide shape inference capabilities.

LANGUAGE: C++
CODE:
REGISTER_OP("ZeroOut")
    .Input("to_zero: int32")
    .Output("zeroed: int32")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      ::tensorflow::shape_inference::ShapeHandle input;
      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &input));
      c->set_output(0, input);
      return Status::OK();
    });

----------------------------------------

TITLE: Importing Libraries and Defining Helper Functions
DESCRIPTION: Sets up necessary imports and defines utility functions for image manipulation, vector interpolation, and animation generation. Includes functions for hypersphere interpolation and image display.

LANGUAGE: Python
CODE:
#@title Imports and function definitions
from absl import logging
import imageio
import PIL.Image
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
tf.random.set_seed(0)
import tensorflow_hub as hub
from tensorflow_docs.vis import embed
import time

try:
  from google.colab import files
except ImportError:
  pass

from IPython import display
from skimage import transform

latent_dim = 512

def interpolate_hypersphere(v1, v2, num_steps):
  v1_norm = tf.norm(v1)
  v2_norm = tf.norm(v2)
  v2_normalized = v2 * (v1_norm / v2_norm)

  vectors = []
  for step in range(num_steps):
    interpolated = v1 + (v2_normalized - v1) * step / (num_steps - 1)
    interpolated_norm = tf.norm(interpolated)
    interpolated_normalized = interpolated * (v1_norm / interpolated_norm)
    vectors.append(interpolated_normalized)
  return tf.stack(vectors)

def display_image(image):
  image = tf.constant(image)
  image = tf.image.convert_image_dtype(image, tf.uint8)
  return PIL.Image.fromarray(image.numpy())

def animate(images):
  images = np.array(images)
  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)
  imageio.mimsave('./animation.gif', converted_images)
  return embed.embed_file('./animation.gif')

logging.set_verbosity(logging.ERROR)

----------------------------------------

TITLE: Creating Probability Model with Softmax
DESCRIPTION: Wrap trained model with softmax layer to get probability outputs.

LANGUAGE: python
CODE:
probability_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])

----------------------------------------

TITLE: Importing TensorFlow and Required Libraries
DESCRIPTION: Import TensorFlow and other necessary libraries for the text generation project.

LANGUAGE: Python
CODE:
import tensorflow.compat.v1 as tf

import numpy as np
import os
import time

----------------------------------------

TITLE: DELF Model Loading and Feature Extraction
DESCRIPTION: Loads the DELF model from TensorFlow Hub and implements feature extraction function for input images

LANGUAGE: python
CODE:
delf = hub.load('https://tfhub.dev/google/delf/1').signatures['default']

def run_delf(image):
  np_image = np.array(image)
  float_image = tf.image.convert_image_dtype(np_image, tf.float32)

  return delf(
      image=float_image,
      score_threshold=tf.constant(100.0),
      image_scales=tf.constant([0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0]),
      max_feature_num=tf.constant(1000))

----------------------------------------

TITLE: Creating Basic MNIST Classification Model with TensorFlow Keras
DESCRIPTION: Demonstrates how to build and train a basic neural network for MNIST digit classification using TensorFlow's Keras API. The model includes flatten, dense, and dropout layers with relu and softmax activations. The example shows data loading, model compilation, training and evaluation.

LANGUAGE: python
CODE:
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)

----------------------------------------

TITLE: Loading Heart Disease Dataset
DESCRIPTION: Downloads and loads the heart disease dataset using TensorFlow utilities and pandas.

LANGUAGE: Python
CODE:
csv_file = tf.keras.utils.get_file('heart.csv', 'https://storage.googleapis.com/download.tensorflow.org/data/heart.csv')
df = pd.read_csv(csv_file)

----------------------------------------

TITLE: Saving Custom Gradients in SavedModel
DESCRIPTION: Demonstrate how to save and load custom gradients using SavedModel with the experimental_custom_gradients option.

LANGUAGE: Python
CODE:
class MyModule(tf.Module):

  @tf.function(input_signature=[tf.TensorSpec(None)])
  def call_custom_grad(self, x):
    return clip_gradients(x)

model = MyModule()

tf.saved_model.save(
    model,
    'saved_model',
    options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))

# The loaded gradients will be the same as the above example.
v = tf.Variable(2.0)
loaded = tf.saved_model.load('saved_model')
with tf.GradientTape() as t:
  output = loaded.call_custom_grad(v * v)
print(t.gradient(output, v))

----------------------------------------

TITLE: Creating a Sequential Model in TensorFlow
DESCRIPTION: This snippet shows how to create a Sequential model using tf.keras.Sequential.

LANGUAGE: Python
CODE:
my_seq = tf.keras.Sequential([tf.keras.layers.Conv2D(1, (1, 1),
                                                    input_shape=(
                                                        None, None, 3)),
                             tf.keras.layers.BatchNormalization(),
                             tf.keras.layers.Conv2D(2, 1,
                                                    padding='same'),
                             tf.keras.layers.BatchNormalization(),
                             tf.keras.layers.Conv2D(3, (1, 1)),
                             tf.keras.layers.BatchNormalization()])
my_seq(tf.zeros([1, 2, 3, 3]))

my_seq.summary()

----------------------------------------

TITLE: Using Image Feature Vector Model with Keras
DESCRIPTION: Demonstrates how to use the image feature extraction model within Keras using KerasLayer wrapper.

LANGUAGE: python
CODE:
features = hub.KerasLayer("path/to/model")(images)

----------------------------------------

TITLE: Defining Functions in Python
DESCRIPTION: Shows how to define custom functions in TensorFlow using Python.

LANGUAGE: Python
CODE:
tf.python.framework.function.Defun

----------------------------------------

TITLE: Looping with Continue Statement
DESCRIPTION: Shows how AutoGraph handles Python loops with continue statements by converting them to equivalent graph operations.

LANGUAGE: Python
CODE:
def sum_even(items):
  s = 0
  for c in items:
    if c % 2 > 0:
      continue
    s += c
  return s

----------------------------------------

TITLE: Using tf.gather for Index-based Selection
DESCRIPTION: Demonstrates using tf.gather to select specific indices from tensors, including non-sequential selections

LANGUAGE: Python
CODE:
alphabet = tf.constant(list('abcdefghijklmnopqrstuvwxyz'))

print(tf.gather(alphabet,
                indices=[2, 0, 19, 18]))

----------------------------------------

TITLE: Basic Text Embedding Usage with TensorFlow Hub
DESCRIPTION: Demonstrates loading and using a text embedding model directly from TensorFlow Hub to generate embeddings from text input.

LANGUAGE: python
CODE:
obj = hub.load("path/to/model")
text_input = ["A long sentence.",
              "single-word",
              "http://example.com"]
embeddings = obj(text_input)

----------------------------------------

TITLE: Basic TensorFlow Dataset Creation
DESCRIPTION: Creates a simple TensorFlow dataset using tf.data.Dataset.range() and demonstrates basic iteration.

LANGUAGE: Python
CODE:
dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])

for elem in dataset:
  print(elem.numpy())

----------------------------------------

TITLE: Loading and Preprocessing Fashion MNIST Dataset
DESCRIPTION: Load the Fashion MNIST dataset and normalize the pixel values for training.

LANGUAGE: python
CODE:
(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()

# Normalize pixel values between 0 and 1
img_train = img_train.astype('float32') / 255.0
img_test = img_test.astype('float32') / 255.0

----------------------------------------

TITLE: Loading and Preprocessing MNIST Dataset
DESCRIPTION: Loads the MNIST dataset, normalizes the images, and prepares training and validation datasets.

LANGUAGE: python
CODE:
def normalize_img(image, label):
  """Normalizes images: `uint8` -> `float32`."""
  return tf.cast(image, tf.float32) / 255., label

training_dataset, validation_dataset = tfds.load(
    "mnist",
    split=["train", "test"],
    shuffle_files=True,
    as_supervised=True,
    with_info=False,
)
training_dataset = training_dataset.map(normalize_img)
validation_dataset = validation_dataset.map(normalize_img)

----------------------------------------

TITLE: Resuming Training from Checkpoint in TensorFlow 1
DESCRIPTION: Rebuilds the TensorFlow 1 Estimator using the last saved checkpoint and continues training from where it left off.

LANGUAGE: python
CODE:
classifier = tf1.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[256, 32],
    optimizer=tf1.train.AdamOptimizer(0.001),
    n_classes=10,
    dropout=0.2,
    model_dir=path,
    config = config
)
classifier.train(input_fn=train_input_fn,
                   max_steps = 10)

----------------------------------------

TITLE: Training and Evaluating a Linear Classifier in TensorFlow
DESCRIPTION: This snippet demonstrates how to create, train, and evaluate a linear classifier using tf.estimator.LinearClassifier with specified feature columns.

LANGUAGE: Python
CODE:
e = tf.estimator.LinearClassifier(
    feature_columns=[
        native_country, education, occupation, workclass, marital_status,
        race, age_buckets, education_x_occupation,
        age_buckets_x_race_x_occupation],
    model_dir=YOUR_MODEL_DIRECTORY)
e.train(input_fn=input_fn_train, steps=200)
# Evaluate for one step (one pass through the test data).
results = e.evaluate(input_fn=input_fn_test)

# Print the stats for the evaluation.
for key in sorted(results):
    print("%s: %s" % (key, results[key]))

----------------------------------------

TITLE: Configuring Dataset Batching and Shuffling
DESCRIPTION: Sets up dataset batching and shuffling with specified batch size and shuffle buffer size.

LANGUAGE: python
CODE:
BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
test_dataset = test_dataset.batch(BATCH_SIZE)

----------------------------------------

TITLE: Importing Dependencies for Video Inbetweening in Python
DESCRIPTION: This code snippet imports the necessary libraries for video inbetweening, including TensorFlow, TensorFlow Hub, and visualization tools.

LANGUAGE: python
CODE:
import tensorflow as tf

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow_hub as hub
import tensorflow_datasets as tfds

from tensorflow_datasets.core import SplitGenerator
from tensorflow_datasets.video.bair_robot_pushing import BairRobotPushingSmall

import tempfile
import pathlib

TEST_DIR = pathlib.Path(tempfile.mkdtemp()) / "bair_robot_pushing_small/softmotion30_44k/test/"

----------------------------------------

TITLE: Custom Layer Implementation
DESCRIPTION: Shows how to create a custom layer by inheriting from tf.keras.layers.Layer with build and call methods

LANGUAGE: python
CODE:
class MySimpleLayer(tf.keras.layers.Layer):
  def __init__(self, output_units):
    super(MySimpleLayer, self).__init__()
    self.output_units = output_units

  def build(self, input_shape):
    self.kernel = self.add_variable(
      "kernel", [input_shape[-1], self.output_units])

  def call(self, input):
    return tf.matmul(input, self.kernel)

----------------------------------------

TITLE: BigGAN Model Loading and Input Setup
DESCRIPTION: Loads the BigGAN model from TensorFlow Hub and configures input placeholders

LANGUAGE: Python
CODE:
tf.reset_default_graph()
print('Loading BigGAN module from:', module_path)
module = hub.Module(module_path)
inputs = {k: tf.placeholder(v.dtype, v.get_shape().as_list(), k)
          for k, v in module.get_input_info_dict().items()}
output = module(inputs)

----------------------------------------

TITLE: Defining Titanic Dataset Input Function
DESCRIPTION: Creates an input function that reads the Titanic dataset CSV file and returns a tf.data.Dataset object with batched and preprocessed data

LANGUAGE: Python
CODE:
def train_input_fn():
  titanic_file = tf.keras.utils.get_file("train.csv", "https://storage.googleapis.com/tf-datasets/titanic/train.csv")
  titanic = tf.data.experimental.make_csv_dataset(
      titanic_file, batch_size=32,
      label_name="survived")
  titanic_batches = (
      titanic.cache().repeat().shuffle(500)
      .prefetch(tf.data.AUTOTUNE))
  return titanic_batches

----------------------------------------

TITLE: Loading and Using a TensorFlow Hub SavedModel in Python
DESCRIPTION: This snippet demonstrates how to load a SavedModel from TensorFlow Hub and use it to generate outputs. It showcases the basic usage of the Reusable SavedModel API.

LANGUAGE: python
CODE:
obj = hub.load("path/to/model")  # That's tf.saved_model.load() after download.
outputs = obj(inputs, training=False)  # Invokes the tf.function obj.__call__.

----------------------------------------

TITLE: Common TensorFlow Installation Error Messages
DESCRIPTION: Collection of error messages encountered during TensorFlow installation and import, including package compatibility issues, missing dependencies, and build failures.

LANGUAGE: text
CODE:
No matching distribution found for tensorflow

LANGUAGE: text
CODE:
ImportError: libcudart.so.Version: cannot open shared object file

LANGUAGE: text
CODE:
ImportError: cannot import name 'descriptor'

LANGUAGE: text
CODE:
SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed

LANGUAGE: text
CODE:
ModuleNotFoundError: No module named 'tensorflow.python._pywrap_tensorflow_internal'

----------------------------------------

TITLE: Creating TensorFlow Dataset Iterators
DESCRIPTION: Example demonstrating how to create and use different types of iterators (one-shot, initializable) for TensorFlow datasets

LANGUAGE: python
CODE:
dataset = tf.data.Dataset.range(100)
iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

for i in range(100):
  value = sess.run(next_element)
  assert i == value

----------------------------------------

TITLE: Listing Files from ZIP URL
DESCRIPTION: Function to list files contained within a remote ZIP file using the remotezip library

LANGUAGE: Python
CODE:
def list_files_from_zip_url(zip_url):
  files = []
  with rz.RemoteZip(zip_url) as zip:
    for zip_info in zip.infolist():
      files.append(zip_info.filename)
  return files

----------------------------------------

TITLE: Audio File Loading Utility Function
DESCRIPTION: Function to load WAV files and convert them to 16kHz mono audio format required by YAMNet

LANGUAGE: python
CODE:
@tf.function
def load_wav_16k_mono(filename):
    """ Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. """
    file_contents = tf.io.read_file(filename)
    wav, sample_rate = tf.audio.decode_wav(
          file_contents,
          desired_channels=1)
    wav = tf.squeeze(wav, axis=-1)
    sample_rate = tf.cast(sample_rate, dtype=tf.int64)
    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)
    return wav

----------------------------------------

TITLE: Defining Custom 2D Convolutional Layer
DESCRIPTION: Creates a custom 2D convolutional layer class that can be subclassed for implementing Entropy Penalized Reparameterization (EPR).

LANGUAGE: python
CODE:
class CustomConv2D(tf.keras.layers.Layer):

  def __init__(self, filters, kernel_size,
               strides=1, padding="SAME", name="conv2d"):
    super().__init__(name=name)
    self.filters = filters
    self.kernel_size = kernel_size
    self.strides = strides
    self.padding = padding

  @classmethod
  def copy(cls, other, **kwargs):
    """Returns an instantiated and built layer, initialized from `other`."""
    self = cls(filters=other.filters, kernel_size=other.kernel_size,
               strides=other.strides, padding=other.padding, name=other.name,
               **kwargs)
    self.build(None, other=other)
    return self

  def build(self, input_shape, other=None):
    """Instantiates weights, optionally initializing them from `other`."""
    if other is None:
      kernel_shape = 2 * (self.kernel_size,) + (input_shape[-1], self.filters)
      kernel = tf.keras.initializers.GlorotUniform()(shape=kernel_shape)
      bias = tf.keras.initializers.Zeros()(shape=(self.filters,))
    else:
      kernel, bias = other.kernel, other.bias
    self.kernel = tf.Variable(
        tf.cast(kernel, self.variable_dtype), name="kernel")
    self.bias = tf.Variable(
        tf.cast(bias, self.variable_dtype), name="bias")
    self.built = True

  def call(self, inputs):
    outputs = tf.nn.convolution(
        inputs, self.kernel, strides=self.strides, padding=self.padding)
    outputs = tf.nn.bias_add(outputs, self.bias)
    return tf.nn.leaky_relu(outputs)

----------------------------------------

TITLE: Verifying Variable Reuse in TF2 Inception ResNet V2 Model
DESCRIPTION: Test that the TF2 Inception ResNet V2 model reuses variables correctly after the first call.

LANGUAGE: Python
CODE:
model = InceptionResnetV2(1000)
height, width = 299, 299
num_classes = 1000

inputs = tf.ones( (1, height, width, 3))
# Create all weights on the first call
model(inputs)

# Verify that no new weights are created in followup calls
with assert_no_variable_creations():
  model(inputs)
with catch_and_raise_created_variables():
  model(inputs)

----------------------------------------

TITLE: Basic Gradient Tape Usage in TensorFlow
DESCRIPTION: Demonstrates basic gradient computation using tf.GradientTape to calculate derivatives of a sum operation with respect to input tensor.

LANGUAGE: Python
CODE:
x = tf.ones((2, 2))

with tf.GradientTape() as t:
  t.watch(x)
  y = tf.reduce_sum(x)
  z = tf.multiply(y, y)

# Derivative of z with respect to the original input tensor x
dz_dx = t.gradient(z, x)
for i in [0, 1]:
  for j in [0, 1]:
    assert dz_dx[i][j].numpy() == 8.0

----------------------------------------

TITLE: Importing TensorFlow
DESCRIPTION: Import TensorFlow and print version information for environment setup verification.

LANGUAGE: python
CODE:
import tensorflow as tf
print("TensorFlow version:", tf.__version__)

----------------------------------------

TITLE: Stacking Multiple LSTM Layers in TensorFlow
DESCRIPTION: Implementation of stacking multiple LSTM layers using tf.contrib.rnn.MultiRNNCell to create a more expressive model.

LANGUAGE: python
CODE:
def lstm_cell():
  return tf.contrib.rnn.BasicLSTMCell(lstm_size)
stacked_lstm = tf.contrib.rnn.MultiRNNCell(
    [lstm_cell() for _ in range(number_of_layers)])

initial_state = state = stacked_lstm.zero_state(batch_size, tf.float32)
for i in range(num_steps):
    # The value of state is updated after processing each batch of words.
    output, state = stacked_lstm(words[:, i], state)

    # The rest of the code.
    # ...

final_state = state

----------------------------------------

TITLE: Running Inference on a Single Image
DESCRIPTION: This code snippet demonstrates how to run MoveNet inference on a single input image. It loads an image, resizes it to the required input size, runs the model, and visualizes the detected keypoints on the image.

LANGUAGE: Python
CODE:
# Load the input image.
image_path = 'input_image.jpeg'
image = tf.io.read_file(image_path)
image = tf.image.decode_jpeg(image)

# Resize and pad the image to keep the aspect ratio and fit the expected size.
input_image = tf.expand_dims(image, axis=0)
input_image = tf.image.resize_with_pad(input_image, input_size, input_size)

# Run model inference.
keypoints_with_scores = movenet(input_image)

# Visualize the predictions with image.
display_image = tf.expand_dims(image, axis=0)
display_image = tf.cast(tf.image.resize_with_pad(
    display_image, 1280, 1280), dtype=tf.int32)
output_overlay = draw_prediction_on_image(
    np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)

plt.figure(figsize=(5, 5))
plt.imshow(output_overlay)
_ = plt.axis('off')

----------------------------------------

TITLE: Installing Dependencies for TensorFlow Model Maker
DESCRIPTION: Installs required libraries including TensorFlow Model Maker, TensorFlow Datasets, and other dependencies with specific version constraints.

LANGUAGE: Shell
CODE:
!sudo apt install -q libportaudio2
!pip install "numpy<=1.23.5"
!pip install --use-deprecated=legacy-resolver tflite-model-maker-nightly
!pip install -U tensorflow-datasets
!pip install "tensorflow<2.9.0"
!pip install "tensorflow-datasets~=4.8.0"
!pip install tensorflow-metadata~=1.10.0
!pip install "packaging<20.10"

----------------------------------------

TITLE: Importing Required Libraries for Image Super Resolution
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Hub, PIL, numpy, and matplotlib for image processing and visualization.

LANGUAGE: Python
CODE:
import os
import time
from PIL import Image
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
os.environ["TFHUB_DOWNLOAD_PROGRESS"] = "True"

----------------------------------------

TITLE: Initializing Basic TensorFlow Dependencies
DESCRIPTION: Setting up required imports and configuring numpy print options for TensorFlow CSV data processing

LANGUAGE: Python
CODE:
import pandas as pd
import numpy as np

# Make numpy values easier to read.
np.set_printoptions(precision=3, suppress=True)

import tensorflow as tf
from tensorflow.keras import layers

----------------------------------------

TITLE: Defining TF2 Inception ResNet V2 as Keras Layer
DESCRIPTION: Create a Keras layer class for the Inception ResNet V2 model, using the TF1 compatibility decorator.

LANGUAGE: Python
CODE:
class InceptionResnetV2(tf.keras.layers.Layer):
  """Slim InceptionResnetV2 forward pass as a Keras layer"""

  def __init__(self, num_classes, **kwargs):
    super().__init__(**kwargs)
    self.num_classes = num_classes

  @tf.compat.v1.keras.utils.track_tf1_style_variables
  def call(self, inputs, training=None):
    is_training = training or False 
    
    with slim.arg_scope(
        inception.inception_resnet_v2_arg_scope(batch_norm_scale=True)):
      return inception.inception_resnet_v2(
          inputs, self.num_classes, is_training=is_training)

----------------------------------------

TITLE: Converting Tensors to NumPy Arrays in TensorFlow
DESCRIPTION: Demonstrates how to convert TensorFlow tensors to NumPy arrays using np.array and tensor.numpy().

LANGUAGE: Python
CODE:
np.array(rank_2_tensor)

rank_2_tensor.numpy()

----------------------------------------

TITLE: Training Loop Implementation
DESCRIPTION: Implements a basic training loop using gradient descent to update model parameters

LANGUAGE: python
CODE:
def train(model, inputs, outputs, learning_rate):
  with tf.GradientTape() as t:
    current_loss = loss(model(inputs), outputs)
  dW, db = t.gradient(current_loss, [model.W, model.b])
  model.W.assign_sub(learning_rate * dW)
  model.b.assign_sub(learning_rate * db)

----------------------------------------

TITLE: Implementing TF2 Keras Custom Callback
DESCRIPTION: Creation of a custom Keras callback in TensorFlow 2.x that provides similar functionality to the SessionRunHook example.

LANGUAGE: python
CODE:
class CustomCallback(tf.keras.callbacks.Callback):

    def on_train_begin(self, logs = None):
      self._step = -1
      self._start_time = time.time()
      self.log_frequency = 10

    def on_train_batch_begin(self, batch, logs = None):
      self._step += 1

    def on_train_batch_end(self, batch, logs = None):
      if self._step % self.log_frequency == 0:
        current_time = time.time()
        duration = current_time - self._start_time
        self._start_time = current_time
        examples_per_sec = self.log_frequency / duration
        print('Time:', datetime.now(), ', Step #:', self._step,
              ', Examples per second:', examples_per_sec)

callback = CustomCallback()

dataset = tf.data.Dataset.from_tensor_slices(
    (features, labels)).batch(1).repeat(100)

model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])
optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)

model.compile(optimizer, "mse")

# Begin training.
result = model.fit(dataset, callbacks=[callback], verbose = 0)
# Provide the results of training metrics.
result.history

----------------------------------------

TITLE: Installing Dependencies with pip
DESCRIPTION: Installing seaborn package for data visualization

LANGUAGE: python
CODE:
!pip install seaborn

----------------------------------------

TITLE: Loading and Splitting IMDB Dataset
DESCRIPTION: Load the IMDB reviews dataset and split it into training (60%), validation (40%) and test sets using TensorFlow Datasets

LANGUAGE: python
CODE:
train_data, validation_data, test_data = tfds.load(
    name="imdb_reviews", 
    split=('train[:60%]', 'train[60%:]', 'test'),
    as_supervised=True)

----------------------------------------

TITLE: Encoding Categorical Features with Hash Bucket in TensorFlow
DESCRIPTION: This snippet shows how to create a feature column for a categorical feature with unknown vocabulary using tf.feature_column.categorical_column_with_hash_bucket().

LANGUAGE: Python
CODE:
education = tf.feature_column.categorical_column_with_hash_bucket(
    "education", hash_bucket_size=1000)

----------------------------------------

TITLE: Running tf_upgrade_v2 on a Directory Tree
DESCRIPTION: Shows how to use tf_upgrade_v2 to upgrade an entire directory of Python files, including generating a report file.

LANGUAGE: bash
CODE:
tf_upgrade_v2 \
    --intree models/samples/cookbook/regression/ \
    --outtree regression_v2/ \
    --reportfile tree_report.txt

----------------------------------------

TITLE: Creating a Simple Input Function in Python
DESCRIPTION: Shows a basic implementation of an input function that returns features and labels for the Iris dataset.

LANGUAGE: python
CODE:
def input_evaluation_set():
    features = {'SepalLength': np.array([6.4, 5.0]),
                'SepalWidth':  np.array([2.8, 2.3]),
                'PetalLength': np.array([5.6, 3.3]),
                'PetalWidth':  np.array([2.2, 1.0])}
    labels = np.array([2, 1])
    return features, labels

----------------------------------------

TITLE: Applying Image Augmentations
DESCRIPTION: Defines and applies various image augmentation techniques to the training dataset to improve model generalization.

LANGUAGE: Python
CODE:
def random_crop_and_random_augmentations_fn(image):
  image = image_preprocessing.preprocess_for_train(image)
  image = tf.image.random_brightness(image, 0.2)
  image = tf.image.random_contrast(image, 0.5, 2.0)
  image = tf.image.random_saturation(image, 0.75, 1.25)
  image = tf.image.random_hue(image, 0.1)
  return image

# ... (other augmentation functions)

ds_train_with_unknown = ds_train_with_unknown.map(train_augment_fn)
ds_validation = ds_validation.map(eval_augment_fn)
ds_test = ds_test.map(eval_augment_fn)
ds_unknown_test = ds_unknown_test.map(eval_augment_fn)

----------------------------------------

TITLE: Installing SentEval and Downloading Task Data
DESCRIPTION: This snippet clones the SentEval repository and downloads the necessary task data for evaluation.

LANGUAGE: bash
CODE:
rm -rf ./SentEval
git clone https://github.com/facebookresearch/SentEval.git
cd $PWD/SentEval/data/downstream && bash get_transfer_data.bash > /dev/null 2>&1

----------------------------------------

TITLE: Adding RNN Layers to TensorFlow Model
DESCRIPTION: Function to add bidirectional LSTM layers to the model and create a fixed-length embedding by summing up the LSTM outputs.

LANGUAGE: Python
CODE:
outputs, _, _ = contrib_rnn.stack_bidirectional_dynamic_rnn(
    cells_fw=[cell(params.num_nodes) for _ in range(params.num_layers)],
    cells_bw=[cell(params.num_nodes) for _ in range(params.num_layers)],
    inputs=convolved,
    sequence_length=lengths,
    dtype=tf.float32,
    scope="rnn_classification")

mask = tf.tile(
    tf.expand_dims(tf.sequence_mask(lengths, tf.shape(outputs)[1]), 2),
    [1, 1, tf.shape(outputs)[2]])
zero_outside = tf.where(mask, outputs, tf.zeros_like(outputs))
outputs = tf.reduce_sum(zero_outside, axis=1)

----------------------------------------

TITLE: Installing TensorFlow Text Dependencies
DESCRIPTION: Installs required TensorFlow Text package version

LANGUAGE: python
CODE:
!pip install --quiet "tensorflow-text==2.11.*"

----------------------------------------

TITLE: Loading and Parsing Iris Dataset using TensorFlow and Pandas in Python
DESCRIPTION: This code downloads the Iris dataset, loads it into Pandas DataFrames, and separates features from labels for both training and test sets.

LANGUAGE: Python
CODE:
train_path = tf.keras.utils.get_file(
    "iris_training.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv")
test_path = tf.keras.utils.get_file(
    "iris_test.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv")

train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)

train_y = train.pop('Species')
test_y = test.pop('Species')

----------------------------------------

TITLE: Converting JAX Model to TensorFlow SavedModel
DESCRIPTION: Creates a TensorFlow Module wrapper around the JAX model, converts its methods using jax2tf, and saves it as a TensorFlow SavedModel for deployment or further training.

LANGUAGE: Python
CODE:
class TFModel(tf.Module):
  def __init__(self, state, model):
    super().__init__()

    @jax.jit
    def loss_with_train_bool(state, rng, data, labels, train):
      other_state, params = state.pop('params')
      loss, batch_stats = jax.lax.cond(train,
                                       lambda state, data, labels: model.loss(params, other_state, rng, data, labels, train=True),
                                       lambda state, data, labels: model.loss(params, other_state, rng, data, labels, train=False),
                                       state, data, labels)
      new_rng, _ = jax.random.split(rng)
      return loss, batch_stats, new_rng

    self.state_vars = tf.nest.map_structure(tf.Variable, state)
    self.vars = tf.nest.flatten(self.state_vars)
    self.jax_rng = tf.Variable(jax.random.PRNGKey(0))

    self.loss_fn = jax2tf.convert(loss_with_train_bool, polymorphic_shapes=["...", "...", "(b, 28, 28, 1)", "(b, 10)", "..."])
    self.accuracy_fn = jax2tf.convert(model.accuracy, polymorphic_shapes=["...", "(b, 28, 28, 1)", "(b, 10)"])
    self.predict_fn = jax2tf.convert(model.predict, polymorphic_shapes=["...", "(b, 28, 28, 1)"])

  @tf.function(autograph=False, input_signature=[tf.TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32)])
  def predict(self, data):
    return self.predict_fn(self.state_vars, data)

  @tf.function(input_signature=[tf.TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32),
                                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],
               autograph=False)
  def train_loss(self, data, labels):
      loss, batch_stats, new_rng = self.loss_fn(self.state_vars, self.jax_rng, data, labels, True)
      flat_vars = tf.nest.flatten(self.state_vars['batch_stats'])
      flat_values = tf.nest.flatten(batch_stats['batch_stats'])
      for var, val in zip(flat_vars, flat_values):
        var.assign(val)
      self.jax_rng.assign(new_rng)
      return loss

  @tf.function(input_signature=[tf.TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32),
                                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],
               autograph=False)
  def eval_loss(self, data, labels):
      loss, batch_stats, new_rng = self.loss_fn(self.state_vars, self.jax_rng, data, labels, False)
      return loss

  @tf.function(input_signature=[tf.TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32),
                                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],
               autograph=False)
  def accuracy(self, data, labels):
    return self.accuracy_fn(self.state_vars, data, labels)

----------------------------------------

TITLE: Importing TensorFlow and Time Libraries
DESCRIPTION: Imports the TensorFlow library and the time module for performance measurements.

LANGUAGE: Python
CODE:
import tensorflow as tf

import time

----------------------------------------

TITLE: Visualizing CIFAR10 Dataset Sample
DESCRIPTION: This snippet plots the first 25 images from the training set and displays their class names.

LANGUAGE: Python
CODE:
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i])
    # The CIFAR labels happen to be arrays, 
    # which is why you need the extra index
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()

----------------------------------------

TITLE: Visualizing CIFAR10 Dataset Sample
DESCRIPTION: This snippet plots the first 25 images from the training set and displays their class names.

LANGUAGE: Python
CODE:
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i])
    # The CIFAR labels happen to be arrays, 
    # which is why you need the extra index
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()

----------------------------------------

TITLE: Training Results for K80 with Real Data
DESCRIPTION: Raw performance numbers for training with real ImageNet data on NVIDIA Tesla K80 GPUs, showing images processed per second for different models and GPU counts.

LANGUAGE: markdown
CODE:
GPUs | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16
---- | ----------- | --------- | ---------- | ------- | -----
1    | 30.6        | 51.2      | 20.0       | 639     | 34.2
2    | 58.4        | 98.8      | 38.3       | 1136    | 62.9
4    | 115         | 194       | 75.4       | 2067    | 118
8    | 225         | 381       | 148        | 4056    | 230

----------------------------------------

TITLE: Loading and Using a TensorFlow Hub Model with KerasLayer API
DESCRIPTION: This snippet demonstrates how to import tensorflow_hub, load a pre-trained model using hub.KerasLayer, and apply it to input text data. It shows the basic workflow for utilizing TensorFlow Hub models in a Python program.

LANGUAGE: python
CODE:
import tensorflow_hub as hub

embed = hub.KerasLayer("https://tfhub.dev/google/nnlm-en-dim128/2")
embeddings = embed(["A long sentence.", "single-word", "http://example.com"])
print(embeddings.shape, embeddings.dtype)

----------------------------------------

TITLE: Basic ND Array Operations
DESCRIPTION: Demonstrates creation and manipulation of TensorFlow NumPy ND arrays

LANGUAGE: Python
CODE:
# Create an ND array and check out different attributes.
ones = tnp.ones([5, 3], dtype=tnp.float32)
print("Created ND array with shape = %s, rank = %s, " 
      "dtype = %s on device = %s\n" % (
          ones.shape, ones.ndim, ones.dtype, ones.device))

# `ndarray` is just an alias to `tf.Tensor`.
print("Is `ones` an instance of tf.Tensor: %s\n" % isinstance(ones, tf.Tensor))

# Try commonly used member functions.
print("ndarray.T has shape %s" % str(ones.T.shape))
print("narray.reshape(-1) has shape %s" % ones.reshape(-1).shape)

----------------------------------------

TITLE: Evaluating a Trained TensorFlow Estimator Model
DESCRIPTION: Demonstrates how to evaluate a trained Estimator model on test data and print the accuracy.

LANGUAGE: python
CODE:
# Evaluate the model.
eval_result = classifier.evaluate(
    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y, args.batch_size))

print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))

----------------------------------------

TITLE: Configuring Maven Dependency for TensorFlow
DESCRIPTION: Basic Maven dependency configuration for adding TensorFlow to a Java project using version 2.4.0.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.tensorflow</groupId>
  <artifactId>tensorflow</artifactId>
  <version>2.4.0</version>
</dependency>

----------------------------------------

TITLE: Loading and Preprocessing Images in TensorFlow
DESCRIPTION: Functions to load and preprocess images to the correct format for the model. Handles image loading, decoding, resizing and normalization.

LANGUAGE: Python
CODE:
def read_image(file_name):
  image = tf.io.read_file(file_name)
  image = tf.io.decode_jpeg(image, channels=3)
  image = tf.image.convert_image_dtype(image, tf.float32)
  image = tf.image.resize_with_pad(image, target_height=224, target_width=224)
  return image

----------------------------------------

TITLE: Setting up TensorFlow Imports and Sample Data
DESCRIPTION: Initial setup code importing required TensorFlow libraries and creating sample feature and label data for demonstration.

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow.compat.v1 as tf1

import time
from datetime import datetime
from absl import flags

features = [[1., 1.5], [2., 2.5], [3., 3.5]]
labels = [[0.3], [0.5], [0.7]]
eval_features = [[4., 4.5], [5., 5.5], [6., 6.5]]
eval_labels = [[0.8], [0.9], [1.]]

----------------------------------------

TITLE: Downloading Shakespeare Dataset
DESCRIPTION: Download the Shakespeare dataset using TensorFlow's utility function.

LANGUAGE: Python
CODE:
path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')

----------------------------------------

TITLE: Accessing Tensor Slices in TensorFlow
DESCRIPTION: Demonstrates how to access specific elements or slices of tensors using indexing and slicing syntax.

LANGUAGE: python
CODE:
my_scalar = my_vector[2]

my_scalar = my_matrix[1, 2]

my_row_vector = my_matrix[2]
my_column_vector = my_matrix[:, 3]

----------------------------------------

TITLE: Importing TensorFlow and Required Libraries
DESCRIPTION: This snippet imports TensorFlow, Keras, NumPy, and Matplotlib for building and visualizing the neural network model.

LANGUAGE: Python
CODE:
# TensorFlow and tf.keras
import tensorflow.compat.v1 as tf

from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

----------------------------------------

TITLE: Using MirroredStrategy with Estimator in TensorFlow
DESCRIPTION: Demonstrates how to use MirroredStrategy with a TensorFlow Estimator. The strategy is passed to the RunConfig for both training and evaluation.

LANGUAGE: Python
CODE:
mirrored_strategy = tf.distribute.MirroredStrategy()
config = tf.estimator.RunConfig(
    train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy)
regressor = tf.estimator.LinearRegressor(
    feature_columns=[tf.feature_column.numeric_column('feats')],
    optimizer='SGD',
    config=config)

----------------------------------------

TITLE: Saving Model with tf.saved_model API
DESCRIPTION: Save the model using the lower-level tf.saved_model API.

LANGUAGE: Python
CODE:
model = get_model()  # get a fresh model
saved_model_path = '/tmp/tf_save'
tf.saved_model.save(model, saved_model_path)

----------------------------------------

TITLE: Preparing Sample Dataset for Demonstration
DESCRIPTION: Creates a simple dataset with features, embedding indices/values, and labels for both training and evaluation.

LANGUAGE: python
CODE:
features = [[1., 1.5]]
embedding_features_indices = [[0, 0], [0, 1]]
embedding_features_values = [0, 5]
labels = [[0.3]]
eval_features = [[4., 4.5]]
eval_embedding_features_indices = [[0, 0], [0, 1]]
eval_embedding_features_values = [4, 3]
eval_labels = [[0.8]]

----------------------------------------

TITLE: Running PDE Simulation and Visualizing Results
DESCRIPTION: This code runs the PDE simulation for 1000 steps using the defined update rules. It initializes the variables, runs the simulation loop with specified parameters for time resolution and damping, and finally displays the resulting state of the pond surface.

LANGUAGE: Python
CODE:
# Initialize state to initial conditions
tf.global_variables_initializer().run()

# Run 1000 steps of PDE
for i in range(1000):
  # Step simulation
  step.run({eps: 0.03, damping: 0.04})

# Show final image
DisplayArray(U.eval(), rng=[-0.1, 0.1])

----------------------------------------

TITLE: Converting Keras Model to TensorFlow Estimator
DESCRIPTION: Demonstrates how to convert a Keras model to a TensorFlow Estimator for distributed training.

LANGUAGE: Python
CODE:
model = tf.keras.Sequential([layers.Dense(64, activation='relu', input_shape=(32,)),
                          layers.Dense(10,activation='softmax')])

model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

estimator = tf.keras.estimator.model_to_estimator(model)

----------------------------------------

TITLE: Importing Required Libraries for Transfer Learning in TensorFlow
DESCRIPTION: Imports necessary libraries including TensorFlow, Keras, NumPy, and Matplotlib for implementing transfer learning.

LANGUAGE: Python
CODE:
import os

import tensorflow.compat.v1 as tf

from tensorflow import keras
print("TensorFlow version is ", tf.__version__)

import numpy as np

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

----------------------------------------

TITLE: TensorFlow Lite Version String Declaration
DESCRIPTION: Location of TensorFlow Lite version string definition

LANGUAGE: c
CODE:
TFLITE_VERSION_STRING in tensorflow/lite/version.h

----------------------------------------

TITLE: Starting Jupyter Notebook with TensorFlow
DESCRIPTION: Command to start a Jupyter Notebook server using TensorFlow's nightly build in a Docker container. This exposes the Jupyter server on port 8888.

LANGUAGE: bash
CODE:
docker run -it -p 8888:8888 tensorflow/tensorflow:nightly-jupyter

----------------------------------------

TITLE: Compiling a TensorFlow Model for Training
DESCRIPTION: This code compiles a TensorFlow model for training by specifying the optimizer and loss function. It uses the Adam optimizer and categorical crossentropy loss, which are common choices for classification tasks.

LANGUAGE: python
CODE:
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss=tf.keras.losses.categorical_crossentropy)

----------------------------------------

TITLE: Image Generation and Utility Functions
DESCRIPTION: Defines helper functions for generating truncated noise samples, one-hot encoding, image sampling and display

LANGUAGE: Python
CODE:
def truncated_z_sample(batch_size, truncation=1., seed=None):
  state = None if seed is None else np.random.RandomState(seed)
  values = truncnorm.rvs(-2, 2, size=(batch_size, dim_z), random_state=state)
  return truncation * values

----------------------------------------

TITLE: Installing Built TensorFlow Package
DESCRIPTION: Installs the locally built TensorFlow pip package.

LANGUAGE: Batch
CODE:
pip install bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-<version>-<tags>.whl

----------------------------------------

TITLE: Cloning a forked repository in Bash
DESCRIPTION: Command to clone a forked repository to the local system. This is part of the contribution workflow for TensorFlow projects.

LANGUAGE: Bash
CODE:
$ git clone git@github.com:your-user-name/project-name.git

----------------------------------------

TITLE: Initializing Basic Dependencies and Constants
DESCRIPTION: Sets up the required imports and defines buffer/batch size constants for data processing.

LANGUAGE: Python
CODE:
import numpy as np
import pandas as pd
import tensorflow as tf

SHUFFLE_BUFFER = 500
BATCH_SIZE = 2

----------------------------------------

TITLE: Using Image Classification Model with Keras
DESCRIPTION: Demonstrates how to use the image classification model within Keras using KerasLayer wrapper.

LANGUAGE: python
CODE:
logits = hub.KerasLayer("path/to/model")(images)

----------------------------------------

TITLE: Adding CUPTI to Library Path
DESCRIPTION: Add CUPTI installation directory to the LD_LIBRARY_PATH environment variable.

LANGUAGE: Shell
CODE:
export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH

----------------------------------------

TITLE: Implementing a ResNet Identity Block in TensorFlow
DESCRIPTION: This snippet demonstrates how to implement a ResNet Identity Block by extending tf.keras.Model.

LANGUAGE: Python
CODE:
class ResnetIdentityBlock(tf.keras.Model):
  def __init__(self, kernel_size, filters):
    super(ResnetIdentityBlock, self).__init__(name='')
    filters1, filters2, filters3 = filters

    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))
    self.bn2a = tf.keras.layers.BatchNormalization()

    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')
    self.bn2b = tf.keras.layers.BatchNormalization()

    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))
    self.bn2c = tf.keras.layers.BatchNormalization()

  def call(self, input_tensor, training=False):
    x = self.conv2a(input_tensor)
    x = self.bn2a(x, training=training)
    x = tf.nn.relu(x)

    x = self.conv2b(x)
    x = self.bn2b(x, training=training)
    x = tf.nn.relu(x)

    x = self.conv2c(x)
    x = self.bn2c(x, training=training)

    x += input_tensor
    return tf.nn.relu(x)

block = ResnetIdentityBlock(1, [1, 2, 3])
_ = block(tf.zeros([1, 2, 3, 3]))

block.layers
len(block.variables)
block.summary()

----------------------------------------

TITLE: Initializing TensorFlow Session
DESCRIPTION: Creates a TensorFlow session and initializes variables for running BigBiGAN operations.

LANGUAGE: Python
CODE:
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

----------------------------------------

TITLE: Creating and Compiling a Simple Transfer Learning Model
DESCRIPTION: Sets up a transfer learning model using MobileNetV2 as a base, and compiles it for training.

LANGUAGE: Python
CODE:
mobile_net = tf.keras.applications.MobileNetV2(input_shape=(192, 192, 3), include_top=False)
mobile_net.trainable=False

model = tf.keras.Sequential([
  mobile_net,
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(len(label_names), activation = 'softmax')])

model.compile(optimizer=tf.train.AdamOptimizer(),
              loss=tf.keras.losses.sparse_categorical_crossentropy,
              metrics=["accuracy"])

----------------------------------------

TITLE: Defining a Shape Function for a TensorFlow Op in C++
DESCRIPTION: Shows how to define a shape function for the 'ZeroOut' op to provide shape inference capabilities.

LANGUAGE: C++
CODE:
REGISTER_OP("ZeroOut")
    .Input("to_zero: int32")
    .Output("zeroed: int32")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      ::tensorflow::shape_inference::ShapeHandle input;
      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &input));
      c->set_output(0, input);
      return Status::OK();
    });

----------------------------------------

TITLE: Importing TensorFlow and Matplotlib for Adversarial Example Generation
DESCRIPTION: Import necessary libraries including TensorFlow for deep learning and Matplotlib for visualization. Sets up figure size and grid display options for plots.

LANGUAGE: Python
CODE:
import tensorflow as tf
import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams['figure.figsize'] = (8, 8)
mpl.rcParams['axes.grid'] = False

----------------------------------------

TITLE: Caching Dataset Elements
DESCRIPTION: Shows how to use the cache transformation to store dataset elements in memory or on disk to avoid redundant computations.

LANGUAGE: Python
CODE:
benchmark(
    ArtificialDataset()
    .map(  # Apply time consuming operations before cache
        mapped_function
    ).cache(
    ),
    5
)

----------------------------------------

TITLE: Building TensorFlow CPU Package
DESCRIPTION: Bazel command to build TensorFlow pip package with CPU support

LANGUAGE: bash
CODE:
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu

----------------------------------------

TITLE: Creating Basic TensorFlow Variable
DESCRIPTION: Demonstrates the basic creation of a TensorFlow variable using tf.get_variable with a name and shape specification.

LANGUAGE: python
CODE:
my_variable = tf.get_variable("my_variable", [1, 2, 3])

----------------------------------------

TITLE: TensorFlow 1 Estimator Implementation
DESCRIPTION: Implementation of model training and evaluation using tf.estimator.Estimator, including input functions and model definition.

LANGUAGE: python
CODE:
def _input_fn():
  return tf1.data.Dataset.from_tensor_slices((features, labels)).batch(1)

def _eval_input_fn():
  return tf1.data.Dataset.from_tensor_slices(
      (eval_features, eval_labels)).batch(1)

def _model_fn(features, labels, mode):
  logits = tf1.layers.Dense(1)(features)
  loss = tf1.losses.mean_squared_error(labels=labels, predictions=logits)
  optimizer = tf1.train.AdagradOptimizer(0.05)
  train_op = optimizer.minimize(loss, global_step=tf1.train.get_global_step())
  return tf1.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

estimator = tf1.estimator.Estimator(model_fn=_model_fn)
estimator.train(_input_fn)
estimator.evaluate(_eval_input_fn)

----------------------------------------

TITLE: Importing Dependencies and Defining Utility Functions for S3GAN
DESCRIPTION: This snippet imports necessary libraries and defines utility functions for image handling, displaying, and interacting with the S3GAN models. It sets up the TensorFlow environment and defines Generator and Discriminator classes.

LANGUAGE: Python
CODE:
import os

import IPython
from IPython.display import display
import numpy as np
import PIL.Image
import pandas as pd
import six

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

import tensorflow_hub as hub

def imgrid(imarray, cols=8, pad=1):
    # Function implementation...

def imshow(a, format='png', jpeg_fallback=True):
    # Function implementation...

class Generator(object):
    # Class implementation...

class Discriminator(object):
    # Class implementation...

----------------------------------------

TITLE: Installing Python Dependencies on macOS
DESCRIPTION: Commands to install Python using Homebrew package manager on macOS

LANGUAGE: bash
CODE:
brew install python

----------------------------------------

TITLE: Defining Sample Sentences for BERT Processing
DESCRIPTION: Creates a list of sample sentences from Wikipedia to be processed by the BERT model for demonstration purposes.

LANGUAGE: python
CODE:
sentences = [
  "Here We Go Then, You And I is a 1999 album by Norwegian pop artist Morten Abel. It was Abel's second CD as a solo artist.",
  "The album went straight to number one on the Norwegian album chart, and sold to double platinum.",
  "Among the singles released from the album were the songs \"Be My Lover\" and \"Hard To Stay Awake\".",
  "Riccardo Zegna is an Italian jazz musician.",
  "Rajko Maksimovi is a composer, writer, and music pedagogue.",
  "One of the most significant Serbian composers of our time, Maksimovi has been and remains active in creating works for different ensembles.",
  "Ceylon spinach is a common name for several plants and may refer to: Basella alba Talinum fruticosum",
  "A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby totally or partly obscuring the image of the Sun for a viewer on Earth.",
  "A partial solar eclipse occurs in the polar regions of the Earth when the center of the Moon's shadow misses the Earth."
]

----------------------------------------

TITLE: Instantiating the Model
DESCRIPTION: Creates an instance of the Net model defined above.

LANGUAGE: Python
CODE:
net = Net()

----------------------------------------

TITLE: Setting up TensorFlow and TensorFlow Hub imports
DESCRIPTION: Imports the required libraries including TensorFlow, TensorFlow Hub, and other utilities for data processing and visualization.

LANGUAGE: Python
CODE:
import numpy as np
import time

import PIL.Image as Image
import matplotlib.pylab as plt

import tensorflow as tf
import tensorflow_hub as hub

import datetime

%load_ext tensorboard

----------------------------------------

TITLE: Setting up TensorFlow and TensorFlow Hub imports
DESCRIPTION: Imports the required libraries including TensorFlow, TensorFlow Hub, and other utilities for data processing and visualization.

LANGUAGE: Python
CODE:
import numpy as np
import time

import PIL.Image as Image
import matplotlib.pylab as plt

import tensorflow as tf
import tensorflow_hub as hub

import datetime

%load_ext tensorboard

----------------------------------------

TITLE: Training Model with Keras fit()
DESCRIPTION: Train the model using the Keras fit() method with the prepared dataset.

LANGUAGE: Python
CODE:
model = get_model()
train_dataset, eval_dataset = get_data()
model.fit(train_dataset, epochs=2)

----------------------------------------

TITLE: Training and Checkpointing Function
DESCRIPTION: Defines a function that trains the model and saves checkpoints periodically.

LANGUAGE: Python
CODE:
def train_and_checkpoint(net, manager):
  ckpt.restore(manager.latest_checkpoint)
  if manager.latest_checkpoint:
    print("Restored from {}".format(manager.latest_checkpoint))
  else:
    print("Initializing from scratch.")

  for _ in range(50):
    example = next(iterator)
    loss = train_step(net, example, opt)
    ckpt.step.assign_add(1)
    if int(ckpt.step) % 10 == 0:
      save_path = manager.save()
      print("Saved checkpoint for step {}: {}".format(int(ckpt.step), save_path))
      print("loss {:1.2f}".format(loss.numpy()))

----------------------------------------

TITLE: Simple Neural Network Model Implementation
DESCRIPTION: Implements a basic neural network model with dense and linear layers using TensorFlow NumPy operations

LANGUAGE: Python
CODE:
class Model(object):
  """Model with a dense and a linear layer."""

  def __init__(self):
    self.weights = None

  def predict(self, inputs):
    if self.weights is None:
      size = inputs.shape[1]
      # Note that type `tnp.float32` is used for performance.
      stddev = tnp.sqrt(size).astype(tnp.float32)
      w1 = tnp.random.randn(size, 64).astype(tnp.float32) / stddev
      bias = tnp.random.randn(64).astype(tnp.float32)
      w2 = tnp.random.randn(64, 2).astype(tnp.float32) / 8
      self.weights = (w1, bias, w2)
    else:
      w1, bias, w2 = self.weights
    y = tnp.matmul(inputs, w1) + bias
    y = tnp.maximum(y, 0)  # Relu
    return tnp.matmul(y, w2)  # Linear projection

----------------------------------------

TITLE: Importing TensorFlow Libraries
DESCRIPTION: Basic TensorFlow imports including compatibility mode for TF1

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow.compat.v1 as tf1

----------------------------------------

TITLE: Loading Class Names for YAMNet Sound Classification
DESCRIPTION: This function loads the class names from a CSV file provided with the YAMNet model, which are used to label the classification results.

LANGUAGE: python
CODE:
def class_names_from_csv(class_map_csv_text):
  """Returns list of class names corresponding to score vector."""
  class_names = []
  with tf.io.gfile.GFile(class_map_csv_text) as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
      class_names.append(row['display_name'])

  return class_names

class_map_path = model.class_map_path().numpy()
class_names = class_names_from_csv(class_map_path)

----------------------------------------

TITLE: Committing changes in Git
DESCRIPTION: Commands to stage all changes and commit them with a message in Git. This is part of the process for contributing changes to TensorFlow projects.

LANGUAGE: Bash
CODE:
$ git add -A
$ git commit -m "commit message here"

----------------------------------------

TITLE: TensorFlow Dataset Creation and Transformation
DESCRIPTION: Shows how to create and transform TensorFlow datasets using both tensor slices and file inputs.

LANGUAGE: python
CODE:
ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])

# Create a CSV file
import tempfile
_, filename = tempfile.mkstemp()

with open(filename, 'w') as f:
  f.write("""Line 1
Line 2
Line 3
  """)

ds_file = tf.data.TextLineDataset(filename)

ds_tensors = ds_tensors.map(tf.square).shuffle(2).batch(2)
ds_file = ds_file.batch(2)

print('Elements of ds_tensors:')
for x in ds_tensors:
  print(x)

print('\nElements in ds_file:')
for x in ds_file:
  print(x)

----------------------------------------

TITLE: Running TensorFlow Script from Host in Docker
DESCRIPTION: Command to run a TensorFlow script located on the host machine within a Docker container. This mounts the current directory and sets the working directory in the container.

LANGUAGE: bash
CODE:
docker run -it --rm -v $PWD:/tmp -w /tmp tensorflow/tensorflow python ./script.py

----------------------------------------

TITLE: Compiling TensorFlow C Example with Explicit Library Paths
DESCRIPTION: This bash script compiles the example C program with explicit include and library paths for the TensorFlow C library, then runs the resulting executable.

LANGUAGE: bash
CODE:
gcc -I/usr/local/include -L/usr/local/lib hello_tf.c -ltensorflow -o hello_tf

./hello_tf

----------------------------------------

TITLE: Deprecating TensorFlow Operation Example
DESCRIPTION: Example showing how to deprecate a TensorFlow operation using the REGISTER_OP macro with a deprecation notice.

LANGUAGE: cpp
CODE:
REGISTER_OP(...).Deprecated(deprecated_at_version, message)

----------------------------------------

TITLE: Downloading and Preparing Flower Dataset
DESCRIPTION: Downloads the flower dataset and sets up the data directory structure for processing

LANGUAGE: Python
CODE:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
archive = tf.keras.utils.get_file(origin=dataset_url, extract=True)
data_dir = pathlib.Path(archive).with_suffix('')

----------------------------------------

TITLE: Creating Variable with Constant Initializer
DESCRIPTION: Demonstrates initializing a TensorFlow variable using a constant tensor value.

LANGUAGE: python
CODE:
other_variable = tf.get_variable("other_variable", dtype=tf.int32,
  initializer=tf.constant([23, 42]))

----------------------------------------

TITLE: Applying a TF1 Hub Module in Python
DESCRIPTION: Demonstrates how to apply a TF1 Hub module to input tensors, including the use of named signatures and multiple inputs/outputs.

LANGUAGE: Python
CODE:
y = m(x)

LANGUAGE: Python
CODE:
outputs = m(dict(apples=x1, oranges=x2), signature="fruit_to_pet", as_dict=True)
y1 = outputs["cats"]
y2 = outputs["dogs"]

----------------------------------------

TITLE: Installing Dependencies
DESCRIPTION: Installing required packages ffmpeg and mediapy for video processing

LANGUAGE: bash
CODE:
!sudo apt install -y ffmpeg
!pip install -q mediapy

----------------------------------------

TITLE: Defining a Simple Linear Model
DESCRIPTION: Creates a simple linear model class using tf.keras.Model as the base class. The model has a single dense layer with 5 units.

LANGUAGE: Python
CODE:
class Net(tf.keras.Model):
  """A simple linear model."""

  def __init__(self):
    super(Net, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)

  def call(self, x):
    return self.l1(x)

----------------------------------------

TITLE: Transformer Encoder Text Processing
DESCRIPTION: Shows how to process multiple text segments using a Transformer encoder with tokenization and input packing.

LANGUAGE: python
CODE:
preprocessor = hub.load("path/to/preprocessor")

# Tokenize batches of both text inputs.
text_premises = tf.constant(["The quick brown fox jumped over the lazy dog.",
                             "Good day."])
tokenized_premises = preprocessor.tokenize(text_premises)
text_hypotheses = tf.constant(["The dog was lazy.",  # Implied.
                               "Axe handle!"])       # Not implied.
tokenized_hypotheses = preprocessor.tokenize(text_hypotheses)

# Pack input sequences for the Transformer encoder.
seq_length = 128
encoder_inputs = preprocessor.bert_pack_inputs(
    [tokenized_premises, tokenized_hypotheses],
    seq_length=seq_length)  # Optional argument.

----------------------------------------

TITLE: Importing Required Libraries
DESCRIPTION: Imports necessary libraries including matplotlib, TensorFlow, TensorFlow Compression, and TensorFlow Datasets.

LANGUAGE: python
CODE:
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_compression as tfc
import tensorflow_datasets as tfds

----------------------------------------

TITLE: Handling Unknown TensorShape in TensorFlow 2
DESCRIPTION: Shows how to handle a TensorShape with unknown rank in TensorFlow 2, using boolean checks to determine if the shape is known.

LANGUAGE: Python
CODE:
shape = tf.TensorShape(None)

if shape:
  dim = shape.dims[i]
  dim.is_compatible_with(other_dim) # or any other dimension method

----------------------------------------

TITLE: Downloading and Preparing Flower Dataset
DESCRIPTION: Downloads the flower dataset and sets up the data directory structure for processing

LANGUAGE: Python
CODE:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
archive = tf.keras.utils.get_file(origin=dataset_url, extract=True)
data_dir = pathlib.Path(archive).with_suffix('')

----------------------------------------

TITLE: Installing Required Python Packages
DESCRIPTION: Installs necessary Python packages including imageio, opencv-python, and TensorFlow Docs.

LANGUAGE: Python
CODE:
!pip install -q imageio
!pip install -q opencv-python
!pip install -q git+https://github.com/tensorflow/docs

----------------------------------------

TITLE: Installing Required Packages for Wav2Vec2 Fine-tuning
DESCRIPTION: Installs the gsoc-wav2vec2 package and its dependencies, including SoundFile for audio processing.

LANGUAGE: bash
CODE:
!pip3 install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main
!sudo apt-get install -y libsndfile1-dev
!pip3 install -q SoundFile

----------------------------------------

TITLE: Installing TensorFlow Docs Notebook Tools
DESCRIPTION: Command to install the TensorFlow docs package with notebook formatting tools.

LANGUAGE: bash
CODE:
python3 -m pip install -U [--user] git+https://github.com/tensorflow/docs

----------------------------------------

TITLE: Saving Complete Model in HDF5 Format
DESCRIPTION: Saving and loading the entire model including weights, architecture and optimizer state in HDF5 format

LANGUAGE: Python
CODE:
model = create_model()
model.fit(train_images, train_labels, epochs=5)
model.save('my_model.h5')

# Reload model
new_model = keras.models.load_model('my_model.h5')

----------------------------------------

TITLE: Defining RNN Model Architecture in TensorFlow
DESCRIPTION: Code snippet showing the high-level structure of the RNN model, including input processing, convolutional layers, LSTM layers, and classification.

LANGUAGE: Python
CODE:
inks, lengths, targets = _get_input_tensors(features, targets)
convolved = _add_conv_layers(inks)
final_state = _add_rnn_layers(convolved, lengths)
logits =_add_fc_layers(final_state)

----------------------------------------

TITLE: Defining a Training Step
DESCRIPTION: Defines a function to perform a single training step on the model.

LANGUAGE: Python
CODE:
def train_step(net, example, optimizer):
  """Trains `net` on `example` using `optimizer`."""
  with tf.GradientTape() as tape:
    output = net(example['x'])
    loss = tf.reduce_mean(tf.abs(output - example['y']))
  variables = net.trainable_variables
  gradients = tape.gradient(loss, variables)
  optimizer.apply_gradients(zip(gradients, variables))
  return loss

----------------------------------------

TITLE: Importing Required Libraries for BERT Processing
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Hub, and TensorFlow Text for BERT model processing and visualization.

LANGUAGE: python
CODE:
import seaborn as sns
from sklearn.metrics import pairwise

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text  # Imports TF ops for preprocessing.

----------------------------------------

TITLE: Multi-dimensional Tensor Slicing
DESCRIPTION: Shows slicing operations on 2D and 3D tensors using both tf.slice and Python slice syntax

LANGUAGE: Python
CODE:
t2 = tf.constant([[0, 1, 2, 3, 4],
                  [5, 6, 7, 8, 9],
                  [10, 11, 12, 13, 14],
                  [15, 16, 17, 18, 19]])

print(t2[:-1, 1:3])

----------------------------------------

TITLE: Loading TF2 SavedModel in TF1.15/TF1 Compatibility Mode using hub.KerasLayer
DESCRIPTION: Illustrates loading a TF2 SavedModel in TensorFlow 1.15 or TF1 compatibility mode using the hub.KerasLayer API.

LANGUAGE: python
CODE:
m = hub.KerasLayer(handle)
outputs = m(inputs)

----------------------------------------

TITLE: Setup and Import Dependencies for TensorFlow Migration
DESCRIPTION: Imports required TensorFlow libraries and prepares MNIST dataset for demonstration.

LANGUAGE: python
CODE:
import tensorflow.compat.v1 as tf1
import tensorflow as tf
import numpy as np
import tempfile

mnist = tf.keras.datasets.mnist
(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

----------------------------------------

TITLE: Building ANN Index with ANNOY
DESCRIPTION: Implements a function to build an Approximate Nearest Neighbors (ANN) index using the ANNOY library, saving the index and mapping to disk.

LANGUAGE: python
CODE:
def build_index(embedding_files_pattern, index_filename, vector_length, 
    metric='angular', num_trees=100):
  annoy_index = annoy.AnnoyIndex(vector_length, metric=metric)
  mapping = {}

  embed_files = tf.io.gfile.glob(embedding_files_pattern)
  num_files = len(embed_files)
  print('Found {} embedding file(s).'.format(num_files))

  item_counter = 0
  for i, embed_file in enumerate(embed_files):
    print('Loading embeddings in file {} of {}...'.format(i+1, num_files))
    dataset = tf.data.TFRecordDataset(embed_file)
    for record in dataset.map(_parse_example):
      text = record['text'].numpy().decode("utf-8")
      embedding = record['embedding'].numpy()
      mapping[item_counter] = text
      annoy_index.add_item(item_counter, embedding)
      item_counter += 1
      if item_counter % 100000 == 0:
        print('{} items loaded to the index'.format(item_counter))

  print('A total of {} items added to the index'.format(item_counter))

  print('Building the index with {} trees...'.format(num_trees))
  annoy_index.build(n_trees=num_trees)
  print('Index is successfully built.')
  
  print('Saving index to disk...')
  annoy_index.save(index_filename)
  print('Index is saved to disk.')
  print("Index file size: {} GB".format(
    round(os.path.getsize(index_filename) / float(1024 ** 3), 2)))
  annoy_index.unload()

  print('Saving mapping to disk...')
  with open(index_filename + '.mapping', 'wb') as handle:
    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)
  print('Mapping is saved to disk.')
  print("Mapping file size: {} MB".format(
    round(os.path.getsize(index_filename + '.mapping') / float(1024 ** 2), 2)))

----------------------------------------

TITLE: Configuring Virtual CPUs for DTensor
DESCRIPTION: Sets up virtual CPU devices and configures them for use with DTensor operations. Creates 6 virtual CPU devices for demonstration purposes.

LANGUAGE: Python
CODE:
import tensorflow as tf
from tensorflow.experimental import dtensor

print('TensorFlow version:', tf.__version__)

def configure_virtual_cpus(ncpu):
  phy_devices = tf.config.list_physical_devices('CPU')
  tf.config.set_logical_device_configuration(phy_devices[0], [
        tf.config.LogicalDeviceConfiguration(),
    ] * ncpu)

configure_virtual_cpus(6)
DEVICES = [f'CPU:{i}' for i in range(6)]

tf.config.list_logical_devices('CPU')

----------------------------------------

TITLE: Importing TensorFlow and Basic Operations
DESCRIPTION: Demonstrates importing TensorFlow and performing basic mathematical operations using tf.math functions with automatic type conversion.

LANGUAGE: python
CODE:
import tensorflow as tf

print(tf.math.add(1, 2))
print(tf.math.add([1, 2], [3, 4]))
print(tf.math.square(5))
print(tf.math.reduce_sum([1, 2, 3]))

# Operator overloading is also supported
print(tf.math.square(2) + tf.math.square(3))

----------------------------------------

TITLE: Computing Gradients in Python
DESCRIPTION: Illustrates how to compute gradients using TensorFlow's Python API.

LANGUAGE: Python
CODE:
tf.gradients

----------------------------------------

TITLE: Loading Cassava Leaf Disease Dataset from TFDS
DESCRIPTION: Loads the Cassava Leaf Disease dataset from TensorFlow Datasets, splitting it into train, validation, and test sets.

LANGUAGE: Python
CODE:
tfds_name = 'cassava'
(ds_train, ds_validation, ds_test), ds_info = tfds.load(
    name=tfds_name,
    split=['train', 'validation', 'test'],
    with_info=True,
    as_supervised=True)
TFLITE_NAME_PREFIX = tfds_name

----------------------------------------

TITLE: Copying Subsite Template in Bash
DESCRIPTION: This command copies the subsite template directory from the TensorFlow docs repo to the project repo. It sets up the base structure for the project's documentation.

LANGUAGE: bash
CODE:
$ cp -r tensorflow/docs/tools/templates/subsite/g3doc tensorflow/myproject/

----------------------------------------

TITLE: Creating Symbolic Links for CUDA Libraries
DESCRIPTION: Create symbolic links to CUDA libraries for profiling multiple GPUs on TensorFlow 2.2 and 2.3.

LANGUAGE: Shell
CODE:
sudo ln -s /usr/local/cuda/lib64/libcudart.so.10.2 /usr/local/cuda/lib64/libcudart.so.10.1
sudo ln -s /usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.2 /usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.1

----------------------------------------

TITLE: Importing a Graph Definition in Python
DESCRIPTION: Demonstrates how to import a graph definition and create a session in Python using TensorFlow.

LANGUAGE: Python
CODE:
tf.import_graph_def, tf.Session

----------------------------------------

TITLE: Loading FILM Model from TensorFlow Hub
DESCRIPTION: Loads the FILM frame interpolation model from TensorFlow Hub

LANGUAGE: python
CODE:
model = hub.load("https://tfhub.dev/google/film/1")

----------------------------------------

TITLE: Customizing ExtensionType Behavior in TensorFlow
DESCRIPTION: Demonstrates how to customize ExtensionType behavior by overriding methods like __init__, __repr__, and __eq__, as well as adding custom methods and properties.

LANGUAGE: Python
CODE:
class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor

  def __repr__(self):
    return masked_tensor_str(self.values, self.mask)

  def with_default(self, default):
    return tf.where(self.mask, self.values, default)

  @property
  def dtype(self):
    return self.values.dtype

  def __eq__(self, other):
    result = tf.math.equal(self.values, other.values)
    result = result | ~(self.mask & other.mask)
    return tf.reduce_all(result)

----------------------------------------

TITLE: Loading TF1 Hub Module in TF2 using hub.KerasLayer
DESCRIPTION: Illustrates loading a TF1 Hub module in TensorFlow 2 using the hub.KerasLayer API for easy integration with Keras models.

LANGUAGE: python
CODE:
m = hub.KerasLayer(handle, signature="sig")
outputs = m(inputs)

----------------------------------------

TITLE: Processing Multiple Audio Frames for Bird Classification
DESCRIPTION: Applies the model to all audio frames and displays classification results for each frame.

LANGUAGE: python
CODE:
all_logits, all_embeddings = model.infer_tf(fixed_tm[:1])
for window in fixed_tm[1:]:
  logits, embeddings = model.infer_tf(window[np.newaxis, :])
  all_logits = np.concatenate([all_logits, logits], axis=0)

all_logits.shape

frame = 0
for frame_logits in all_logits:
  probabilities = tf.nn.softmax(frame_logits)
  argmax = np.argmax(probabilities)
  print(f"For frame {frame}, the audio is from the class {classes[argmax]} (element:{argmax} in the label.csv file), with probability of {probabilities[argmax]}")
  frame += 1

----------------------------------------

TITLE: Creating DTensor from Array Helper Function
DESCRIPTION: Helper function to convert arrays or tensors into DTensors by first replicating across mesh devices then sharding according to specified layout.

LANGUAGE: Python
CODE:
def dtensor_from_array(arr, layout, shape=None, dtype=None):
  """Convert a DTensor from something that looks like an array or Tensor.

  This function is convenient for quick doodling DTensors from a known,
  unsharded data object in a single-client environment. This is not the
  most efficient way of creating a DTensor, but it will do for this
  tutorial.
  """
  if shape is not None or dtype is not None:
    arr = tf.constant(arr, shape=shape, dtype=dtype)

  # replicate the input to the mesh
  a = dtensor.copy_to_mesh(arr,
          layout=dtensor.Layout.replicated(layout.mesh, rank=layout.rank))
  # shard the copy to the desirable layout
  return dtensor.relayout(a, layout=layout)

----------------------------------------

TITLE: Visualizing TensorFlow Graph with TensorBoard
DESCRIPTION: Shows how to save and visualize a TensorFlow computation graph using TensorBoard.

LANGUAGE: python
CODE:
writer = tf.summary.FileWriter('.')
writer.add_graph(tf.get_default_graph())
writer.flush()

LANGUAGE: bash
CODE:
tensorboard --logdir .

----------------------------------------

TITLE: Demonstrating Embedding Layer Usage
DESCRIPTION: Shows how to use the embedding layer by passing a list of Bangla words and getting their embedding vectors.

LANGUAGE: Python
CODE:
embedding_layer(['', '', '', '', '']) 

----------------------------------------

TITLE: Building TensorFlow with MKL Optimizations
DESCRIPTION: These bash commands demonstrate how to build TensorFlow from source with Intel MKL-DNN optimizations enabled, which can significantly improve performance on Intel CPUs.

LANGUAGE: bash
CODE:
./configure
# Pick the desired options
bazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package

LANGUAGE: bash
CODE:
./configure
Do you wish to build TensorFlow with MKL support? [y/N] Y
Do you wish to download MKL LIB from the web? [Y/n] Y
# Select the defaults for the rest of the options.

bazel build --config=mkl --copt="-DEIGEN_USE_VML" -c opt //tensorflow/tools/pip_package:build_pip_package

----------------------------------------

TITLE: Training CIFAR-10 Model
DESCRIPTION: Command to run the CIFAR-10 model training script on a single GPU.

LANGUAGE: Shell
CODE:
python cifar10_train.py

----------------------------------------

TITLE: Creating Higher Rank Tensors in TensorFlow
DESCRIPTION: Illustrates the creation of rank 2 and higher tensors using nested lists and tf.Variable.

LANGUAGE: python
CODE:
mymat = tf.Variable([[7],[11]], tf.int16)
myxor = tf.Variable([[False, True],[True, False]], tf.bool)
linear_squares = tf.Variable([[4], [9], [16], [25]], tf.int32)
squarish_squares = tf.Variable([ [4, 9], [16, 25] ], tf.int32)
rank_of_squares = tf.rank(squarish_squares)
mymatC = tf.Variable([[7],[11]], tf.int32)

my_image = tf.zeros([10, 299, 299, 3])  # batch x height x width x color

----------------------------------------

TITLE: WeakTensor Creation Example
DESCRIPTION: Shows how WeakTensors are created when dtype is not explicitly specified, allowing for flexible type promotion.

LANGUAGE: Python
CODE:
tf.constant(5)  # Returns WeakTensor
tf.constant([5.0, 10.0, 3])  # Returns WeakTensor

----------------------------------------

TITLE: Using a Custom FileSystem in C++ for TensorFlow
DESCRIPTION: Example of how to use a custom FileSystem implementation in TensorFlow C++ code, demonstrating the creation of a WritableFile using the registered scheme.

LANGUAGE: C++
CODE:
string filename = "foobar://path/to/file.txt";
std::unique_ptr<WritableFile> file;

// Calls FooBarFileSystem::NewWritableFile to return
// a WritableFile class, which happens to be the FooBarFileSystem's
// WritableFile implementation.
TF_RETURN_IF_ERROR(env->NewWritableFile(filename, &file));

----------------------------------------

TITLE: Using a Custom FileSystem in C++ for TensorFlow
DESCRIPTION: Example of how to use a custom FileSystem implementation in TensorFlow C++ code, demonstrating the creation of a WritableFile using the registered scheme.

LANGUAGE: C++
CODE:
string filename = "foobar://path/to/file.txt";
std::unique_ptr<WritableFile> file;

// Calls FooBarFileSystem::NewWritableFile to return
// a WritableFile class, which happens to be the FooBarFileSystem's
// WritableFile implementation.
TF_RETURN_IF_ERROR(env->NewWritableFile(filename, &file));

----------------------------------------

TITLE: Defining Compressed Conv2D Layer
DESCRIPTION: Creates a compressed 2D convolutional layer that stores weights in a compressed format and decompresses them on-the-fly.

LANGUAGE: python
CODE:
class CompressedConv2D(CustomConv2D):

  def build(self, input_shape, other=None):
    assert isinstance(other, CompressibleConv2D)
    self.input_channels = other.kernel.shape[2]
    self.kernel_compressed, self.kernel_log_step = compress_latent(
        other.kernel_latent, other.kernel_log_step, "kernel")
    self.bias_compressed, self.bias_log_step = compress_latent(
        other.bias_latent, other.bias_log_step, "bias")
    self.built = True

  @property
  def kernel(self):
    rdft_shape = (self.input_channels, self.filters,
                  self.kernel_size, self.kernel_size // 2 + 1, 2)
    kernel_rdft = decompress_latent(
        self.kernel_compressed, rdft_shape, self.kernel_log_step)
    return from_rdft(kernel_rdft, self.kernel_size)

  @property
  def bias(self):
    bias_shape = (self.filters,)
    return decompress_latent(
        self.bias_compressed, bias_shape, self.bias_log_step)

----------------------------------------

TITLE: Adding Unknown Examples to Training Dataset
DESCRIPTION: Adds additional unknown (negative) examples to the training dataset from various TFDS datasets to improve model robustness.

LANGUAGE: Python
CODE:
UNKNOWN_TFDS_DATASETS = [{
    'tfds_name': 'imagenet_v2/matched-frequency',
    'train_split': 'test[:80%]',
    'test_split': 'test[80%:]',
    'num_examples_ratio_to_normal': 1.0,
}, {
    'tfds_name': 'oxford_flowers102',
    'train_split': 'train',
    'test_split': 'test',
    'num_examples_ratio_to_normal': 1.0,
}, {
    'tfds_name': 'beans',
    'train_split': 'train',
    'test_split': 'test',
    'num_examples_ratio_to_normal': 1.0,
}]

# Load unknown datasets and merge with original training data
# ... (code for loading and merging datasets)

----------------------------------------

TITLE: Cloning TensorFlow Models Repository
DESCRIPTION: Clones the TensorFlow models repository to provide sample code for testing the upgrade script.

LANGUAGE: bash
CODE:
git clone --branch r1.13.0 --depth 1 https://github.com/tensorflow/models

----------------------------------------

TITLE: Plotting Correlation Matrix for CORD-19 Embeddings
DESCRIPTION: This function generates embeddings for a list of COVID-19 related terms and plots a correlation matrix to visualize semantic similarities.

LANGUAGE: Python
CODE:
def plot_correlation(labels, features):
  corr = np.inner(features, features)
  corr /= np.max(corr)
  sns.heatmap(corr, xticklabels=labels, yticklabels=labels)

queries = [
  'coronavirus', 'SARS', 'MERS',
  'Italy', 'Spain', 'Europe',
  'cough', 'fever', 'throat'
]

module = hub.load('https://tfhub.dev/tensorflow/cord-19/swivel-128d/3')
embeddings = module(queries)

plot_correlation(queries, embeddings)

----------------------------------------

TITLE: Fine-tuning TF2 SavedModel in TF2 using hub.load
DESCRIPTION: Shows how to load a TF2 SavedModel for fine-tuning in TensorFlow 2 using the hub.load function, including the training flag.

LANGUAGE: python
CODE:
m = hub.load(handle)
outputs = m(inputs, training=is_training)

----------------------------------------

TITLE: Setup Virtual CPUs for DTensor
DESCRIPTION: Configures TensorFlow to use 8 virtual CPUs and imports required libraries

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.experimental import dtensor

def configure_virtual_cpus(ncpu):
  phy_devices = tf.config.list_physical_devices('CPU')
  tf.config.set_logical_device_configuration(
        phy_devices[0], 
        [tf.config.LogicalDeviceConfiguration()] * ncpu)
  
configure_virtual_cpus(8)
tf.config.list_logical_devices('CPU')

devices = [f'CPU:{i}' for i in range(8)]

----------------------------------------

TITLE: Performing Computations with TensorFlow Variables on Different Devices
DESCRIPTION: This code demonstrates how to place TensorFlow variables on one device (CPU) and perform computations on another device (GPU), which can be useful in distributed computing scenarios.

LANGUAGE: Python
CODE:
with tf.device('CPU:0'):
  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
  b = tf.Variable([[1.0, 2.0, 3.0]])

with tf.device('GPU:0'):
  # Element-wise multiply
  k = a * b

print(k)

----------------------------------------

TITLE: Installing TensorFlow 1.15 and TensorFlow Hub for Legacy Use
DESCRIPTION: Commands to install TensorFlow 1.15 (the only supported 1.x version) and TensorFlow Hub. This setup is for legacy use and provides TF1-compatible behavior with some TF2 features.

LANGUAGE: bash
CODE:
$ pip install "tensorflow>=1.15,<2.0"
$ pip install --upgrade tensorflow-hub

----------------------------------------

TITLE: Using S3GAN Discriminator
DESCRIPTION: This snippet demonstrates how to use the discriminator component of the S3GAN model. It generates random input images and labels, then uses the discriminator to make predictions on these inputs.

LANGUAGE: Python
CODE:
disc = Discriminator(module_spec)

batch_size = 4
num_classes = 1000
images = np.random.random(size=[batch_size] + disc.image_shape)
labels = np.random.randint(0, num_classes, size=(batch_size))

disc.predict(images, labels=labels)

----------------------------------------

TITLE: Implementing Basic Input Function in TensorFlow Estimator
DESCRIPTION: Example skeleton for creating an input function that returns feature dictionary and label for Estimator model.

LANGUAGE: python
CODE:
def input_fn(dataset):
   ...  # manipulate dataset, extracting the feature dict and the label
   return feature_dict, label

----------------------------------------

TITLE: Defining Computational Functions for PDE Simulation
DESCRIPTION: This snippet defines helper functions for the PDE simulation. It includes functions to create convolution kernels, perform simplified 2D convolutions, and compute the 2D Laplacian of an array. These functions are crucial for implementing the PDE update rules.

LANGUAGE: Python
CODE:
def make_kernel(a):
  """Transform a 2D array into a convolution kernel"""
  a = np.asarray(a)
  a = a.reshape(list(a.shape) + [1,1])
  return tf.constant(a, dtype=1)

def simple_conv(x, k):
  """A simplified 2D convolution operation"""
  x = tf.expand_dims(tf.expand_dims(x, 0), -1)
  y = tf.nn.depthwise_conv2d(x, k, [1, 1, 1, 1], padding='SAME')
  return y[0, :, :, 0]

def laplace(x):
  """Compute the 2D laplacian of an array"""
  laplace_k = make_kernel([[0.5, 1.0, 0.5],
                           [1.0, -6., 1.0],
                           [0.5, 1.0, 0.5]])
  return simple_conv(x, laplace_k)

----------------------------------------

TITLE: Importing Required Libraries
DESCRIPTION: Imports necessary libraries including matplotlib, TensorFlow, TensorFlow Compression, and TensorFlow Datasets.

LANGUAGE: Python
CODE:
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_compression as tfc
import tensorflow_datasets as tfds


----------------------------------------

TITLE: Building TensorFlow Package with Bazel (CLANG)
DESCRIPTION: Builds the TensorFlow pip package using CLANG compiler.

LANGUAGE: Batch
CODE:
bazel build --config=win_clang --repo_env=TF_PYTHON_VERSION=3.11 //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu

----------------------------------------

TITLE: Training TensorFlow 1 Estimator with Interrupt Hook
DESCRIPTION: Trains the TensorFlow 1 Estimator with the custom interrupt hook to simulate a failure during training.

LANGUAGE: python
CODE:
try:
  classifier.train(input_fn=train_input_fn,
                   hooks=[InterruptHook()],
                   max_steps=10)
except Exception as e:
  print(f'{type(e).__name__}:{e}')

----------------------------------------

TITLE: Setting Random Seeds and Sampling Rate
DESCRIPTION: Sets random seeds for reproducibility and defines the audio sampling rate constant.

LANGUAGE: Python
CODE:
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

# Sampling rate for audio playback
_SAMPLING_RATE = 16000

----------------------------------------

TITLE: Checking TensorFlow Version
DESCRIPTION: Imports TensorFlow and prints the installed version to ensure TensorFlow 2.x is being used.

LANGUAGE: python
CODE:
import tensorflow as tf

print(tf.__version__)

----------------------------------------

TITLE: TensorFlow Documentation Project Table in Markdown
DESCRIPTION: A markdown table mapping tensorflow.org project paths to their corresponding GitHub documentation locations for the TensorFlow ecosystem.

LANGUAGE: markdown
CODE:
tensorflow.org project | GitHub docs location
-----------------------|---------------------
[/addons](https://www.tensorflow.org/addons) | https://github.com/tensorflow/addons/tree/master/docs
[/agents](https://www.tensorflow.org/agents) | https://github.com/tensorflow/agents/tree/master/docs
[/cloud](https://www.tensorflow.org/cloud) | https://github.com/tensorflow/cloud/tree/master/g3doc
[/datasets](https://www.tensorflow.org/datasets) | https://github.com/tensorflow/datasets/tree/master/docs

----------------------------------------

TITLE: Loading and Preprocessing MNIST Dataset
DESCRIPTION: Loads the MNIST dataset and preprocesses it by normalizing the pixel values.

LANGUAGE: python
CODE:
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

----------------------------------------

TITLE: TensorFlow 1.x Checkpoint Saving with Estimator
DESCRIPTION: Demonstrates checkpoint saving configuration using tf.estimator.RunConfig and tf.estimator.DNNClassifier in TensorFlow 1.x style.

LANGUAGE: python
CODE:
feature_columns = [tf1.feature_column.numeric_column("x", shape=[28, 28])]

config = tf1.estimator.RunConfig(save_summary_steps=1,
                                 save_checkpoints_steps=1)

path = tempfile.mkdtemp()

classifier = tf1.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[256, 32],
    optimizer=tf1.train.AdamOptimizer(0.001),
    n_classes=10,
    dropout=0.2,
    model_dir=path,
    config = config
)

train_input_fn = tf1.estimator.inputs.numpy_input_fn(
    x={"x": x_train},
    y=y_train.astype(np.int32),
    num_epochs=10,
    batch_size=50,
    shuffle=True,
)

test_input_fn = tf1.estimator.inputs.numpy_input_fn(
    x={"x": x_test},
    y=y_test.astype(np.int32),
    num_epochs=10,
    shuffle=False
)

train_spec = tf1.estimator.TrainSpec(input_fn=train_input_fn, max_steps=10)
eval_spec = tf1.estimator.EvalSpec(input_fn=test_input_fn,
                                   steps=10,
                                   throttle_secs=0)

tf1.estimator.train_and_evaluate(estimator=classifier,
                                train_spec=train_spec,
                                eval_spec=eval_spec)

----------------------------------------

TITLE: Implementing Audio Preprocessing Functions
DESCRIPTION: Defines functions for framing audio and ensuring correct sample rate for model input.

LANGUAGE: python
CODE:
def frame_audio(
      audio_array: np.ndarray,
      window_size_s: float = 5.0,
      hop_size_s: float = 5.0,
      sample_rate = 32000,
  ) -> np.ndarray:
    """Helper function for framing audio for inference."""
    if window_size_s is None or window_size_s < 0:
      return audio_array[np.newaxis, :]
    frame_length = int(window_size_s * sample_rate)
    hop_length = int(hop_size_s * sample_rate)
    framed_audio = tf.signal.frame(audio_array, frame_length, hop_length, pad_end=True)
    return framed_audio

def ensure_sample_rate(waveform, original_sample_rate,
                       desired_sample_rate=32000):
  """Resample waveform if required."""
  if original_sample_rate != desired_sample_rate:
    waveform = tfio.audio.resample(waveform, original_sample_rate, desired_sample_rate)
  return desired_sample_rate, waveform

----------------------------------------

TITLE: Configuring TPU RunConfig
DESCRIPTION: Demonstrates how to build a complete RunConfig for TPU usage, including cluster resolver setup and configuration parameters.

LANGUAGE: python
CODE:
import tempfile
import subprocess

class FLAGS(object):
  use_tpu=False
  tpu_name=None
  # Use a local temporary path for the `model_dir`
  model_dir = tempfile.mkdtemp()
  # Number of training steps to run on the Cloud TPU before returning control.
  iterations = 50
  # A single Cloud TPU has 8 shards.
  num_shards = 8

if FLAGS.use_tpu:
    my_project_name = subprocess.check_output([
        'gcloud','config','get-value','project'])
    my_zone = subprocess.check_output([
        'gcloud','config','get-value','compute/zone'])
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
            tpu=[FLAGS.tpu_name],
            zone=my_zone,
            project=my_project_name)
    master = tpu_cluster_resolver.get_master()
else:
    master = ''

my_tpu_run_config = tf.estimator.tpu.RunConfig(
    master=master,
    evaluation_master=master,
    model_dir=FLAGS.model_dir,
    session_config=tf.ConfigProto(
        allow_soft_placement=True, log_device_placement=True),
    tpu_config=tf.estimator.tpu.TPUConfig(FLAGS.iterations,
                                          FLAGS.num_shards),
)

----------------------------------------

TITLE: Wrapping Loaded Model in Keras Layer
DESCRIPTION: Use TensorFlow Hub to wrap the loaded model in a Keras layer for further training or embedding in a larger model.

LANGUAGE: Python
CODE:
import tensorflow_hub as hub

def build_model(loaded):
  x = tf.keras.layers.Input(shape=(28, 28, 1), name='input_x')
  # Wrap what's loaded to a KerasLayer
  keras_layer = hub.KerasLayer(loaded, trainable=True)(x)
  model = tf.keras.Model(x, keras_layer)
  return model

another_strategy = tf.distribute.MirroredStrategy()
with another_strategy.scope():
  loaded = tf.saved_model.load(saved_model_path)
  model = build_model(loaded)

  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=[tf.metrics.SparseCategoricalAccuracy()])
  model.fit(train_dataset, epochs=2)

----------------------------------------

TITLE: Basic Feature Column Input Handling
DESCRIPTION: Demonstrates basic input handling with feature columns by creating numeric columns and concatenating them

LANGUAGE: Python
CODE:
input_dict = {
  'foo': tf.constant([1]),
  'bar': tf.constant([0]), 
  'baz': tf.constant([-1])
}

columns = [
  tf1.feature_column.numeric_column('foo'),
  tf1.feature_column.numeric_column('bar'),
  tf1.feature_column.numeric_column('baz'),
]
call_feature_columns(columns, input_dict)

----------------------------------------

TITLE: Installing Clang-Format for C++ Code Formatting
DESCRIPTION: Command to install clang-format on Ubuntu 16+ for C++ code style checking.

LANGUAGE: bash
CODE:
$ apt-get install -y clang-format

----------------------------------------

TITLE: Importing Required Libraries
DESCRIPTION: Imports necessary Python libraries for image processing, visualization, and machine learning tasks using TensorFlow Hub

LANGUAGE: python
CODE:
from absl import logging

import matplotlib.pyplot as plt
import numpy as np
from PIL import Image, ImageOps
from scipy.spatial import cKDTree
from skimage.feature import plot_matches
from skimage.measure import ransac
from skimage.transform import AffineTransform
from six import BytesIO

import tensorflow as tf

import tensorflow_hub as hub
from six.moves.urllib.request import urlopen

----------------------------------------

TITLE: Iterating Over TensorShape in TensorFlow 2
DESCRIPTION: Shows how to iterate over a TensorShape object in TensorFlow 2, which is simpler than in TensorFlow 1.x as it directly yields integer values.

LANGUAGE: Python
CODE:
for value in shape:
  print(value)

----------------------------------------

TITLE: Persistent Gradient Tape Example
DESCRIPTION: Demonstrates the use of persistent gradient tape to compute multiple gradients from the same recorded computation.

LANGUAGE: Python
CODE:
x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as t:
  t.watch(x)
  y = x * x
  z = y * y
dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)
dy_dx = t.gradient(y, x)  # 6.0
del t  # Drop the reference to the tape

----------------------------------------

TITLE: TensorFlow Core Version String Declaration
DESCRIPTION: Location of main TensorFlow version string definition

LANGUAGE: c
CODE:
TF_VERSION_STRING in tensorflow/core/public/release_version.h

----------------------------------------

TITLE: Installing Pylint for Python Code Analysis
DESCRIPTION: Command to install pylint package using pip for Python code style checking.

LANGUAGE: bash
CODE:
$ pip install pylint

----------------------------------------

TITLE: Installing Pylint for Python Code Analysis
DESCRIPTION: Command to install pylint package using pip for Python code style checking.

LANGUAGE: bash
CODE:
$ pip install pylint

----------------------------------------

TITLE: Creating a Custom Keras Model
DESCRIPTION: Shows how to create a custom Keras model by subclassing tf.keras.Model.

LANGUAGE: Python
CODE:
@keras.saving.register_keras_serializable()
class MySequentialModel(tf.keras.Model):
  def __init__(self, name=None, **kwargs):
    super().__init__(**kwargs)

    self.dense_1 = FlexibleDense(out_features=3)
    self.dense_2 = FlexibleDense(out_features=2)
  def call(self, x):
    x = self.dense_1(x)
    return self.dense_2(x)

# You have made a Keras model!
my_sequential_model = MySequentialModel(name="the_model")

# Call it on a tensor, with random results
print("Model results:", my_sequential_model(tf.constant([[2.0, 2.0, 2.0]])))

----------------------------------------

TITLE: Formatting RFC Directory Name in Markdown
DESCRIPTION: Shows the format for naming directories containing auxiliary files for an RFC.

LANGUAGE: markdown
CODE:
`YYYYMMDD-descriptive-name`

----------------------------------------

TITLE: Major Version Format Example in Python
DESCRIPTION: Example showing the format of TensorFlow version numbers

LANGUAGE: python
CODE:
TensorFlow version 1.2.3 has:
MAJOR version 1
MINOR version 2
PATCH version 3

----------------------------------------

TITLE: Loading TensorFlow Hub Module for Video Inbetweening in Python
DESCRIPTION: This code loads a TensorFlow Hub module for video inbetweening using 3D convolutions on the BAIR dataset.

LANGUAGE: python
CODE:
hub_handle = 'https://tfhub.dev/google/tweening_conv3d_bair/1'
module = hub.load(hub_handle).signatures['default']

----------------------------------------

TITLE: MoViNet Base Model Inference
DESCRIPTION: Loading and running inference with the base MoViNet model from TensorFlow Hub

LANGUAGE: python
CODE:
id = 'a2'
mode = 'base'
version = '3'
hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'
model = hub.load(hub_url)

logits = sig(image = jumpingjack[tf.newaxis, ...])
logits = logits['classifier_head'][0]

----------------------------------------

TITLE: Defining PDE Parameters and Update Rules in TensorFlow
DESCRIPTION: This snippet defines the parameters and update rules for the partial differential equation. It creates TensorFlow placeholders for time resolution and damping, variables for the simulation state, and specifies the discretized PDE update rules using TensorFlow operations.

LANGUAGE: Python
CODE:
# Parameters:
# eps -- time resolution
# damping -- wave damping
eps = tf.placeholder(tf.float32, shape=())
damping = tf.placeholder(tf.float32, shape=())

# Create variables for simulation state
U  = tf.Variable(u_init)
Ut = tf.Variable(ut_init)

# Discretized PDE update rules
U_ = U + eps * Ut
Ut_ = Ut + eps * (laplace(U) - damping * Ut)

# Operation to update the state
step = tf.group(
  U.assign(U_),
  Ut.assign(Ut_))

----------------------------------------

TITLE: Distributed Training with ParameterServerStrategy in TensorFlow 2
DESCRIPTION: Demonstrates how to use ParameterServerStrategy for distributed training in TensorFlow 2. This includes defining the model, compiling it within the strategy scope, and using Keras Model.fit for training.

LANGUAGE: Python
CODE:
dataset = tf.data.Dataset.from_tensor_slices(
      (features, labels)).shuffle(10).repeat().batch(64)

eval_dataset = tf.data.Dataset.from_tensor_slices(
      (eval_features, eval_labels)).repeat().batch(1)

with strategy.scope():
  model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])
  optimizer = tf.keras.optimizers.legacy.Adagrad(learning_rate=0.05)
  model.compile(optimizer, "mse")

model.fit(dataset, epochs=5, steps_per_epoch=10)

model.evaluate(eval_dataset, steps=10, return_dict=True)

----------------------------------------

TITLE: Using TFDBG with Keras
DESCRIPTION: Example showing how to use tfdbg with Keras models

LANGUAGE: python
CODE:
import tensorflow as tf
from tensorflow.python import debug as tf_debug

tf.keras.backend.set_session(tf_debug.LocalCLIDebugWrapperSession(tf.Session()))

# Define your keras model, called "model".

model.fit(...)
model.evaluate(...)
model.predict(...)

----------------------------------------

TITLE: Constructing Ragged Tensors from Row Partitions
DESCRIPTION: Shows different methods to construct ragged tensors using row partitioning tensors like value_rowids, row_lengths, and row_splits.

LANGUAGE: python
CODE:
print(tf.RaggedTensor.from_value_rowids(
    values=[3, 1, 4, 1, 5, 9, 2],
    value_rowids=[0, 0, 0, 0, 2, 2, 3]))

print(tf.RaggedTensor.from_row_lengths(
    values=[3, 1, 4, 1, 5, 9, 2],
    row_lengths=[4, 0, 2, 1]))

print(tf.RaggedTensor.from_row_splits(
    values=[3, 1, 4, 1, 5, 9, 2],
    row_splits=[0, 4, 4, 6, 7]))

----------------------------------------

TITLE: Defining CNN Model Architecture
DESCRIPTION: Implements a convolutional neural network model using Keras layers within an Estimator model function. Includes loss calculation and optimization setup.

LANGUAGE: Python
CODE:
LEARNING_RATE = 1e-4
def model_fn(features, labels, mode):
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10)
  ])
  logits = model(features, training=False)

  if mode == tf.estimator.ModeKeys.PREDICT:
    predictions = {'logits': logits}
    return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)

  optimizer = tf.compat.v1.train.GradientDescentOptimizer(
      learning_rate=LEARNING_RATE)
  loss = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(labels, logits)
  loss = tf.reduce_sum(loss) * (1. / BATCH_SIZE)
  if mode == tf.estimator.ModeKeys.EVAL:
    return tf.estimator.EstimatorSpec(mode, loss=loss)

  return tf.estimator.EstimatorSpec(
      mode=mode,
      loss=loss,
      train_op=optimizer.minimize(
          loss, tf.compat.v1.train.get_or_create_global_step()))

----------------------------------------

TITLE: Image Processing Utility Functions
DESCRIPTION: Defines helper functions for loading, cropping, resizing and displaying images. Includes functions to center-crop images and maintain aspect ratios during resizing.

LANGUAGE: python
CODE:
def crop_center(image):
  """Returns a cropped square image."""
  shape = image.shape
  new_shape = min(shape[1], shape[2])
  offset_y = max(shape[1] - shape[2], 0) // 2
  offset_x = max(shape[2] - shape[1], 0) // 2
  image = tf.image.crop_to_bounding_box(
      image, offset_y, offset_x, new_shape, new_shape)
  return image

@functools.lru_cache(maxsize=None)
def load_image(image_url, image_size=(256, 256), preserve_aspect_ratio=True):
  """Loads and preprocesses images."""
  image_path = tf.keras.utils.get_file(os.path.basename(image_url)[-128:], image_url)
  img = tf.io.decode_image(
      tf.io.read_file(image_path),
      channels=3, dtype=tf.float32)[tf.newaxis, ...]
  img = crop_center(img)
  img = tf.image.resize(img, image_size, preserve_aspect_ratio=True)
  return img

----------------------------------------

TITLE: Markdown Header and Logo Image
DESCRIPTION: Markdown formatting for the TensorFlow documentation header with centered logo image.

LANGUAGE: markdown
CODE:
# TensorFlow Documentation

<div align="center">
  <img src="https://www.tensorflow.org/images/tf_logo_horizontal.png"><br><br>
</div>

----------------------------------------

TITLE: Audio File Format Conversion for SPICE Model
DESCRIPTION: Function to convert audio files to the format required by SPICE model - 16kHz sample rate and mono channel

LANGUAGE: python
CODE:
def convert_audio_for_model(user_file, output_file='converted_audio_file.wav'):
  audio = AudioSegment.from_file(user_file)
  audio = audio.set_frame_rate(EXPECTED_SAMPLE_RATE).set_channels(1)
  audio.export(output_file, format="wav")
  return output_file

----------------------------------------

TITLE: Configuring GPU Memory Growth in TensorFlow
DESCRIPTION: Examples of configuring GPU memory allocation behavior through allow_growth and per_process_gpu_memory_fraction options.

LANGUAGE: python
CODE:
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config, ...)

LANGUAGE: python
CODE:
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
session = tf.Session(config=config, ...)

----------------------------------------

TITLE: Creating Random Number Generators
DESCRIPTION: Demonstrates creating RNG generators using different methods including from_seed and global generator.

LANGUAGE: python
CODE:
g1 = tf.random.Generator.from_seed(1)
print(g1.normal(shape=[2, 3]))
g2 = tf.random.get_global_generator()
print(g2.normal(shape=[2, 3]))

----------------------------------------

TITLE: Converting Python Objects to Tensors in TensorFlow
DESCRIPTION: Shows how tf.convert_to_tensor can be used to convert Python objects to tensors.

LANGUAGE: Python
CODE:
tf.convert_to_tensor([1,2,3])

tf.reduce_max([1,2,3])

tf.reduce_max(np.array([1,2,3]))

----------------------------------------

TITLE: Setting up TensorFlow Imports and Dependencies
DESCRIPTION: Imports required TensorFlow packages and libraries for the examples

LANGUAGE: Python
CODE:
import time
import numpy as np
import tensorflow as tf
import tensorflow.compat.v1 as tf1
import tensorflow_datasets as tfds

----------------------------------------

TITLE: Loading Libraries and Configurations
DESCRIPTION: Importing required Python libraries and setting matplotlib configurations

LANGUAGE: python
CODE:
import pathlib
import matplotlib as mpl
import matplotlib.pyplot as plt
import mediapy as media
import numpy as np
import PIL
import tensorflow as tf
import tensorflow_hub as hub
import tqdm

mpl.rcParams.update({
    'font.size': 10,
})

----------------------------------------

TITLE: Time Series Data Windowing
DESCRIPTION: Demonstrates how to create sliding windows over sequential data using tf.data.Dataset.window()

LANGUAGE: Python
CODE:
def make_window_dataset(ds, window_size=5, shift=1, stride=1):
  windows = ds.window(window_size, shift=shift, stride=stride)

  def sub_to_batch(sub):
    return sub.batch(window_size, drop_remainder=True)

  windows = windows.flat_map(sub_to_batch)
  return windows

----------------------------------------

TITLE: Custom Training Loop with Manual Checkpointing in TensorFlow 2
DESCRIPTION: Implements a custom training loop that manually saves and loads checkpoints at the beginning of each epoch, demonstrating fault tolerance in TensorFlow 2.

LANGUAGE: python
CODE:
for epoch in range(epochs):
  if epoch > 0:
      tf.train.load_checkpoint(save_path)
  print(f"\nStart of epoch {epoch}")

  for step in range(steps_per_epoch):
    with tf.GradientTape() as tape:

      logits = model(x_train, training=True)
      loss_value = loss_fn(y_train, logits)

      grads = tape.gradient(loss_value, model.trainable_weights)
      optimizer.apply_gradients(zip(grads, model.trainable_weights))

    save_path = checkpoint_manager.save()
    print(f"Checkpoint saved to {save_path}")
    print(f"Training loss at step {step}: {loss_value}")

----------------------------------------

TITLE: Creating Feature Columns for TensorFlow Estimator
DESCRIPTION: Python code to create numeric feature columns for each input feature in a TensorFlow Estimator.

LANGUAGE: python
CODE:
# Feature columns describe how to use the input.
my_feature_columns = []
for key in train_x.keys():
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))

----------------------------------------

TITLE: Creating Linear Classifier Estimator
DESCRIPTION: Instantiates a pre-made LinearClassifier estimator using the defined feature columns

LANGUAGE: Python
CODE:
model_dir = tempfile.mkdtemp()
model = tf.estimator.LinearClassifier(
    model_dir=model_dir,
    feature_columns=[embark, cls, age],
    n_classes=2
)

----------------------------------------

TITLE: Using nblint Notebook Linter
DESCRIPTION: Basic usage commands for the nblint tool including linting, fixing, and accessing help documentation.

LANGUAGE: shell
CODE:
$ python3 -m tensorflow_docs.tools.nblint [options] notebook.ipynb [...]

$ python3 -m tensorflow_docs.tools.nblint --fix [options] notebook.ipynb [...]

$ python3 -m tensorflow_docs.tools.nblint --help

----------------------------------------

TITLE: Implementing Interrupt Callback for TensorFlow 2 Keras
DESCRIPTION: Defines a custom Keras callback that artificially interrupts training at a specific epoch to simulate a failure scenario.

LANGUAGE: python
CODE:
class InterruptAtEpoch(tf.keras.callbacks.Callback):
  # A callback for artificially interrupting training.
  def __init__(self, interrupting_epoch=3):
    self.interrupting_epoch = interrupting_epoch

  def on_epoch_end(self, epoch, log=None):
    if epoch == self.interrupting_epoch:
      raise RuntimeError('Interruption')

----------------------------------------

TITLE: TensorFlow Hub Model Loading
DESCRIPTION: Loads the pre-trained arbitrary image stylization model from TensorFlow Hub. The model takes content and style images as input and outputs a stylized image.

LANGUAGE: python
CODE:
hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'
hub_module = hub.load(hub_handle)

----------------------------------------

TITLE: Importing TensorFlow Dependencies
DESCRIPTION: Basic imports of TensorFlow and helper libraries needed for distributed training

LANGUAGE: python
CODE:
import tensorflow as tf

# Helper libraries
import numpy as np
import os

print(tf.__version__)

----------------------------------------

TITLE: Installing Dependencies - TensorFlow Setup
DESCRIPTION: Installs required Python packages including TensorFlow Text, Bokeh visualization library, and neighbor search dependencies.

LANGUAGE: python
CODE:
%%capture
#@title Setup Environment
# Install the latest Tensorflow version.
!pip install "tensorflow-text==2.11.*"
!pip install bokeh
!pip install simpleneighbors[annoy]
!pip install tqdm

----------------------------------------

TITLE: Defining Feature Columns for TensorFlow Estimator in Python
DESCRIPTION: This code creates a list of numeric feature columns for the Iris dataset, which will be used to configure the Estimator.

LANGUAGE: Python
CODE:
# Feature columns describe how to use the input.
my_feature_columns = []
for key in train.keys():
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))

----------------------------------------

TITLE: Preprocessing Audio Data for YAMNet Model
DESCRIPTION: This code reads a WAV file, ensures the correct sample rate, and normalizes the audio data for input to the YAMNet model.

LANGUAGE: python
CODE:
wav_file_name = 'miaow_16k.wav'
sample_rate, wav_data = wavfile.read(wav_file_name, 'rb')
sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)

# Show some basic information about the audio.
duration = len(wav_data)/sample_rate
print(f'Sample rate: {sample_rate} Hz')
print(f'Total duration: {duration:.2f}s')
print(f'Size of the input: {len(wav_data)}')

# Listening to the wav file.
Audio(wav_data, rate=sample_rate)

waveform = wav_data / tf.int16.max

----------------------------------------

TITLE: Stopping Gradient Recording in TensorFlow
DESCRIPTION: Demonstrate how to temporarily stop gradient recording using tf.GradientTape.stop_recording() method.

LANGUAGE: Python
CODE:
x = tf.Variable(2.0)
y = tf.Variable(3.0)

with tf.GradientTape() as t:
  x_sq = x * x
  with t.stop_recording():
    y_sq = y * y
  z = x_sq + y_sq

grad = t.gradient(z, {'x': x, 'y': y})

print('dz/dx:', grad['x'])  # 2*x => 4
print('dz/dy:', grad['y'])

----------------------------------------

TITLE: Logging GPU and Memory Utilization
DESCRIPTION: Use nvidia-smi to log GPU and memory utilization to a CSV file.

LANGUAGE: Shell
CODE:
nvidia-smi
--query-gpu=utilization.gpu,utilization.memory,memory.total,
memory.free,memory.used --format=csv

----------------------------------------

TITLE: Importing Required Libraries
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Hub, and mediapy for image/video processing

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow_hub as hub
import requests
import numpy as np
from typing import Generator, Iterable, List, Optional
import mediapy as media

----------------------------------------

TITLE: Splitting dataset into training and test sets in Python
DESCRIPTION: Splits the dataset into training and test sets using an 80-20 split ratio.

LANGUAGE: Python
CODE:
train_dataset = dataset.sample(frac=0.8, random_state=0)
test_dataset = dataset.drop(train_dataset.index)

----------------------------------------

TITLE: Profiling with TensorBoard Keras Callback
DESCRIPTION: Use TensorBoard Keras Callback to profile specific batches during model training.

LANGUAGE: Python
CODE:
# Profile from batches 10 to 15
tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,
                                                 profile_batch='10, 15')

# Train the model and use the TensorBoard Keras callback to collect
# performance profiling data
model.fit(train_data,
              steps_per_epoch=20,
              epochs=5,
              callbacks=[tb_callback])

----------------------------------------

TITLE: Debug Stripper Optimization Example
DESCRIPTION: Creates a test function with numeric checking to demonstrate the debug stripper optimizer's functionality.

LANGUAGE: python
CODE:
def test_function_2():
  @tf.function
  def simple_func(input_arg):
    output = input_arg
    tf.debugging.check_numerics(output, "Bad!")
    return output
  return simple_func

----------------------------------------

TITLE: Using nbfmt Notebook Formatter
DESCRIPTION: Basic usage commands for the nbfmt tool to format Jupyter notebooks and access help documentation.

LANGUAGE: shell
CODE:
$ python3 -m tensorflow_docs.tools.nbfmt [options] notebook.ipynb [...]

$ python3 -m tensorflow_docs.tools.nbfmt --help

----------------------------------------

TITLE: Using ExtensionTypes with tf.data.Dataset in TensorFlow
DESCRIPTION: Demonstrates how to use extension types with tf.data.Dataset, including creating datasets from extension types and batching/unbatching datasets containing extension types.

LANGUAGE: Python
CODE:
ds = tf.data.Dataset.from_tensors(Pastry(5, 5))

mt = MaskedTensor(tf.reshape(range(20), [5, 4]), tf.ones([5, 4]))
ds = tf.data.Dataset.from_tensor_slices(mt)

def value_gen():
  for i in range(2, 7):
    yield MaskedTensor(range(10), [j%i != 0 for j in range(10)])

ds = tf.data.Dataset.from_generator(
    value_gen, output_signature=MaskedTensor.Spec(shape=[10], dtype=tf.int32))

batched_ds = ds.batch(2)
unbatched_ds = batched_ds.unbatch()

----------------------------------------

TITLE: Batching Dataset Elements
DESCRIPTION: Example showing how to batch dataset elements using simple batching and padded batching for variable length sequences

LANGUAGE: python
CODE:
dataset = tf.data.Dataset.range(100)
dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))
dataset = dataset.padded_batch(4, padded_shapes=(None,))

----------------------------------------

TITLE: Pushing changes to GitHub
DESCRIPTION: Command to push local changes to a GitHub repository. This is used to share contributions with the TensorFlow project.

LANGUAGE: Bash
CODE:
$ git push origin branch-name

----------------------------------------

TITLE: Importing Required Libraries for TensorFlow Model Training
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Datasets, and TensorFlow Model Maker components for image classification tasks.

LANGUAGE: Python
CODE:
import matplotlib.pyplot as plt
import os
import seaborn as sns

import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow_examples.lite.model_maker.core.export_format import ExportFormat
from tensorflow_examples.lite.model_maker.core.task import image_preprocessing

from tflite_model_maker import image_classifier
from tflite_model_maker import ImageClassifierDataLoader
from tflite_model_maker.image_classifier import ModelSpec

----------------------------------------

TITLE: Formatting RFC Filename in Markdown
DESCRIPTION: Demonstrates the correct format for naming RFC files, using a date and descriptive name.

LANGUAGE: markdown
CODE:
`YYYYMMDD-descriptive-name.md`

----------------------------------------

TITLE: Implementing Evaluation Logic in TensorFlow Estimator
DESCRIPTION: Python code to implement evaluation logic in a TensorFlow Estimator, including accuracy metric calculation.

LANGUAGE: python
CODE:
# Compute evaluation metrics.
accuracy = tf.metrics.accuracy(labels=labels,
                               predictions=predicted_classes,
                               name='acc_op')
metrics = {'accuracy': accuracy}
tf.summary.scalar('accuracy', accuracy[1])

if mode == tf.estimator.ModeKeys.EVAL:
    return tf.estimator.EstimatorSpec(
        mode, loss=loss, eval_metric_ops=metrics)

----------------------------------------

TITLE: Creating and Manipulating Multiple TensorFlow Variables
DESCRIPTION: This code demonstrates creating multiple variables, assigning values, and performing operations like addition and subtraction on TensorFlow variables.

LANGUAGE: Python
CODE:
a = tf.Variable([2.0, 3.0])
# Create b based on the value of a
b = tf.Variable(a)
a.assign([5, 6])

# a and b are different
print(a.numpy())
print(b.numpy())

# There are other versions of assign
print(a.assign_add([2,3]).numpy())  # [7. 9.]
print(a.assign_sub([7,9]).numpy())  # [0. 0.]

----------------------------------------

TITLE: Defining Helper Functions for Image Loading and Preprocessing
DESCRIPTION: Defines functions to load images from URLs, preprocess them for model input, and display images using matplotlib.

LANGUAGE: Python
CODE:
original_image_cache = {}

def preprocess_image(image):
  image = np.array(image)
  # reshape into shape [batch_size, height, width, num_channels]
  img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])
  # Use `convert_image_dtype` to convert to floats in the [0,1] range.
  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)
  return image

def load_image_from_url(img_url):
  """Returns an image with shape [1, height, width, num_channels]."""
  user_agent = {'User-agent': 'Colab Sample (https://tensorflow.org)'}
  response = requests.get(img_url, headers=user_agent)
  image = Image.open(BytesIO(response.content))
  image = preprocess_image(image)
  return image

def load_image(image_url, image_size=256, dynamic_size=False, max_dynamic_size=512):
  """Loads and preprocesses images."""
  # Cache image file locally.
  if image_url in original_image_cache:
    img = original_image_cache[image_url]
  elif image_url.startswith('https://'):
    img = load_image_from_url(image_url)
  else:
    fd = tf.io.gfile.GFile(image_url, 'rb')
    img = preprocess_image(Image.open(fd))
  original_image_cache[image_url] = img
  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].
  img_raw = img
  if tf.reduce_max(img) > 1.0:
    img = img / 255.
  if len(img.shape) == 3:
    img = tf.stack([img, img, img], axis=-1)
  if not dynamic_size:
    img = tf.image.resize_with_pad(img, image_size, image_size)
  elif img.shape[1] > max_dynamic_size or img.shape[2] > max_dynamic_size:
    img = tf.image.resize_with_pad(img, max_dynamic_size, max_dynamic_size)
  return img, img_raw

def show_image(image, title=''):
  image_size = image.shape[1]
  w = (image_size * 6) // 320
  plt.figure(figsize=(w, w))
  plt.imshow(image[0], aspect='equal')
  plt.axis('off')
  plt.title(title)
  plt.show()

----------------------------------------

TITLE: Creating MultiWorkerMirroredStrategy for Distributed Training in TensorFlow
DESCRIPTION: Creates a MultiWorkerMirroredStrategy instance for synchronous distributed training across multiple workers. This strategy uses CollectiveOps for multi-worker all-reduce communication.

LANGUAGE: Python
CODE:
multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

----------------------------------------

TITLE: TensorFlow 2 Keras Implementation
DESCRIPTION: Equivalent implementation using tf.keras APIs with built-in training methods Model.fit() and Model.evaluate().

LANGUAGE: python
CODE:
dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(1)
eval_dataset = tf.data.Dataset.from_tensor_slices(
      (eval_features, eval_labels)).batch(1)

model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])
optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)

model.compile(optimizer=optimizer, loss="mse")
model.fit(dataset)
model.evaluate(eval_dataset, return_dict=True)

----------------------------------------

TITLE: Configuring TensorFlow Data Threading Options
DESCRIPTION: Example showing how to configure intra-op parallelism options for tf.data pipeline optimization. This can improve efficiency by reducing threading overhead.

LANGUAGE: python
CODE:
dataset = ...
options = tf.data.Options()
options.experimental_threading.max_intra_op_parallelism = 1
dataset = dataset.with_options(options)

----------------------------------------

TITLE: Defining Image Display Functions
DESCRIPTION: Defines helper functions for displaying image grids and converting image arrays to uint8 format.

LANGUAGE: Python
CODE:
def imgrid(imarray, cols=4, pad=1, padval=255, row_major=True):
    """Lays out a [N, H, W, C] image array as a single image grid."""
    # Function implementation...

def interleave(*args):
    """Interleaves input arrays of the same shape along the batch axis."""
    # Function implementation...

def imshow(a, format='png', jpeg_fallback=True):
    """Displays an image in the given format."""
    # Function implementation...

def image_to_uint8(x):
    """Converts [-1, 1] float array to [0, 255] uint8."""
    # Function implementation...

----------------------------------------

TITLE: TensorFlow 2 SidecarEvaluator Implementation
DESCRIPTION: Demonstrates using tf.keras.utils.SidecarEvaluator to evaluate the trained model using test data

LANGUAGE: python
CODE:
data = tf.data.Dataset.from_tensor_slices((x_test, y_test))
data = data.batch(64)

tf.keras.utils.SidecarEvaluator(
    model=model,
    data=data,
    checkpoint_dir=log_dir,
    max_evaluations=1
).start()

----------------------------------------

TITLE: Naming TensorFlow Variables and Comparing Values
DESCRIPTION: This snippet shows how to assign names to TensorFlow variables and compare their values, demonstrating that variables with the same name can have different values.

LANGUAGE: Python
CODE:
# Create a and b; they will have the same name but will be backed by
# different tensors.
a = tf.Variable(my_tensor, name="Mark")
# A new variable with the same name, but different value
# Note that the scalar add is broadcast
b = tf.Variable(my_tensor + 1, name="Mark")

# These are elementwise-unequal, despite having the same name
print(a == b)

----------------------------------------

TITLE: Creating and Activating Python Virtualenv
DESCRIPTION: Commands to create and activate a Python virtual environment for TensorFlow Hub development

LANGUAGE: shell
CODE:
virtualenv --system-site-packages tensorflow_hub_env

LANGUAGE: shell
CODE:
source ~/tensorflow_hub_env/bin/activate  # bash, sh, ksh, or zsh
source ~/tensorflow_hub_env/bin/activate.csh  # csh or tcsh

----------------------------------------

TITLE: Building and Saving a SavedModel
DESCRIPTION: Example showing how to build a SavedModel with multiple MetaGraphDefs using the SavedModelBuilder

LANGUAGE: python
CODE:
export_dir = ...
...
builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph_and_variables(sess,
                                       [tag_constants.TRAINING],
                                       signature_def_map=foo_signatures,
                                       assets_collection=foo_assets,
                                       strip_default_attrs=True)
...
# Add a second MetaGraphDef for inference.
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph([tag_constants.SERVING], strip_default_attrs=True)
...
builder.save()

----------------------------------------

TITLE: Handling AutoTrackable Error in TF2
DESCRIPTION: Demonstrates the correct way to load and use TF1 Hub models in TF2 by using the signatures API. Shows both incorrect and correct implementations.

LANGUAGE: python
CODE:
# BAD: Raises error
embed = hub.load('https://tfhub.dev/google/nnlm-en-dim128/1')
embed(['my text', 'batch'])

LANGUAGE: python
CODE:
embed = hub.load('https://tfhub.dev/google/nnlm-en-dim128/1')
embed.signatures['default'](['my text', 'batch'])

----------------------------------------

TITLE: CSV Line Parsing for TensorFlow Dataset
DESCRIPTION: Function to parse CSV lines into feature dictionaries and labels for TensorFlow dataset processing. Handles field defaults and column mapping.

LANGUAGE: python
CODE:
# Metadata describing the text columns
COLUMNS = ['SepalLength', 'SepalWidth',
           'PetalLength', 'PetalWidth',
           'label']
FIELD_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0]]
def _parse_line(line):
    # Decode the line into its fields
    fields = tf.decode_csv(line, FIELD_DEFAULTS)

    # Pack the result into a dictionary
    features = dict(zip(COLUMNS,fields))

    # Separate the label from the features
    label = features.pop('label')

    return features, label

----------------------------------------

TITLE: Evaluating Model Performance
DESCRIPTION: Generates predictions on the validation set and prints a classification report to evaluate model performance.

LANGUAGE: Python
CODE:
y_pred = model.predict(validation_data)
y_pred = np.argmax(y_pred, axis=1)
y_true = np.array(labels[train_size:])
print(classification_report(y_true, y_pred, target_names=dir_names))

----------------------------------------

TITLE: Implementing Variable Updates in TensorFlow
DESCRIPTION: Shows how to create and modify TensorFlow Variables, which are used to store mutable state like model weights.

LANGUAGE: python
CODE:
var = tf.Variable([0.0, 0.0, 0.0])
var.assign([1, 2, 3])
var.assign_add([1, 1, 1])

----------------------------------------

TITLE: Implementing Metrics with TF1 Estimator in Python
DESCRIPTION: This code demonstrates how to use tf.metrics.accuracy with a TensorFlow 1.x Estimator. It defines input functions, a model function, and creates an Estimator with the accuracy metric.

LANGUAGE: Python
CODE:
def _input_fn():
  return tf1.data.Dataset.from_tensor_slices((features, labels)).batch(1)

def _eval_input_fn():
  return tf1.data.Dataset.from_tensor_slices(
      (eval_features, eval_labels)).batch(1)

def _model_fn(features, labels, mode):
  logits = tf1.layers.Dense(2)(features)
  predictions = tf.math.argmax(input=logits, axis=1)
  loss = tf1.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)
  optimizer = tf1.train.AdagradOptimizer(0.05)
  train_op = optimizer.minimize(loss, global_step=tf1.train.get_global_step())
  accuracy = tf1.metrics.accuracy(labels=labels, predictions=predictions)
  return tf1.estimator.EstimatorSpec(mode, 
                                     predictions=predictions,
                                     loss=loss, 
                                     train_op=train_op,
                                     eval_metric_ops={'accuracy': accuracy})

estimator = tf1.estimator.Estimator(model_fn=_model_fn)
estimator.train(_input_fn)

estimator.evaluate(_eval_input_fn)

----------------------------------------

TITLE: Loading Kinetics-400 Action Labels
DESCRIPTION: Fetches and loads the labels for the Kinetics-400 dataset from the GitHub repository.

LANGUAGE: Python
CODE:
KINETICS_URL = "https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt"
with request.urlopen(KINETICS_URL) as obj:
    labels = [line.decode("utf-8").strip() for line in obj.readlines()]
print("Found %d labels." % len(labels))

----------------------------------------

TITLE: Using TF Hub Text Embedding with Keras Layer
DESCRIPTION: Demonstrates loading and using a pre-trained text embedding model from TF Hub using hub.KerasLayer.

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow_hub as hub

hub_url = "https://tfhub.dev/google/nnlm-en-dim128/2"
embed = hub.KerasLayer(hub_url)
embeddings = embed(["A long sentence.", "single-word", "http://example.com"])
print(embeddings.shape, embeddings.dtype)

----------------------------------------

TITLE: Determining Tensor Rank in TensorFlow
DESCRIPTION: Shows how to use tf.rank to programmatically determine the rank of a tensor.

LANGUAGE: python
CODE:
r = tf.rank(my_image)
# After the graph runs, r will hold the value 4.

----------------------------------------

TITLE: Complete Maven Project Configuration Example
DESCRIPTION: Full Maven pom.xml configuration for a TensorFlow Java project, including compiler settings and main class specification.

LANGUAGE: xml
CODE:
<project>
  <modelVersion>4.0.0</modelVersion>
  <groupId>org.myorg</groupId>
  <artifactId>hellotensorflow</artifactId>
  <version>1.0-SNAPSHOT</version>
  <properties>
    <exec.mainClass>HelloTensorFlow</exec.mainClass>
	<!-- The sample code requires at least JDK 1.7. -->
	<!-- The maven compiler plugin defaults to a lower version -->
	<maven.compiler.source>1.7</maven.compiler.source>
	<maven.compiler.target>1.7</maven.compiler.target>
  </properties>
  <dependencies>
    <dependency>
	  <groupId>org.tensorflow</groupId>
	  <artifactId>tensorflow</artifactId>
	  <version>1.14.0</version>
	</dependency>
  </dependencies>
</project>

----------------------------------------

TITLE: Defining Apache License 2.0 for TensorFlow Tutorial in Python
DESCRIPTION: This code snippet defines the Apache License 2.0 for the TensorFlow tutorial. It specifies the terms under which the software is distributed and the conditions for its use, modification, and distribution.

LANGUAGE: python
CODE:
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

----------------------------------------

TITLE: Setting Up TPU Strategy for TensorFlow 2
DESCRIPTION: Initializes TPU system, creates TPUStrategy, and prepares datasets for TensorFlow 2 training.

LANGUAGE: python
CODE:
cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
print("All devices: ", tf.config.list_logical_devices('TPU'))

global_batch_size = 8

def _input_dataset(context: tf.distribute.InputContext):
  dataset = tf.data.Dataset.from_tensor_slices((
      {"dense_feature": features,
       "sparse_feature": tf.SparseTensor(
           embedding_features_indices,
           embedding_features_values, [1, 2])},
           labels))
  dataset = dataset.shuffle(10).repeat()
  dataset = dataset.batch(
      context.get_per_replica_batch_size(global_batch_size),
      drop_remainder=True)
  return dataset.prefetch(2)

def _eval_dataset(context: tf.distribute.InputContext):
  dataset = tf.data.Dataset.from_tensor_slices((
      {"dense_feature": eval_features,
       "sparse_feature": tf.SparseTensor(
           eval_embedding_features_indices,
           eval_embedding_features_values, [1, 2])},
           eval_labels))
  dataset = dataset.repeat()
  dataset = dataset.batch(
      context.get_per_replica_batch_size(global_batch_size),
      drop_remainder=True)
  return dataset.prefetch(2)

input_options = tf.distribute.InputOptions(
    experimental_fetch_to_device=False)

input_dataset = tf.keras.utils.experimental.DatasetCreator(
    _input_dataset, input_options=input_options)

eval_dataset = tf.keras.utils.experimental.DatasetCreator(
    _eval_dataset, input_options=input_options)

----------------------------------------

TITLE: Using Callbacks in Keras
DESCRIPTION: Shows how to use callbacks in Keras to customize and extend model behavior during training.

LANGUAGE: Python
CODE:
callbacks = [
  # Interrupt training if `val_loss` stops improving for over 2 epochs
  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),
  # Write TensorBoard logs to `./logs` directory
  tf.keras.callbacks.TensorBoard(log_dir='./logs')
]
model.fit(data, labels, batch_size=32, epochs=5, callbacks=callbacks,
          validation_data=(val_data, val_labels))

----------------------------------------

TITLE: TF2 Custom Early Stopping Callback
DESCRIPTION: Implementation of a custom Keras callback for time-based early stopping

LANGUAGE: Python
CODE:
class LimitTrainingTime(tf.keras.callbacks.Callback):
  def __init__(self, max_time_s):
    super().__init__()
    self.max_time_s = max_time_s
    self.start_time = None

  def on_train_begin(self, logs):
    self.start_time = time.time()

  def on_train_batch_end(self, batch, logs):
    now = time.time()
    if now - self.start_time >  self.max_time_s:
      self.model.stop_training = True

----------------------------------------

TITLE: Creating TensorFlow Datasets
DESCRIPTION: Creates TensorFlow Datasets for training and validation data using the prepared file paths and labels.

LANGUAGE: Python
CODE:
def load_file(path, label):
    return tf.io.read_file(path), label

def make_datasets(train_size):
  batch_size = 256

  train_files = file_paths[:train_size]
  train_labels = labels[:train_size]
  train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))
  train_ds = train_ds.map(load_file).shuffle(5000)
  train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)

  test_files = file_paths[train_size:]
  test_labels = labels[train_size:]
  test_ds = tf.data.Dataset.from_tensor_slices((test_files, test_labels))
  test_ds = test_ds.map(load_file)
  test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)


  return train_ds, test_ds

train_data, validation_data = make_datasets(train_size)

----------------------------------------

TITLE: Saving Model Weights
DESCRIPTION: Demonstrates how to save the model weights using tf.keras.Model.save_weights method.

LANGUAGE: Python
CODE:
net.save_weights('easy_checkpoint')

----------------------------------------

TITLE: Working with Sparse Tensors in TensorFlow
DESCRIPTION: Demonstrates how to create and work with sparse tensors using tf.sparse.SparseTensor.

LANGUAGE: Python
CODE:
# Sparse tensors store values by index in a memory-efficient manner
sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                       values=[1, 2],
                                       dense_shape=[3, 4])
print(sparse_tensor, "\n")

# You can convert sparse tensors to dense
print(tf.sparse.to_dense(sparse_tensor))

----------------------------------------

TITLE: Iterating Through Graph Nodes
DESCRIPTION: Loops through all nodes in the graph definition. Each node represents an operation in the TensorFlow graph.

LANGUAGE: python
CODE:
for node in graph_def.node

----------------------------------------

TITLE: Importing Required Modules for Action Recognition
DESCRIPTION: Imports necessary Python modules including TensorFlow, TensorFlow Hub, and various utility libraries for video processing and visualization.

LANGUAGE: Python
CODE:
from absl import logging

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow_docs.vis import embed

logging.set_verbosity(logging.ERROR)

import random
import re
import os
import tempfile
import ssl
import cv2
import numpy as np

import imageio
from IPython import display

from urllib import request

----------------------------------------

TITLE: Implementing LoggingTensorHook and StopAtStepHook in TensorFlow 1
DESCRIPTION: This code demonstrates how to use tf.estimator.LoggingTensorHook and tf.estimator.StopAtStepHook in TensorFlow 1 to log tensors and stop training at a specific step. It defines a model function and creates an Estimator.

LANGUAGE: Python
CODE:
def _model_fn(features, labels, mode):
  dense = tf1.layers.Dense(1)
  logits = dense(features)
  loss = tf1.losses.mean_squared_error(labels=labels, predictions=logits)
  optimizer = tf1.train.AdagradOptimizer(0.05)
  train_op = optimizer.minimize(loss, global_step=tf1.train.get_global_step())

  # Define the stop hook.
  stop_hook = tf1.train.StopAtStepHook(num_steps=2)

  # Access tensors to be logged by names.
  kernel_name = tf.identity(dense.weights[0])
  bias_name = tf.identity(dense.weights[1])
  logging_weight_hook = tf1.train.LoggingTensorHook(
      tensors=[kernel_name, bias_name],
      every_n_iter=1)
  # Log the training loss by the tensor object.
  logging_loss_hook = tf1.train.LoggingTensorHook(
      {'loss from LoggingTensorHook': loss},
      every_n_secs=3)

  # Pass all hooks to `EstimatorSpec`.
  return tf1.estimator.EstimatorSpec(mode,
                                     loss=loss,
                                     train_op=train_op,
                                     training_hooks=[stop_hook,
                                                     logging_weight_hook,
                                                     logging_loss_hook])

estimator = tf1.estimator.Estimator(model_fn=_model_fn)

# Begin training.
# The training will stop after 2 steps, and the weights/loss will also be logged.
estimator.train(_input_fn)

----------------------------------------

TITLE: Importing TensorFlow and Required Libraries
DESCRIPTION: This code snippet imports the necessary Python libraries including TensorFlow, NumPy, Matplotlib, and Seaborn for audio processing and visualization.

LANGUAGE: Python
CODE:
import os
import pathlib

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import models
from IPython import display

# Set the seed value for experiment reproducibility.
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

----------------------------------------

TITLE: Pre-commit Hook Configuration
DESCRIPTION: YAML configuration for setting up pre-commit hooks to automatically run nbfmt.

LANGUAGE: yaml
CODE:
repos:
- repo: https://github.com/tensorflow/docs
  rev: pre-commit

----------------------------------------

TITLE: Training and Evaluating the Model
DESCRIPTION: Compiling and training the model using binary crossentropy loss and Adam optimizer, then evaluating on test data

LANGUAGE: python
CODE:
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['acc'])

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=40,
                    batch_size=512,
                    validation_data=(x_val, y_val),
                    verbose=1)

----------------------------------------

TITLE: Installing TensorFlow Docs Package
DESCRIPTION: Command to install the tensorflow-docs package directly from GitHub repository using pip.

LANGUAGE: shell
CODE:
$ python3 -m pip install -U --user git+https://github.com/tensorflow/docs

----------------------------------------

TITLE: Downloading and preparing flower dataset
DESCRIPTION: Functions to download flower images and split them into training and test sets. It sets up the dataset structure for the classification task.

LANGUAGE: Python
CODE:
FLOWERS_DIR = './flower_photos'
TRAIN_FRACTION = 0.8
RANDOM_SEED = 2018

def download_images():
    """If the images aren't already downloaded, save them to FLOWERS_DIR."""
    if not os.path.exists(FLOWERS_DIR):
        DOWNLOAD_URL = 'http://download.tensorflow.org/example_images/flower_photos.tgz'
        print('Downloading flower images from %s...' % DOWNLOAD_URL)
        urllib.request.urlretrieve(DOWNLOAD_URL, 'flower_photos.tgz')
        !tar xfz flower_photos.tgz
    print('Flower photos are located in %s' % FLOWERS_DIR)

def make_train_and_test_sets():
    """Split the data into train and test sets and get the label classes."""
    train_examples, test_examples = [], []
    shuffler = random.Random(RANDOM_SEED)
    is_root = True
    for (dirname, subdirs, filenames) in tf.gfile.Walk(FLOWERS_DIR):
        # The root directory gives us the classes
        if is_root:
            subdirs = sorted(subdirs)
            classes = collections.OrderedDict(enumerate(subdirs))
            label_to_class = dict([(x, i) for i, x in enumerate(subdirs)])
            is_root = False
        # The sub directories give us the image files for training.
        else:
            filenames.sort()
            shuffler.shuffle(filenames)
            full_filenames = [os.path.join(dirname, f) for f in filenames]
            label = dirname.split('/')[-1]
            label_class = label_to_class[label]
            # An example is the image file and it's label class.
            examples = list(zip(full_filenames, [label_class] * len(filenames)))
            num_train = int(len(filenames) * TRAIN_FRACTION)
            train_examples.extend(examples[:num_train])
            test_examples.extend(examples[num_train:])

    shuffler.shuffle(train_examples)
    shuffler.shuffle(test_examples)
    return train_examples, test_examples, classes

----------------------------------------

TITLE: Converting TF2 Checkpoint to TF1 Format
DESCRIPTION: Function to convert a TensorFlow 2.x checkpoint to TensorFlow 1.x format by loading variables and resaving them using Saver

LANGUAGE: Python
CODE:
def convert_tf2_to_tf1(checkpoint_path, output_prefix):
  vars = {}
  reader = tf.train.load_checkpoint(checkpoint_path)
  dtypes = reader.get_variable_to_dtype_map()
  for key in dtypes.keys():
    if key.startswith('var_list/'):
      var_name = key.split('/')[1]
      var_name = var_name.replace('.S', '/')
      vars[var_name] = tf.Variable(reader.get_tensor(key))
  return tf1.train.Saver(var_list=vars).save(sess=None, save_path=output_prefix)

----------------------------------------

TITLE: Preparing Dataset for Training
DESCRIPTION: Prepares the BARD dataset for training by shuffling and splitting into train and validation sets.

LANGUAGE: Python
CODE:
dir_names = ['economy', 'sports', 'entertainment', 'state', 'international']

file_paths = []
labels = []
for i, dir in enumerate(dir_names):
  file_names = ["/".join([dir, name]) for name in os.listdir(dir)]
  file_paths += file_names
  labels += [i] * len(os.listdir(dir))
  
np.random.seed(42)
permutation = np.random.permutation(len(file_paths))

file_paths = np.array(file_paths)[permutation]
labels = np.array(labels)[permutation]

----------------------------------------

TITLE: Creating Crossed Feature Columns in TensorFlow
DESCRIPTION: Shows how to combine multiple features into crossed columns using bucketing and hashing, commonly used for location-based features.

LANGUAGE: python
CODE:
def make_dataset(latitude, longitude, labels):
    assert latitude.shape == longitude.shape == labels.shape

    features = {'latitude': latitude.flatten(),
                'longitude': longitude.flatten()}
    labels=labels.flatten()

    return tf.data.Dataset.from_tensor_slices((features, labels))


# Bucketize the latitude and longitude using the `edges`
latitude_bucket_fc = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column('latitude'),
    list(atlanta.latitude.edges))

longitude_bucket_fc = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column('longitude'),
    list(atlanta.longitude.edges))

# Cross the bucketized columns, using 5000 hash bins.
crossed_lat_lon_fc = tf.feature_column.crossed_column(
    [latitude_bucket_fc, longitude_bucket_fc], 5000)

fc = [
    latitude_bucket_fc,
    longitude_bucket_fc,
    crossed_lat_lon_fc]

# Build and train the Estimator.
est = tf.estimator.LinearRegressor(fc, ...)

----------------------------------------

TITLE: Creating Non-Trainable TensorFlow Variables
DESCRIPTION: This code demonstrates how to create a non-trainable TensorFlow variable, which is useful for values that should not be updated during training, such as step counters.

LANGUAGE: Python
CODE:
step_counter = tf.Variable(1, trainable=False)

----------------------------------------

TITLE: TensorFlow Python Operation Implementation Example
DESCRIPTION: Example implementation of a TensorFlow operation following the project's conventions, including proper documentation, tensor conversion, and name scoping.

LANGUAGE: python
CODE:
def my_op(tensor_in, other_tensor_in, my_param, other_param=0.5,
          output_collections=(), name=None):
  """My operation that adds two tensors with given coefficients.

  Args:
    tensor_in: `Tensor`, input tensor.
    other_tensor_in: `Tensor`, same shape as `tensor_in`, other input tensor.
    my_param: `float`, coefficient for `tensor_in`.
    other_param: `float`, coefficient for `other_tensor_in`.
    output_collections: `tuple` of `string`s, name of the collection to
                        collect result of this op.
    name: `string`, name of the operation.

  Returns:
    `Tensor` of same shape as `tensor_in`, sum of input values with coefficients.

  Example:
    >>> my_op([1., 2.], [3., 4.], my_param=0.5, other_param=0.6,
              output_collections=['MY_OPS'], name='add_t1t2')
    [2.3, 3.4]
  """
  with tf.name_scope(name or "my_op"):
    tensor_in = tf.convert_to_tensor(tensor_in)
    other_tensor_in = tf.convert_to_tensor(other_tensor_in)
    result = my_param * tensor_in + other_param * other_tensor_in
    tf.add_to_collection(output_collections, result)
    return result

----------------------------------------

TITLE: TensorFlow and Dependencies Setup
DESCRIPTION: Imports required libraries and configures TensorFlow v1 compatibility mode

LANGUAGE: Python
CODE:
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

import os
import io
import IPython.display
import numpy as np
import PIL.Image
from scipy.stats import truncnorm
import tensorflow_hub as hub

----------------------------------------

TITLE: Exporting Text Classification Model
DESCRIPTION: Exports the trained model with preprocessing layers for deployment, allowing it to handle raw text input.

LANGUAGE: Python
CODE:
export_model = tf.keras.Sequential([
    preprocess_text,
    model
])

export_model.compile(
    loss=losses.SparseCategoricalCrossentropy(from_logits=False),
    optimizer='adam',
    metrics=['accuracy'])

tf.saved_model.save(export_model, 'export.tf')

----------------------------------------

TITLE: Importing TensorFlow and NumPy
DESCRIPTION: Import the TensorFlow and NumPy libraries for use in the notebook.

LANGUAGE: Python
CODE:
import tensorflow as tf
import numpy as np

----------------------------------------

TITLE: Configuring Custom Checkpoint Settings
DESCRIPTION: Shows how to customize checkpoint frequency and retention using RunConfig.

LANGUAGE: python
CODE:
my_checkpointing_config = tf.estimator.RunConfig(
    save_checkpoints_secs = 20*60,  # Save checkpoints every 20 minutes.
    keep_checkpoint_max = 10,       # Retain the 10 most recent checkpoints.
)

classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    hidden_units=[10, 10],
    n_classes=3,
    model_dir='models/iris',
    config=my_checkpointing_config)

----------------------------------------

TITLE: Loading and Preprocessing MNIST Dataset
DESCRIPTION: Loading the MNIST dataset and preprocessing it by reshaping and normalizing the first 1000 samples

LANGUAGE: Python
CODE:
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_labels = train_labels[:1000]
test_labels = test_labels[:1000]

train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0

----------------------------------------

TITLE: Defining Sentiment Analysis Model Class
DESCRIPTION: Creates a custom TensorFlow model class that uses a TF-Hub text embedding module and dense layers for sentiment classification

LANGUAGE: python
CODE:
class MyModel(tf.keras.Model):
  def __init__(self, hub_url):
    super().__init__()
    self.hub_url = hub_url
    self.embed = hub.load(self.hub_url).signatures['default']
    self.sequential = tf.keras.Sequential([
      tf.keras.layers.Dense(500),
      tf.keras.layers.Dense(100),
      tf.keras.layers.Dense(5),
    ])

  def call(self, inputs):
    phrases = inputs['Phrase'][:,0]
    embedding = 5*self.embed(phrases)['default']
    return self.sequential(embedding)

  def get_config(self):
    return {"hub_url":self.hub_url}

----------------------------------------

TITLE: Training the Model
DESCRIPTION: Trains the Keras model on the prepared datasets for 5 epochs with early stopping.

LANGUAGE: Python
CODE:
history = model.fit(train_data, 
                    validation_data=validation_data, 
                    epochs=5, 
                    callbacks=[early_stopping_callback])

----------------------------------------

TITLE: Registering a TensorFlow Gradient in Python
DESCRIPTION: Demonstrates how to register a gradient function for the 'ZeroOut' op to enable automatic differentiation.

LANGUAGE: Python
CODE:
from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import sparse_ops

@ops.RegisterGradient("ZeroOut")
def _zero_out_grad(op, grad):
  """The gradients for `zero_out`.

  Args:
    op: The `zero_out` `Operation` that we are differentiating, which we can use
      to find the inputs and outputs of the original op.
    grad: Gradient with respect to the output of the `zero_out` op.

  Returns:
    Gradients with respect to the input of `zero_out`.
  """
  to_zero = op.inputs[0]
  shape = array_ops.shape(to_zero)
  index = array_ops.zeros_like(shape)
  first_grad = array_ops.reshape(grad, [-1])[0]
  to_zero_grad = sparse_ops.sparse_to_dense([index], shape, first_grad, 0)
  return [to_zero_grad]  # List of one Tensor, since we have one input

----------------------------------------

TITLE: Building and Training Citation Intent Classifier with CORD-19 Embeddings
DESCRIPTION: This code creates a Keras model using the CORD-19 embeddings and a dense layer for classification, then compiles and trains the model on the SciCite dataset.

LANGUAGE: Python
CODE:
EMBEDDING = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'
TRAINABLE_MODULE = False

hub_layer = hub.KerasLayer(EMBEDDING, input_shape=[], 
                           dtype=tf.string, trainable=TRAINABLE_MODULE)

model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(3))
model.summary()
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

EPOCHS = 35
BATCH_SIZE = 32

history = model.fit(train_data.shuffle(10000).batch(BATCH_SIZE),
                    epochs=EPOCHS,
                    validation_data=validation_data.batch(BATCH_SIZE),
                    verbose=1)

----------------------------------------

TITLE: Visualizing Predictions of Citation Intent Classifier
DESCRIPTION: This code generates predictions for a batch of test data and displays them alongside the true labels in a pandas DataFrame for easy comparison.

LANGUAGE: Python
CODE:
prediction_dataset = next(iter(test_data.batch(20)))

prediction_texts = [ex.numpy().decode('utf8') for ex in prediction_dataset[0]]
prediction_labels = [label2str(x) for x in prediction_dataset[1]]

predictions = [
    label2str(x) for x in np.argmax(model.predict(prediction_texts), axis=-1)]

pd.DataFrame({
    TEXT_FEATURE_NAME: prediction_texts,
    LABEL_NAME: prediction_labels,
    'prediction': predictions
})

----------------------------------------

TITLE: Importing Required Libraries
DESCRIPTION: Import necessary TensorFlow packages and other dependencies

LANGUAGE: python
CODE:
import pandas as pd
import tensorflow as tf
import tensorflow.compat.v1 as tf1
import tensorflow_decision_forests as tfdf
from tensorflow import keras

----------------------------------------

TITLE: Importing Required Libraries
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Hub, SentencePiece, and visualization libraries

LANGUAGE: python
CODE:
from absl import logging

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

import tensorflow_hub as hub
import sentencepiece as spm
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns

----------------------------------------

TITLE: Adjusting Batch Size and Learning Rate for Distributed Training in TensorFlow
DESCRIPTION: Shows how to adjust the batch size and learning rate based on the number of replicas in the distributed strategy. This ensures effective use of multiple GPUs or workers.

LANGUAGE: Python
CODE:
BATCH_SIZE_PER_REPLICA = 5
global_batch_size = (BATCH_SIZE_PER_REPLICA *
                     mirrored_strategy.num_replicas_in_sync)
dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100)
dataset = dataset.batch(global_batch_size)

LEARNING_RATES_BY_BATCH_SIZE = {5: 0.1, 10: 0.15}
learning_rate = LEARNING_RATES_BY_BATCH_SIZE[global_batch_size]

----------------------------------------

TITLE: Setting up Dependencies for DCGAN
DESCRIPTION: Imports required TensorFlow, data processing, and visualization libraries needed for implementing the DCGAN.

LANGUAGE: Python
CODE:
import tensorflow as tf
import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
from tensorflow.keras import layers
import time
from IPython import display

----------------------------------------

TITLE: Basic TensorFlow Java Example Program
DESCRIPTION: Example Java program demonstrating basic TensorFlow usage, including graph creation and tensor operations.

LANGUAGE: java
CODE:
import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import org.tensorflow.TensorFlow;

public class HelloTensorFlow {
  public static void main(String[] args) throws Exception {
	try (Graph g = new Graph()) {
	  final String value = "Hello from " + TensorFlow.version();

	  // Construct the computation graph with a single operation, a constant
	  // named "MyConst" with a value "value".
	  try (Tensor t = Tensor.create(value.getBytes("UTF-8"))) {
	    // The Java API doesn't yet include convenience functions for adding operations.
		g.opBuilder("Const", "MyConst").setAttr("dtype", t.dataType()).setAttr("value", t).build();
	  }

	  // Execute the "MyConst" operation in a Session.
	  try (Session s = new Session(g);
	      // Generally, there may be multiple output tensors,
		  // all of them must be closed to prevent resource leaks.
		  Tensor output = s.runner().fetch("MyConst").run().get(0)) {
	    System.out.println(new String(output.bytesValue(), "UTF-8"));
	  }
    }
  }
}

----------------------------------------

TITLE: Defining TensorFlow Variables for Mandelbrot Set Computation
DESCRIPTION: This code defines and initializes TensorFlow tensors for the Mandelbrot set computation. It creates constants and variables for complex numbers and iteration counts.

LANGUAGE: Python
CODE:
xs = tf.constant(Z.astype(np.complex64))
zs = tf.Variable(xs)
ns = tf.Variable(tf.zeros_like(xs, tf.float32))

tf.global_variables_initializer().run()

----------------------------------------

TITLE: Preprocessing the Image Data
DESCRIPTION: This snippet preprocesses the image data by scaling the pixel values to a range of 0 to 1.

LANGUAGE: Python
CODE:
train_images = train_images / 255.0

test_images = test_images / 255.0

----------------------------------------

TITLE: Downloading FastText Embeddings and Exporter Script
DESCRIPTION: Downloads FastText word embeddings for Bangla and the TensorFlow Hub embedding exporter script.

LANGUAGE: Bash
CODE:
curl -O https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.bn.300.vec.gz
curl -O https://raw.githubusercontent.com/tensorflow/hub/master/examples/text_embeddings_v2/export_v2.py
gunzip -qf cc.bn.300.vec.gz --k

----------------------------------------

TITLE: Implementing Model Function for Citation Intent Classification
DESCRIPTION: Defines a model function that uses the CORD-19 embeddings with a classification layer on top. This function is used to create a TensorFlow Estimator for the citation intent classification task.

LANGUAGE: python
CODE:
def model_fn(features, labels, mode, params):
  embed = hub.Module(params['module_name'], trainable=params['trainable_module'])
  embeddings = embed(features['feature'])

  logits = tf.layers.dense(
      embeddings, units=THE_DATASET.num_classes(), activation=None)
  predictions = tf.argmax(input=logits, axis=1)

  if mode == tf.estimator.ModeKeys.PREDICT:
    return tf.estimator.EstimatorSpec(
        mode=mode,
        predictions={
            'logits': logits,
            'predictions': predictions,
            'features': features['feature'],
            'labels': features['label']
        })
  
  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
      labels=labels, logits=logits)
  loss = tf.reduce_mean(loss)

  if mode == tf.estimator.ModeKeys.TRAIN:
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=params['learning_rate'])
    train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())
    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

  elif mode == tf.estimator.ModeKeys.EVAL:
    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions)
    precision = tf.metrics.precision(labels=labels, predictions=predictions)
    recall = tf.metrics.recall(labels=labels, predictions=predictions)

    return tf.estimator.EstimatorSpec(
        mode=mode,
        loss=loss,
        eval_metric_ops={
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
        })

----------------------------------------

TITLE: Initializing Essential Libraries for TensorFlow Data Augmentation
DESCRIPTION: Imports required libraries including TensorFlow, matplotlib, numpy and tensorflow_datasets for working with image data augmentation

LANGUAGE: Python
CODE:
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow.keras import layers

----------------------------------------

TITLE: Creating Optimizer Context Manager
DESCRIPTION: Implements a context manager to temporarily modify TensorFlow optimizer settings and restore them afterward.

LANGUAGE: python
CODE:
@contextlib.contextmanager
def options(options):
  old_opts = tf.config.optimizer.get_experimental_options()
  tf.config.optimizer.set_experimental_options(options)
  try:
    yield
  finally:
    tf.config.optimizer.set_experimental_options(old_opts)

----------------------------------------

TITLE: Performing Inference with Fine-tuned Wav2Vec2 Model
DESCRIPTION: Loads the saved fine-tuned Wav2Vec2 model and performs inference on a new audio sample.

LANGUAGE: Python
CODE:
finetuned_model = tf.keras.models.load_model(save_dir)

speech, _ = sf.read("SA2.wav")
speech = np.pad(speech, (0, AUDIO_MAXLEN - len(speech)))
speech = tf.expand_dims(processor(tf.constant(speech)), 0)

outputs = finetuned_model(speech)
predictions = tf.argmax(outputs, axis=-1)
predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]
predictions

----------------------------------------

TITLE: Printing Tensor Values in TensorFlow
DESCRIPTION: Shows how to properly print tensor values using tf.Print for debugging purposes.

LANGUAGE: python
CODE:
t = <<some tensorflow operation>>
tf.Print(t, [t])  # This does nothing
t = tf.Print(t, [t])  # Here we are using the value returned by tf.Print
result = t + 1  # Now when result is evaluated the value of `t` will be printed.

----------------------------------------

TITLE: Defining Input Functions and Model Function for TPUEstimator
DESCRIPTION: Creates input functions for training and evaluation data, and defines a model function for TPUEstimator.

LANGUAGE: python
CODE:
def _input_fn(params):
  dataset = tf1.data.Dataset.from_tensor_slices((
      {"dense_feature": features,
       "sparse_feature": tf1.SparseTensor(
           embedding_features_indices,
           embedding_features_values, [1, 2])},
           labels))
  dataset = dataset.repeat()
  return dataset.batch(params['batch_size'], drop_remainder=True)

def _eval_input_fn(params):
  dataset = tf1.data.Dataset.from_tensor_slices((
      {"dense_feature": eval_features,
       "sparse_feature": tf1.SparseTensor(
           eval_embedding_features_indices,
           eval_embedding_features_values, [1, 2])},
           eval_labels))
  dataset = dataset.repeat()
  return dataset.batch(params['batch_size'], drop_remainder=True)

def _model_fn(features, labels, mode, params):
  embedding_features = tf1.keras.layers.DenseFeatures(embedding_column)(features)
  concatenated_features = tf1.keras.layers.Concatenate(axis=1)(
      [embedding_features, features["dense_feature"]])
  logits = tf1.layers.Dense(1)(concatenated_features)
  loss = tf1.losses.mean_squared_error(labels=labels, predictions=logits)
  optimizer = tf1.train.AdagradOptimizer(0.05)
  optimizer = tf1.tpu.CrossShardOptimizer(optimizer)
  train_op = optimizer.minimize(loss, global_step=tf1.train.get_global_step())
  return tf1.estimator.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op)

----------------------------------------

TITLE: Running Mandelbrot Set Computation and Displaying Results in Python
DESCRIPTION: This code runs the Mandelbrot set computation for 200 iterations and then displays the resulting fractal image using the previously defined DisplayFractal function.

LANGUAGE: Python
CODE:
for i in range(200): step.run()

DisplayFractal(ns.eval())

----------------------------------------

TITLE: TensorFlow NumPy Setup and Imports
DESCRIPTION: Imports required libraries and enables NumPy behavior for TensorFlow

LANGUAGE: Python
CODE:
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow.experimental.numpy as tnp
import timeit

print("Using TensorFlow version %s" % tf.__version__)

tnp.experimental_enable_numpy_behavior()

----------------------------------------

TITLE: Implementing Batch Normalization with tf.compat.v1.layers
DESCRIPTION: Creates a Keras layer with batch normalization using tf.compat.v1.layers.batch_normalization.

LANGUAGE: Python
CODE:
class CompatV1BatchNorm(tf.keras.layers.Layer):

  @tf.compat.v1.keras.utils.track_tf1_style_variables
  def call(self, inputs, training=None):
    print("Forward pass called with `training` =", training)
    with v1.variable_scope('batch_norm_layer'):
      return v1.layers.batch_normalization(x, training=training)

----------------------------------------

TITLE: Launching TensorBoard for Training Visualization
DESCRIPTION: Command to launch TensorBoard for visualizing training progress and metrics.

LANGUAGE: Shell
CODE:
tensorboard --logdir /tmp/cifar10_train

----------------------------------------

TITLE: Importing TensorFlow and Setting Up Dataset in Python
DESCRIPTION: This snippet imports TensorFlow, sets up compatibility with TensorFlow 1.x, and creates a simple dataset for demonstration purposes.

LANGUAGE: Python
CODE:
import tensorflow as tf
import tensorflow.compat.v1 as tf1

features = [[1., 1.5], [2., 2.5], [3., 3.5]]
labels = [[0.3], [0.5], [0.7]]

# Define an input function.
def _input_fn():
  return tf1.data.Dataset.from_tensor_slices((features, labels)).batch(1)

----------------------------------------

TITLE: Downloading and Processing Bird Vocalization Audio Sample
DESCRIPTION: Downloads a Common Blackbird audio sample and prepares it for model input.

LANGUAGE: bash
CODE:
!curl -O  "https://upload.wikimedia.org/wikipedia/commons/7/7c/Turdus_merula_2.ogg"

LANGUAGE: python
CODE:
turdus_merula = "Turdus_merula_2.ogg"

audio, sample_rate = librosa.load(turdus_merula)

sample_rate, wav_data_turdus = ensure_sample_rate(audio, sample_rate)
Audio(wav_data_turdus, rate=sample_rate)

----------------------------------------

TITLE: Configuring SGD Optimizer in TensorFlow
DESCRIPTION: Sets up the stochastic gradient descent optimizer with a learning rate of 1.0 to minimize the loss function.

LANGUAGE: Python
CODE:
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)

----------------------------------------

TITLE: Model Loading and Prediction
DESCRIPTION: Loads the pre-trained CropNet classifier from TensorFlow Hub and makes predictions on sample images

LANGUAGE: python
CODE:
classifier = hub.KerasLayer('https://tfhub.dev/google/cropnet/classifier/cassava_disease_V1/2')
probabilities = classifier(examples['image'])
predictions = tf.argmax(probabilities, axis=-1)

----------------------------------------

TITLE: Training the Model
DESCRIPTION: Demonstrates how to train the model using the prepared dataset.

LANGUAGE: Python
CODE:
steps_per_epoch=tf.ceil(len(all_image_paths)/BATCH_SIZE).numpy()
model.fit(ds, epochs=1, steps_per_epoch=3)

----------------------------------------

TITLE: Creating Rank 1 Tensors in TensorFlow
DESCRIPTION: Shows how to create vector (rank 1) tensors of different data types using tf.Variable and lists.

LANGUAGE: python
CODE:
mystr = tf.Variable(["Hello"], tf.string)
cool_numbers  = tf.Variable([3.14159, 2.71828], tf.float32)
first_primes = tf.Variable([2, 3, 5, 7, 11], tf.int32)
its_very_complicated = tf.Variable([12.3 - 4.85j, 7.5 - 6.23j], tf.complex64)

----------------------------------------

TITLE: Evaluating Tensors in TensorFlow
DESCRIPTION: Demonstrates how to evaluate tensors using the eval() method and handle placeholders with feed_dict.

LANGUAGE: python
CODE:
constant = tf.constant([1, 2, 3])
tensor = constant * constant
print(tensor.eval())

p = tf.placeholder(tf.float32)
t = p + 1.0
t.eval()  # This will fail, since the placeholder did not get a value.
t.eval(feed_dict={p:2.0})  # This will succeed because we're feeding a value
                           # to the placeholder.

----------------------------------------

TITLE: Creating Ragged Tensors with tf.ragged.constant
DESCRIPTION: Demonstrates how to create ragged tensors using tf.ragged.constant with nested Python lists.

LANGUAGE: python
CODE:
digits = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])
words = tf.ragged.constant([["So", "long"], ["thanks", "for", "all", "the", "fish"]])
print(tf.add(digits, 3))
print(tf.reduce_mean(digits, axis=1))
print(tf.concat([digits, [[5, 3]]], axis=0))
print(tf.tile(digits, [1, 2]))
print(tf.strings.substr(words, 0, 2))
print(tf.map_fn(tf.math.square, digits))

----------------------------------------

TITLE: Importing Dependencies for SNGP Implementation
DESCRIPTION: Imports necessary libraries including TensorFlow, matplotlib, and scikit-learn for implementing SNGP and visualizing results.

LANGUAGE: Python
CODE:
import matplotlib.pyplot as plt
import matplotlib.colors as colors

import sklearn.datasets

import numpy as np
import tensorflow as tf

import official.nlp.modeling.layers as nlp_layers

----------------------------------------

TITLE: Executing YAMNet Model for Sound Classification
DESCRIPTION: This code runs the YAMNet model on the preprocessed audio data to obtain scores, embeddings, and spectrogram. It then identifies the main sound class.

LANGUAGE: python
CODE:
# Run the model, check the output.
scores, embeddings, spectrogram = model(waveform)

scores_np = scores.numpy()
spectrogram_np = spectrogram.numpy()
infered_class = class_names[scores_np.mean(axis=0).argmax()]
print(f'The main sound is: {infered_class}')

----------------------------------------

TITLE: Documentation Location Reference in Markdown
DESCRIPTION: File path references for TensorFlow documentation contributors and translators showing where to find and update source files.

LANGUAGE: markdown
CODE:
site/en/
site/<lang>/README.md

----------------------------------------

TITLE: Implementing Data Augmentation Layer
DESCRIPTION: Creates a sequential model that applies random flips and rotations to input images for data augmentation

LANGUAGE: Python
CODE:
data_augmentation = tf.keras.Sequential([
  layers.RandomFlip("horizontal_and_vertical"),
  layers.RandomRotation(0.2),
])

----------------------------------------

TITLE: Implementing a TensorFlow Op Kernel in C++
DESCRIPTION: Shows how to implement the kernel for the 'ZeroOut' op, which performs the actual computation.

LANGUAGE: C++
CODE:
#include "tensorflow/core/framework/op_kernel.h"

using namespace tensorflow;

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output_flat = output_tensor->flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output_flat(i) = 0;
    }

    // Preserve the first input value if possible.
    if (N > 0) output_flat(0) = input(0);
  }
};

REGISTER_KERNEL_BUILDER(Name("ZeroOut").Device(DEVICE_CPU), ZeroOutOp);

----------------------------------------

TITLE: Framing Audio for Bird Vocalization Model Input
DESCRIPTION: Splits the audio into 5-second frames as required by the model.

LANGUAGE: python
CODE:
fixed_tm = frame_audio(wav_data_turdus)
fixed_tm.shape

----------------------------------------

TITLE: Defining Feature Columns for Titanic Dataset
DESCRIPTION: Creates feature columns that specify how to use different features from the dataset, including numeric and categorical columns

LANGUAGE: Python
CODE:
age = tf.feature_column.numeric_column('age')
cls = tf.feature_column.categorical_column_with_vocabulary_list('class', ['First', 'Second', 'Third']) 
embark = tf.feature_column.categorical_column_with_hash_bucket('embark_town', 32)

----------------------------------------

TITLE: Loading Universal Sentence Encoder in TensorFlow
DESCRIPTION: This code loads the Universal Sentence Encoder module from TensorFlow Hub and defines an embed function for generating embeddings. It requires TensorFlow, TensorFlow Hub, and other data processing libraries.

LANGUAGE: Python
CODE:
from absl import logging

import tensorflow as tf

import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns

module_url = "https://tfhub.dev/google/universal-sentence-encoder/4" #@param ["https://tfhub.dev/google/universal-sentence-encoder/4", "https://tfhub.dev/google/universal-sentence-encoder-large/5"]
model = hub.load(module_url)
print ("module %s loaded" % module_url)
def embed(input):
  return model(input)

----------------------------------------

TITLE: Using tf.Session.run to Execute Operations in TensorFlow
DESCRIPTION: Demonstrates how to use tf.Session.run to execute operations and evaluate tensors, including feeding values and collecting metadata.

LANGUAGE: python
CODE:
x = tf.constant([[37.0, -23.0], [1.0, 4.0]])
w = tf.Variable(tf.random_uniform([2, 2]))
y = tf.matmul(x, w)
output = tf.nn.softmax(y)
init_op = w.initializer

with tf.Session() as sess:
  # Run the initializer on `w`.
  sess.run(init_op)

  # Evaluate `output`. `sess.run(output)` will return a NumPy array containing
  # the result of the computation.
  print(sess.run(output))

  # Evaluate `y` and `output`. Note that `y` will only be computed once, and its
  # result used both to return `y_val` and as an input to the `tf.nn.softmax()`
  # op. Both `y_val` and `output_val` will be NumPy arrays.
  y_val, output_val = sess.run([y, output])

----------------------------------------

TITLE: Setting Up TPUEstimator for Training and Evaluation
DESCRIPTION: Create a TPUClusterResolver, RunConfig, and TPUEstimator for training and evaluation on TPUs in TensorFlow 1.

LANGUAGE: Python
CODE:
cluster_resolver = tf1.distribute.cluster_resolver.TPUClusterResolver(tpu='')
print("All devices: ", tf1.config.list_logical_devices('TPU'))

tpu_config = tf1.estimator.tpu.TPUConfig(iterations_per_loop=10)
config = tf1.estimator.tpu.RunConfig(
    cluster=cluster_resolver,
    save_checkpoints_steps=None,
    tpu_config=tpu_config)
estimator = tf1.estimator.tpu.TPUEstimator(
    model_fn=_model_fn,
    config=config,
    train_batch_size=8,
    eval_batch_size=8)

estimator.train(_input_fn, steps=1)
estimator.evaluate(_eval_input_fn, steps=1)

----------------------------------------

TITLE: Comparing Numerical Equivalence
DESCRIPTION: Compares the numerical equivalence of learning rates, losses, and layer outputs between TF1 and TF2 models.

LANGUAGE: Python
CODE:
np.testing.assert_allclose(model_tf1.logs['lr'], model_tf2.logs['lr'])
np.testing.assert_allclose(model_tf1.logs['loss'], model_tf2.logs['loss'])
for step in range(step_num):
  for name in model_tf1.logs['layer_out'][step]:
    np.testing.assert_allclose(
        model_tf1.logs['layer_out'][step][name],
        model_tf2.logs['layer_out'][step][name])

----------------------------------------

TITLE: Parallelizing Data Extraction with Interleave
DESCRIPTION: Shows how to use the interleave transformation to parallelize data loading from multiple sources.

LANGUAGE: Python
CODE:
benchmark(
    tf.data.Dataset.range(2)
    .interleave(
        lambda _: ArtificialDataset(),
        num_parallel_calls=tf.data.AUTOTUNE
    )
)

----------------------------------------

TITLE: Defining Custom Dense Layer
DESCRIPTION: Creates a custom dense layer class that can be subclassed for implementing Entropy Penalized Reparameterization (EPR).

LANGUAGE: python
CODE:
class CustomDense(tf.keras.layers.Layer):

  def __init__(self, filters, name="dense"):
    super().__init__(name=name)
    self.filters = filters

  @classmethod
  def copy(cls, other, **kwargs):
    """Returns an instantiated and built layer, initialized from `other`."""
    self = cls(filters=other.filters, name=other.name, **kwargs)
    self.build(None, other=other)
    return self

  def build(self, input_shape, other=None):
    """Instantiates weights, optionally initializing them from `other`."""
    if other is None:
      kernel_shape = (input_shape[-1], self.filters)
      kernel = tf.keras.initializers.GlorotUniform()(shape=kernel_shape)
      bias = tf.keras.initializers.Zeros()(shape=(self.filters,))
    else:
      kernel, bias = other.kernel, other.bias
    self.kernel = tf.Variable(
        tf.cast(kernel, self.variable_dtype), name="kernel")
    self.bias = tf.Variable(
        tf.cast(bias, self.variable_dtype), name="bias")
    self.built = True

  def call(self, inputs):
    outputs = tf.linalg.matvec(self.kernel, inputs, transpose_a=True)
    outputs = tf.nn.bias_add(outputs, self.bias)
    return tf.nn.leaky_relu(outputs)

----------------------------------------

TITLE: Custom Gradient Implementation
DESCRIPTION: Shows how to implement custom gradients for numerical stability using tf.custom_gradient decorator

LANGUAGE: python
CODE:
@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.log(1 + e), grad

----------------------------------------

TITLE: Word Embedding Lookup in TensorFlow
DESCRIPTION: Code snippet demonstrating how to use tf.nn.embedding_lookup to convert word IDs into dense vector representations before feeding them into the LSTM.

LANGUAGE: python
CODE:
# embedding_matrix is a tensor of shape [vocabulary_size, embedding size]
word_embeddings = tf.nn.embedding_lookup(embedding_matrix, word_ids)

----------------------------------------

TITLE: Verifying CUPTI Installation
DESCRIPTION: Check if NVIDIA CUDA Profiling Tools Interface (CUPTI) is installed on the system.

LANGUAGE: Shell
CODE:
/sbin/ldconfig -N -v $(sed 's/:/ /g' <<< $LD_LIBRARY_PATH) | \
grep libcupti

----------------------------------------

TITLE: Performing Inference on Bird Vocalization Audio
DESCRIPTION: Applies the model to the framed audio and interprets the results.

LANGUAGE: python
CODE:
logits, embeddings = model.infer_tf(fixed_tm[:1])

probabilities = tf.nn.softmax(logits)
argmax = np.argmax(probabilities)
print(f"The audio is from the class {classes[argmax]} (element:{argmax} in the label.csv file), with probability of {probabilities[0][argmax]}")

----------------------------------------

TITLE: Creating CNN Model using Keras
DESCRIPTION: Defines a simple convolutional neural network model using Keras Sequential API for MNIST digit classification.

LANGUAGE: python
CODE:
def create_model(input_shape):
  """Creates a simple convolutional neural network model using the Keras API"""
  return tf.keras.Sequential([
      tf.keras.layers.Conv2D(28, kernel_size=(3, 3), activation='relu', input_shape=input_shape),
      tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(128, activation=tf.nn.relu),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10, activation=tf.nn.softmax),
  ])

----------------------------------------

TITLE: Implementing Latent Space Vector Search
DESCRIPTION: Implements gradient descent optimization to find the closest latent vector that generates an image similar to a target image. Includes loss function and optimization loop.

LANGUAGE: Python
CODE:
def find_closest_latent_vector(initial_vector, num_optimization_steps, steps_per_image):
  images = []
  losses = []

  vector = tf.Variable(initial_vector)  
  optimizer = tf.optimizers.Adam(learning_rate=0.01)
  loss_fn = tf.losses.MeanAbsoluteError(reduction="sum")

  for step in range(num_optimization_steps):
    if (step % 100)==0:
      print()
    print('.', end='')
    with tf.GradientTape() as tape:
      image = progan(vector.read_value())['default'][0]
      if (step % steps_per_image) == 0:
        images.append(image.numpy())
      target_image_difference = loss_fn(image, target_image[:,:,:3])
      regularizer = tf.abs(tf.norm(vector) - np.sqrt(latent_dim))
      
      loss = target_image_difference + regularizer
      losses.append(loss.numpy())
    grads = tape.gradient(loss, [vector])
    optimizer.apply_gradients(zip(grads, [vector]))
    
  return images, losses

----------------------------------------

TITLE: Configuring Model Parameters and Fake Dataset
DESCRIPTION: Sets up model configuration parameters and prepares a small fake dataset for testing.

LANGUAGE: Python
CODE:
params = {
    'input_size': 3,
    'num_classes': 3,
    'layer_1_size': 2,
    'layer_2_size': 2,
    'num_train_steps': 100,
    'init_lr': 1e-3,
    'end_lr': 0.0,
    'decay_steps': 1000,
    'lr_power': 1.0,
}

# make a small fixed dataset
fake_x = np.ones((2, params['input_size']), dtype=np.float32)
fake_y = np.zeros((2, params['num_classes']), dtype=np.int32)
fake_y[0][0] = 1
fake_y[1][1] = 1

step_num = 3

----------------------------------------

TITLE: Generating Image Samples with S3GAN
DESCRIPTION: This snippet demonstrates how to generate image samples using the loaded S3GAN model. It allows specifying the number of rows and columns, noise seed, and label for the generated images.

LANGUAGE: Python
CODE:
num_rows = 2  # @param {type: "slider", min:1, max:16}
num_cols = 3  # @param {type: "slider", min:1, max:16}
noise_seed = 23  # @param {type:"slider", min:0, max:100, step:1}
label_str = "980) volcano"  # @param [...]

num_samples = num_rows * num_cols
z = sampler.get_noise(num_samples, seed=noise_seed)

label = int(label_str.split(')')[0])
if label == -1:
  labels = np.random.randint(0, num_classes, size=(num_samples))
else:
  labels = np.asarray([label] * num_samples)

samples = sampler.get_samples(z, labels)
imshow(imgrid(samples, cols=num_cols))

----------------------------------------

TITLE: Saving and Converting Model to TensorFlow Lite
DESCRIPTION: Demonstrates how to save the trained model and optionally convert it to TensorFlow Lite format for mobile deployment.

LANGUAGE: Python
CODE:
saved_model_path = f"/tmp/saved_flowers_model_{model_name}"
tf.saved_model.save(model, saved_model_path)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
if optimize_lite_model:
  converter.optimizations = [tf.lite.Optimize.DEFAULT]
  if representative_dataset:  
    converter.representative_dataset = representative_dataset
lite_model_content = converter.convert()

with open(f"/tmp/lite_flowers_model_{model_name}.tflite", "wb") as f:
  f.write(lite_model_content)

----------------------------------------

TITLE: Downloading BARD Dataset
DESCRIPTION: Uses gdown to download the Bangla Article Dataset (BARD) from Google Drive.

LANGUAGE: Python
CODE:
gdown.download(
    url='https://drive.google.com/uc?id=1Ag0jd21oRwJhVFIBohmX_ogeojVtapLy',
    output='bard.zip',
    quiet=True
)

----------------------------------------

TITLE: TensorFlow Service API Definitions in Protocol Buffers
DESCRIPTION: Protocol buffer definitions for TensorFlow's master and worker services, defining the RPC interfaces for distributed communication.

LANGUAGE: protobuf
CODE:
MasterService API definition\nWorkerService API definition

----------------------------------------

TITLE: TPU Optimizer Configuration
DESCRIPTION: Shows how to wrap an optimizer with CrossShardOptimizer for TPU training while maintaining compatibility with local training.

LANGUAGE: python
CODE:
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
if FLAGS.use_tpu:
  optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)

----------------------------------------

TITLE: Profiling Multiple Workers
DESCRIPTION: Profile multiple workers in a distributed TensorFlow setup.

LANGUAGE: Python
CODE:
# E.g., your worker IP addresses are 10.0.0.2, 10.0.0.3, 10.0.0.4, and you
# would like to profile for a duration of 2 seconds.
tf.profiler.experimental.client.trace(
    'grpc://10.0.0.2:8466,grpc://10.0.0.3:8466,grpc://10.0.0.4:8466',
    'gs://your_tb_logdir',
    2000)

----------------------------------------

TITLE: Training Results for P100 with Synthetic Data
DESCRIPTION: Raw performance numbers for training with synthetic data on NVIDIA Tesla P100 GPUs, showing images processed per second for different models and GPU counts.

LANGUAGE: markdown
CODE:
GPUs | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16
---- | ----------- | --------- | ---------- | ------- | -----
1    | 142         | 219       | 91.8       | 2987    | 154
2    | 284         | 422       | 181        | 5658    | 295
4    | 569         | 852       | 356        | 10509   | 584
8    | 1131        | 1734      | 716        | 17822   | 1081

----------------------------------------

TITLE: Instantiating a DNNClassifier Estimator in TensorFlow
DESCRIPTION: Demonstrates how to create a Deep Neural Network Classifier Estimator with specified hidden layers and number of classes.

LANGUAGE: python
CODE:
# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.
classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    # Two hidden layers of 10 nodes each.
    hidden_units=[10, 10],
    # The model must choose between 3 classes.
    n_classes=3)

----------------------------------------

TITLE: Creating Batchable ExtensionTypes in TensorFlow
DESCRIPTION: Shows how to create batchable extension types by using tf.experimental.BatchableExtensionType as the base class and adjusting field shapes to include batch dimensions.

LANGUAGE: Python
CODE:
class Network(tf.experimental.BatchableExtensionType):
  shape: tf.TensorShape  # batch shape. A single network has shape=[].
  work: tf.Tensor        # work[*shape, n] = work left to do at node n
  bandwidth: tf.Tensor   # bandwidth[*shape, n1, n2] = bandwidth from n1->n2

  def __init__(self, work, bandwidth):
    self.work = tf.convert_to_tensor(work)
    self.bandwidth = tf.convert_to_tensor(bandwidth)
    work_batch_shape = self.work.shape[:-1]
    bandwidth_batch_shape = self.bandwidth.shape[:-2]
    self.shape = work_batch_shape.merge_with(bandwidth_batch_shape)

----------------------------------------

TITLE: Defining Helper Function for Similarity Visualization
DESCRIPTION: Creates a helper function to plot a similarity matrix of the sentence embeddings using seaborn for visualization.

LANGUAGE: python
CODE:
def plot_similarity(features, labels):
  """Plot a similarity matrix of the embeddings."""
  cos_sim = pairwise.cosine_similarity(features)
  sns.set(font_scale=1.2)
  cbar_kws=dict(use_gridspec=False, location="left")
  g = sns.heatmap(
      cos_sim, xticklabels=labels, yticklabels=labels,
      vmin=0, vmax=1, cmap="Blues", cbar_kws=cbar_kws)
  g.tick_params(labelright=True, labelleft=False)
  g.set_yticklabels(labels, rotation=0)
  g.set_title("Semantic Textual Similarity")

----------------------------------------

TITLE: Visualizing YAMNet Sound Classification Results
DESCRIPTION: This code creates a visualization of the waveform, spectrogram, and top-scoring sound classes identified by the YAMNet model.

LANGUAGE: python
CODE:
plt.figure(figsize=(10, 6))

# Plot the waveform.
plt.subplot(3, 1, 1)
plt.plot(waveform)
plt.xlim([0, len(waveform)])

# Plot the log-mel spectrogram (returned by the model).
plt.subplot(3, 1, 2)
plt.imshow(spectrogram_np.T, aspect='auto', interpolation='nearest', origin='lower')

# Plot and label the model output scores for the top-scoring classes.
mean_scores = np.mean(scores, axis=0)
top_n = 10
top_class_indices = np.argsort(mean_scores)[::-1][:top_n]
plt.subplot(3, 1, 3)
plt.imshow(scores_np[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')

# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS
# values from the model documentation
patch_padding = (0.025 / 2) / 0.01
plt.xlim([-patch_padding-0.5, scores.shape[0] + patch_padding-0.5])
# Label the top_N classes.
yticks = range(0, top_n, 1)
plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])
_ = plt.ylim(-0.5 + np.array([top_n, 0]))

----------------------------------------

TITLE: Building Language Model Architecture
DESCRIPTION: Constructs the TensorFlow computation graph for word embeddings, activations and perplexity

LANGUAGE: python
CODE:
g = tf.Graph()
n_layer = 12
model_dim = 768

with g.as_default():
  text = tf.placeholder(dtype=tf.string, shape=(1,))

  module = hub.Module(hub_module)

  embeddings = module(dict(text=text), signature="word_embeddings", as_dict=True)["word_embeddings"]
  activations = module(dict(text=text), signature="activations", as_dict=True)["activations"]
  neg_log_likelihood = module(dict(text=text), signature="neg_log_likelihood", as_dict=True)["neg_log_likelihood"]
  ppl = tf.exp(tf.reduce_mean(neg_log_likelihood, axis=1))

----------------------------------------

TITLE: Implementing a Custom Dataset in C++ for TensorFlow
DESCRIPTION: This code snippet demonstrates how to implement a custom Dataset class in C++ for reading custom file formats in TensorFlow. It includes the necessary class definitions and registrations for a 'MyReaderDataset' op.

LANGUAGE: C++
CODE:
#include "tensorflow/core/framework/common_shape_fns.h"
#include "tensorflow/core/framework/dataset.h"
#include "tensorflow/core/framework/op.h"
#include "tensorflow/core/framework/shape_inference.h"

namespace myproject {
namespace {

using ::tensorflow::DT_STRING;
using ::tensorflow::PartialTensorShape;
using ::tensorflow::Status;

class MyReaderDatasetOp : public tensorflow::DatasetOpKernel {
 public:

  MyReaderDatasetOp(tensorflow::OpKernelConstruction* ctx)
      : DatasetOpKernel(ctx) {
    // Parse and validate any attrs that define the dataset using
    // `ctx->GetAttr()`, and store them in member variables.
  }

  void MakeDataset(tensorflow::OpKernelContext* ctx,
                   tensorflow::DatasetBase** output) override {
    // Parse and validate any input tensors that define the dataset using
    // `ctx->input()` or the utility function
    // `ParseScalarArgument<T>(ctx, &arg)`.

    // Create the dataset object, passing any (already-validated) arguments from
    // attrs or input tensors.
    *output = new Dataset(ctx);
  }

 private:
  class Dataset : public tensorflow::GraphDatasetBase {
   public:
    Dataset(tensorflow::OpKernelContext* ctx) : GraphDatasetBase(ctx) {}

    std::unique_ptr<tensorflow::IteratorBase> MakeIteratorInternal(
        const string& prefix) const override {
      return std::unique_ptr<tensorflow::IteratorBase>(new Iterator(
          {this, tensorflow::strings::StrCat(prefix, "::MyReader")}));
    }

    // Record structure: Each record is represented by a scalar string tensor.
    //
    // Dataset elements can have a fixed number of components of different
    // types and shapes; replace the following two methods to customize this
    // aspect of the dataset.
    const tensorflow::DataTypeVector& output_dtypes() const override {
      static auto* const dtypes = new tensorflow::DataTypeVector({DT_STRING});
      return *dtypes;
    }
    const std::vector<PartialTensorShape>& output_shapes() const override {
      static std::vector<PartialTensorShape>* shapes =
          new std::vector<PartialTensorShape>({{}});
      return *shapes;
    }

    string DebugString() const override { return "MyReaderDatasetOp::Dataset"; }

   protected:
    // Optional: Implementation of `GraphDef` serialization for this dataset.
    //
    // Implement this method if you want to be able to save and restore
    // instances of this dataset (and any iterators over it).
    Status AsGraphDefInternal(DatasetGraphDefBuilder* b,
                              tensorflow::Node** output) const override {
      // Construct nodes to represent any of the input tensors from this
      // object's member variables using `b->AddScalar()` and `b->AddVector()`.
      std::vector<tensorflow::Node*> input_tensors;
      TF_RETURN_IF_ERROR(b->AddDataset(this, input_tensors, output));
      return Status::OK();
    }

   private:
    class Iterator : public tensorflow::DatasetIterator<Dataset> {
     public:
      explicit Iterator(const Params& params)
          : DatasetIterator<Dataset>(params), i_(0) {}

      // Implementation of the reading logic.
      //
      // The example implementation in this file yields the string "MyReader!"
      // ten times. In general there are three cases:
      //
      // 1. If an element is successfully read, store it as one or more tensors
      //    in `*out_tensors`, set `*end_of_sequence = false` and return
      //    `Status::OK()`.
      // 2. If the end of input is reached, set `*end_of_sequence = true` and
      //    return `Status::OK()`.
      // 3. If an error occurs, return an error status using one of the helper
      //    functions from "tensorflow/core/lib/core/errors.h".
      Status GetNextInternal(tensorflow::IteratorContext* ctx,
                             std::vector<tensorflow::Tensor>* out_tensors,
                             bool* end_of_sequence) override {
        // NOTE: `GetNextInternal()` may be called concurrently, so it is
        // recommended that you protect the iterator state with a mutex.
        tensorflow::mutex_lock l(mu_);
        if (i_ < 10) {
          // Create a scalar string tensor and add it to the output.
          tensorflow::Tensor record_tensor(ctx->allocator({}), DT_STRING, {});
          record_tensor.scalar<string>()() = "MyReader!";
          out_tensors->emplace_back(std::move(record_tensor));
          ++i_;
          *end_of_sequence = false;
        } else {
          *end_of_sequence = true;
        }
        return Status::OK();
      }

     protected:
      // Optional: Implementation of iterator state serialization for this
      // iterator.
      //
      // Implement these two methods if you want to be able to save and restore
      // instances of this iterator.
      Status SaveInternal(tensorflow::IteratorStateWriter* writer) override {
        tensorflow::mutex_lock l(mu_);
        TF_RETURN_IF_ERROR(writer->WriteScalar(full_name("i"), i_));
        return Status::OK();
      }
      Status RestoreInternal(tensorflow::IteratorContext* ctx,
                             tensorflow::IteratorStateReader* reader) override {
        tensorflow::mutex_lock l(mu_);
        TF_RETURN_IF_ERROR(reader->ReadScalar(full_name("i"), &i_));
        return Status::OK();
      }

     private:
      tensorflow::mutex mu_;
      int64 i_ GUARDED_BY(mu_);
    };
  };
};

// Register the op definition for MyReaderDataset.
//
// Dataset ops always have a single output, of type `variant`, which represents
// the constructed `Dataset` object.
//
// Add any attrs and input tensors that define the dataset here.
REGISTER_OP("MyReaderDataset")
    .Output("handle: variant")
    .SetIsStateful()
    .SetShapeFn(tensorflow::shape_inference::ScalarShape);

// Register the kernel implementation for MyReaderDataset.
REGISTER_KERNEL_BUILDER(Name("MyReaderDataset").Device(tensorflow::DEVICE_CPU),
                        MyReaderDatasetOp);

}  // namespace
}  // namespace myproject

----------------------------------------

TITLE: Data Normalization Function
DESCRIPTION: Function to normalize input features using training data statistics

LANGUAGE: python
CODE:
def norm(x):
  return (x - train_stats['mean']) / train_stats['std']
normed_train_data = norm(train_dataset)
normed_test_data = norm(test_dataset)

----------------------------------------

TITLE: Installing TensorFlow Nightly Build
DESCRIPTION: Installs the nightly build of TensorFlow to access the latest features, particularly the 'save_freq' argument in tf.keras.callbacks.BackupAndRestore introduced in TensorFlow 2.10.

LANGUAGE: bash
CODE:
!pip install tf-nightly

----------------------------------------

TITLE: Running Docker with CUPTI Privileges
DESCRIPTION: Run Docker container with privileged access to resolve CUPTI privilege issues.

LANGUAGE: Shell
CODE:
docker run option '--privileged=true'

----------------------------------------

TITLE: Importing Required Libraries
DESCRIPTION: Importing necessary Python libraries including numpy, matplotlib, tensorflow, tensorflow_datasets and tensorflow_hub

LANGUAGE: python
CODE:
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_hub as hub

----------------------------------------

TITLE: Cloning TensorFlow Models Repository
DESCRIPTION: Commands to clone the TensorFlow models repository and navigate to the relevant directory.

LANGUAGE: shell
CODE:
git clone https://github.com/tensorflow/models/
cd models/samples/core/get_started

----------------------------------------

TITLE: Loading and Training Model with Distribution Strategy
DESCRIPTION: Load the saved model and train it using a different distribution strategy.

LANGUAGE: Python
CODE:
another_strategy = tf.distribute.OneDeviceStrategy('/cpu:0')
with another_strategy.scope():
  restored_keras_model_ds = tf.keras.models.load_model(keras_model_path)
  restored_keras_model_ds.fit(train_dataset, epochs=2)

----------------------------------------

TITLE: Loading Language Model Configuration
DESCRIPTION: Sets up model parameters like language choice and maximum generation length

LANGUAGE: python
CODE:
language = "en"
hub_module = "https://tfhub.dev/google/wiki40b-lm-{}/1".format(language)
max_gen_len = 20

print("Using the {} model to generate sequences of max length {}.".format(hub_module, max_gen_len))

----------------------------------------

TITLE: Importing a Graph Definition in C
DESCRIPTION: Shows the C API functions for importing a graph definition and creating a new session in TensorFlow.

LANGUAGE: C
CODE:
TF_GraphImportGraphDef, TF_NewSession

----------------------------------------

TITLE: Customizing Cross-Device Communication in MirroredStrategy
DESCRIPTION: Creates a MirroredStrategy instance with a custom cross-device communication method. This example uses HierarchicalCopyAllReduce instead of the default NCCL.

LANGUAGE: Python
CODE:
mirrored_strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())

----------------------------------------

TITLE: Running TensorFlow in Docker (CPU)
DESCRIPTION: Example of running a TensorFlow program within a Docker container using the CPU-only image. This command demonstrates importing TensorFlow and performing a simple operation.

LANGUAGE: bash
CODE:
docker run -it --rm tensorflow/tensorflow \
   python -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"

----------------------------------------

TITLE: Loading a Custom FileSystem in Python for TensorFlow
DESCRIPTION: Example of how to load a custom FileSystem implementation in Python using the tf.load_file_system_library() function, which loads the shared object containing the implementation.

LANGUAGE: Python
CODE:
tf.load_file_system_library(file_system_library)

----------------------------------------

TITLE: Loading and Preparing SciCite Dataset for Citation Intent Classification
DESCRIPTION: This code loads the SciCite dataset from TensorFlow Datasets, which will be used to train a citation intent classifier using CORD-19 embeddings.

LANGUAGE: Python
CODE:
builder = tfds.builder(name='scicite')
builder.download_and_prepare()
train_data, validation_data, test_data = builder.as_dataset(
    split=('train', 'validation', 'test'),
    as_supervised=True)

----------------------------------------

TITLE: TensorFlow Python Client APIs
DESCRIPTION: Python client APIs for session management and NCCL operations in TensorFlow.

LANGUAGE: Python
CODE:
tf.Session\ntf.contrib.nccl

----------------------------------------

TITLE: Creating and Using a Dense Layer in TensorFlow
DESCRIPTION: This snippet demonstrates how to create and use a Dense layer from tf.keras.layers.

LANGUAGE: Python
CODE:
layer = tf.keras.layers.Dense(100)
layer = tf.keras.layers.Dense(10, input_shape=(None, 5))

layer(tf.zeros([10, 5]))

layer.variables

layer.kernel, layer.bias

----------------------------------------

TITLE: Initializing TensorFlow with TPU Support
DESCRIPTION: Sets up TensorFlow environment and validates TPU availability with version checks.

LANGUAGE: python
CODE:
import tensorflow.compat.v1 as tf
tf.compat.v1.disable_eager_execution()

import numpy as np
import os

assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'

assert float('.'.join(tf.__version__.split('.')[:2])) >= 1.14, 'Make sure that Tensorflow version is at least 1.14'

----------------------------------------

TITLE: Downloading BAIR Robot Pushing Test Data in Bash
DESCRIPTION: This bash command downloads the test split of the BAIR Robot Pushing dataset to a specified directory.

LANGUAGE: bash
CODE:
mkdir -p $TEST_DIR
wget -nv https://storage.googleapis.com/download.tensorflow.org/data/bair_test_traj_0_to_255.tfrecords -O $TEST_DIR/traj_0_to_255.tfrecords

----------------------------------------

TITLE: Loading Test Images from TF-Flowers Dataset
DESCRIPTION: Loads and preprocesses a batch of images from the TF-Flowers dataset for use in BigBiGAN operations.

LANGUAGE: Python
CODE:
def get_flowers_data():
    """Returns a [32, 256, 256, 3] np.array of preprocessed TF-Flowers samples."""
    # Function implementation...

test_images = get_flowers_data()

----------------------------------------

TITLE: Implementing Control Flow in Python
DESCRIPTION: Demonstrates the use of conditional and loop operations in TensorFlow's Python API.

LANGUAGE: Python
CODE:
tf.cond, tf.while_loop

----------------------------------------

TITLE: Setting up TensorFlow Environment
DESCRIPTION: Imports required libraries including TensorFlow, Pandas and Matplotlib. Sets random seed for reproducibility.

LANGUAGE: python
CODE:
import tensorflow as tf
import pandas as pd
import matplotlib
from matplotlib import pyplot as plt
print("TensorFlow version:", tf.__version__)
# Set a random seed for reproducible results 
tf.random.set_seed(22)

----------------------------------------

TITLE: Creating a Simple TensorFlow C Program
DESCRIPTION: This C program demonstrates basic usage of the TensorFlow C API by printing the library version.

LANGUAGE: c
CODE:
#include <stdio.h>
#include <tensorflow/c/c_api.h>

int main() {
  printf("Hello from TensorFlow C library version %s\n", TF_Version());
  return 0;
}

----------------------------------------

TITLE: Basic TensorFlow Operations
DESCRIPTION: Demonstrates basic tensor operations including addition, square, reduction, and base64 encoding.

LANGUAGE: python
CODE:
print(tf.add(1, 2))
print(tf.add([1, 2], [3, 4]))
print(tf.square(5))
print(tf.reduce_sum([1, 2, 3]))
print(tf.encode_base64("hello world"))

# Operator overloading is also supported
print(tf.square(2) + tf.square(3))

----------------------------------------

TITLE: Importing Required Libraries for TensorFlow and Data Analysis
DESCRIPTION: This code snippet imports necessary Python libraries for TensorFlow, data manipulation, visualization, and progress tracking.

LANGUAGE: Python
CODE:
import functools
import itertools
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd

import tensorflow as tf

import tensorflow_datasets as tfds
import tensorflow_hub as hub

from tqdm import trange

----------------------------------------

TITLE: Importing Required Libraries
DESCRIPTION: Importing necessary Python libraries including TensorFlow, pandas, matplotlib and seaborn for data processing and visualization

LANGUAGE: python
CODE:
import pathlib

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

import tensorflow.compat.v1 as tf

from tensorflow import keras
from tensorflow.keras import layers

print(tf.__version__)

----------------------------------------

TITLE: Defining Input Layer for TensorFlow Neural Network
DESCRIPTION: Python code to define the input layer of a neural network using TensorFlow feature columns.

LANGUAGE: python
CODE:
# Use `input_layer` to apply the feature columns.
net = tf.feature_column.input_layer(features, params['feature_columns'])

----------------------------------------

TITLE: Downloading and Extracting TensorFlow C Library on Linux
DESCRIPTION: This bash script downloads the TensorFlow C library for Linux and extracts it to /usr/local. It uses wget to download the file and tar to extract it.

LANGUAGE: bash
CODE:
FILENAME=libtensorflow-cpu-linux-x86_64.tar.gz
wget -q --no-check-certificate https://storage.googleapis.com/tensorflow/versions/2.18.1/${FILENAME}
sudo tar -C /usr/local -xzf ${FILENAME}

----------------------------------------

TITLE: Loading the Preprocessed Penguins Dataset
DESCRIPTION: Loads the preprocessed penguins dataset, splitting it into training and test sets.

LANGUAGE: Python
CODE:
ds_split, info = tfds.load("penguins/processed", split=['train[:20%]', 'train[20%:]'], as_supervised=True, with_info=True)

ds_test = ds_split[0]
ds_train = ds_split[1]
assert isinstance(ds_test, tf.data.Dataset)

print(info.features)
df_test = tfds.as_dataframe(ds_test.take(5), info)
print("Test dataset sample: ")
print(df_test)

df_train = tfds.as_dataframe(ds_train.take(5), info)
print("Train dataset sample: ")
print(df_train)

ds_train_batch = ds_train.batch(32)

----------------------------------------

TITLE: Installing TensorFlow Docs Package
DESCRIPTION: Command to install the tensorflow_docs package from GitHub, which includes tools for generating API reference docs.

LANGUAGE: bash
CODE:
pip install git+https://github.com/tensorflow/docs

----------------------------------------

TITLE: Transfer Learning with Custom Classification Head
DESCRIPTION: Creates a new model by combining a pre-trained feature extractor with a custom classification layer for transfer learning on the flowers dataset.

LANGUAGE: Python
CODE:
feature_extractor_url = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2"

feature_extractor_layer = hub.KerasLayer(feature_extractor_url,
                                         input_shape=(224,224,3))
feature_extractor_layer.trainable = False

model = tf.keras.Sequential([
  feature_extractor_layer,
  layers.Dense(image_data.num_classes, activation='softmax')
])

----------------------------------------

TITLE: Implementing Compression with Desired Factor
DESCRIPTION: Defines a function to compress an image to a specified proportion of its original size.

LANGUAGE: Python
CODE:
def compress_image_with_factor(I, compression_factor, verbose=False):
  # Returns a compressed image based on a desired compression factor
  m,n,o = I.shape
  r = int((compression_factor * m * n)/(m + n + 1))
  I_r, I_r_prop = compress_image(I, r, verbose=verbose)
  return I_r

----------------------------------------

TITLE: TF1 SavedModel Builder Example
DESCRIPTION: Example showing how to save a model using TF1's SavedModelBuilder API with a simple graph that adds 2 to an input.

LANGUAGE: python
CODE:
remove_dir("saved-model-builder")

with tf.Graph().as_default() as g:
  with tf1.Session() as sess:
    input = tf1.placeholder(tf.float32, shape=[])
    output = add_two(input)
    print("add two output: ", sess.run(output, {input: 3.}))

    # Save with SavedModelBuilder
    builder = tf1.saved_model.Builder('saved-model-builder')
    sig_def = tf1.saved_model.predict_signature_def(
        inputs={'input': input},
        outputs={'output': output})
    builder.add_meta_graph_and_variables(
        sess, tags=["serve"], signature_def_map={
            tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY: sig_def
    })
    builder.save()

----------------------------------------

TITLE: Using Tensor References as Hashable Keys in TensorFlow 2
DESCRIPTION: Demonstrates how to use tensor.ref() to get a hashable reference that can be used as a key in sets or dictionaries in TensorFlow 2.

LANGUAGE: Python
CODE:
tf.compat.v1.enable_tensor_equality()
x = tf.Variable(0.0)

tensor_set = set([x.ref(), tf.constant(2.0).ref()])
assert x.ref() in tensor_set

tensor_set

----------------------------------------

TITLE: Importing Required Python Libraries
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Hub, NumPy, scikit-learn, matplotlib, and seaborn.

LANGUAGE: Python
CODE:
import os

import tensorflow as tf
import tensorflow_hub as hub

import gdown
import numpy as np
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns

----------------------------------------

TITLE: Patching BAIR Dataset Builder for Test Data in Python
DESCRIPTION: This code modifies the BAIR dataset builder to only expect test data, then downloads and prepares the dataset.

LANGUAGE: python
CODE:
builder = BairRobotPushingSmall()
test_generator = SplitGenerator(name='test', gen_kwargs={"filedir": str(TEST_DIR)})
builder._split_generators = lambda _: [test_generator]
builder.download_and_prepare()

----------------------------------------

TITLE: Ragged Tensor Construction
DESCRIPTION: Shows different ways to construct ragged tensors using row splits, row lengths, and nested structures

LANGUAGE: python
CODE:
sentences = tf.ragged.constant([
    ["Let's", "build", "some", "ragged", "tensors", "!"],
    ["We", "can", "use", "tf.ragged.constant", "."]])
print(sentences)

----------------------------------------

TITLE: Computing Discriminator Scores and Losses
DESCRIPTION: Calculates and displays discriminator scores and losses for encoder and generator pairs.

LANGUAGE: Python
CODE:
feed_dict = {enc_ph: test_images, gen_ph: np.random.randn(32, 120)}
_out_scores_enc, _out_scores_gen, _out_losses = sess.run(
    [disc_scores_enc, disc_scores_gen, losses], feed_dict=feed_dict)
print('Encoder scores:', {k: v.mean() for k, v in _out_scores_enc.items()})
print('Generator scores:', {k: v.mean() for k, v in _out_scores_gen.items()})
print('Losses:', _out_losses)

----------------------------------------

TITLE: TensorFlow 1.x TensorBoard Implementation with Estimator
DESCRIPTION: Configure and train a DNNClassifier using TF1.x Estimator API with TensorBoard logging enabled through RunConfig settings.

LANGUAGE: python
CODE:
%reload_ext tensorboard

feature_columns = [tf1.feature_column.numeric_column("x", shape=[28, 28])]

config = tf1.estimator.RunConfig(save_summary_steps=1,
                                 save_checkpoints_steps=1)

path = tempfile.mkdtemp()

classifier = tf1.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[256, 32],
    optimizer=tf1.train.AdamOptimizer(0.001),
    n_classes=10,
    dropout=0.1,
    model_dir=path,
    config = config
)

train_input_fn = tf1.estimator.inputs.numpy_input_fn(
    x={"x": x_train},
    y=y_train.astype(np.int32),
    num_epochs=10,
    batch_size=50,
    shuffle=True,
)

test_input_fn = tf1.estimator.inputs.numpy_input_fn(
    x={"x": x_test},
    y=y_test.astype(np.int32),
    num_epochs=10,
    shuffle=False
)

train_spec = tf1.estimator.TrainSpec(input_fn=train_input_fn, max_steps=10)
eval_spec = tf1.estimator.EvalSpec(input_fn=test_input_fn,
                                   steps=10,
                                   throttle_secs=0)

tf1.estimator.train_and_evaluate(estimator=classifier,
                                train_spec=train_spec,
                                eval_spec=eval_spec)

----------------------------------------

TITLE: Installing TensorFlow Datasets
DESCRIPTION: Installs the tfds-nightly package to access the penguins dataset.

LANGUAGE: Bash
CODE:
!pip install -q tfds-nightly

----------------------------------------

TITLE: Setting GCS Bucket as Cache Location in Python
DESCRIPTION: This snippet shows how to set a Google Cloud Storage (GCS) bucket as the cache location for TensorFlow Hub models, which is useful when running on TPUs in Colab notebooks.

LANGUAGE: python
CODE:
import os
os.environ["TFHUB_CACHE_DIR"] = "gs://my-bucket/tfhub-modules-cache"

----------------------------------------

TITLE: Instantiating a TF1 Hub Module in Python
DESCRIPTION: Creates a hub.Module object from a URL or filesystem path to import a TF1 Hub format model into a TensorFlow program.

LANGUAGE: Python
CODE:
m = hub.Module("path/to/a/module_dir")

----------------------------------------

TITLE: Building and Testing TensorFlow Hub
DESCRIPTION: Commands to build the pip package and run tests using Bazel

LANGUAGE: shell
CODE:
bazel test tensorflow_hub:all

LANGUAGE: shell
CODE:
bazel build tensorflow_hub/pip_package:build_pip_package

LANGUAGE: shell
CODE:
bazel-bin/tensorflow_hub/pip_package/build_pip_package /tmp/tensorflow_hub_pkg

----------------------------------------

TITLE: Loading and Preprocessing Image Data
DESCRIPTION: Functions to load, split and preprocess image pairs from the CMP Facade dataset, including resizing, random cropping, and normalization.

LANGUAGE: Python
CODE:
def load(image_file):
  image = tf.io.read_file(image_file)
  image = tf.io.decode_jpeg(image)

  w = tf.shape(image)[1]
  w = w // 2
  input_image = image[:, w:, :]
  real_image = image[:, :w, :]

  input_image = tf.cast(input_image, tf.float32)
  real_image = tf.cast(real_image, tf.float32)

  return input_image, real_image

----------------------------------------

TITLE: Generating Embeddings for Various Text Lengths in Python
DESCRIPTION: This code demonstrates how to generate embeddings for a word, sentence, and paragraph using the Universal Sentence Encoder. It prints the embedding size and a snippet of the embedding for each input.

LANGUAGE: Python
CODE:
word = "Elephant"
sentence = "I am a sentence for which I would like to get its embedding."
paragraph = (
    "Universal Sentence Encoder embeddings also support short paragraphs. "
    "There is no hard limit on how long the paragraph is. Roughly, the longer "
    "the more 'diluted' the embedding will be.")
messages = [word, sentence, paragraph]

# Reduce logging output.
logging.set_verbosity(logging.ERROR)

message_embeddings = embed(messages)

for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):
  print("Message: {}".format(messages[i]))
  print("Embedding size: {}".format(len(message_embedding)))
  message_embedding_snippet = ", ".join(
      (str(x) for x in message_embedding[:3]))
  print("Embedding: [{}, ...]\n".format(message_embedding_snippet))

----------------------------------------

TITLE: Setting TFHUB_CACHE_DIR in Bash for Persistent Caching
DESCRIPTION: This snippet shows how to set the TFHUB_CACHE_DIR environment variable in a Bash shell to enable persistent caching of TensorFlow Hub models across system reboots.

LANGUAGE: bash
CODE:
export TFHUB_CACHE_DIR=$HOME/.cache/tfhub_modules

----------------------------------------

TITLE: Excluding Specific Lint Checks
DESCRIPTION: Example of using nblint while excluding specific lint checks for a project.

LANGUAGE: shell
CODE:
$ python3 -m tensorflow_docs.tools.nblint \
    --styles=tensorflow --arg=repo:community/repo-name \
    --exclude_lint=tensorflow::copyright_check \
    --exclude_lint=tensorflow::button_website \
    ./community/notebook.ipynb

----------------------------------------

TITLE: TensorFlow Operation Usage Example
DESCRIPTION: Example showing how to use a custom TensorFlow operation with proper parameter passing.

LANGUAGE: python
CODE:
output = my_op(t1, t2, my_param=0.5, other_param=0.6,
               output_collections=['MY_OPS'], name='add_t1t2')

----------------------------------------

TITLE: Loading YAMNet Model from TensorFlow Hub
DESCRIPTION: This code loads the pre-trained YAMNet model from TensorFlow Hub for sound classification.

LANGUAGE: python
CODE:
# Load the model.
model = hub.load('https://tfhub.dev/google/yamnet/1')

----------------------------------------

TITLE: Create Data Parallel Mesh
DESCRIPTION: Creates a DTensor mesh for data parallel training with 8 devices

LANGUAGE: python
CODE:
mesh = dtensor.create_mesh([("batch", 8)], devices=devices)

----------------------------------------

TITLE: Creating a Convolutional Neural Network Model with Flax
DESCRIPTION: Defines a convolutional neural network model using the Flax library, including methods for loss calculation, prediction, and accuracy evaluation.

LANGUAGE: Python
CODE:
class ConvModel(flax.linen.Module):

  @flax.linen.compact
  def __call__(self, x, train):
    x = flax.linen.Conv(features=12, kernel_size=(3,3), padding="SAME", use_bias=False)(x)
    x = flax.linen.BatchNorm(use_running_average=not train, use_scale=False, use_bias=True)(x)
    x = x.reshape((x.shape[0], -1))  # flatten
    x = flax.linen.Dense(features=200, use_bias=True)(x)
    x = flax.linen.BatchNorm(use_running_average=not train, use_scale=False, use_bias=True)(x)
    x = flax.linen.Dropout(rate=0.3, deterministic=not train)(x)
    x = flax.linen.relu(x)
    x = flax.linen.Dense(features=10)(x)
    return x

  def loss(self, params, other_state, rng, data, labels, train):
    logits, batch_stats = self.apply({'params': params, **other_state},
                                     data,
                                     mutable=['batch_stats'],
                                     rngs={'dropout': rng},
                                     train=train)
    loss = optax.softmax_cross_entropy(logits, labels).mean()
    return loss, batch_stats

  def predict(self, state, data):
    logits = self.apply(state, data, train=False)
    probabilities = flax.linen.log_softmax(logits)
    return probabilities

  def accuracy(self, state, data, labels):
    probabilities = self.predict(state, data)
    predictions = jnp.argmax(probabilities, axis=-1)
    dense_labels = jnp.argmax(labels, axis=-1)
    accuracy = jnp.equal(predictions, dense_labels).mean()
    return accuracy

----------------------------------------

TITLE: Loading TF2 SavedModel in TF1.15/TF1 Compatibility Mode using hub.load
DESCRIPTION: Shows how to load a TF2 SavedModel in TensorFlow 1.15 or TF1 compatibility mode using the hub.load function.

LANGUAGE: python
CODE:
m = hub.load(handle)
outputs = m(inputs)

----------------------------------------

TITLE: Comparing Tensor Object References in TensorFlow 2
DESCRIPTION: Demonstrates how to compare tensor object references in TensorFlow 2 using the 'is' operator, which is necessary when object identity comparison is needed.

LANGUAGE: Python
CODE:
tf.compat.v1.enable_tensor_equality()
x = tf.Variable(0.0)
y = tf.Variable(0.0)

x is y

----------------------------------------

TITLE: Defining Feature Conversion Functions for tf.Example
DESCRIPTION: Helper functions to convert different data types into tf.train.Feature format compatible with tf.Example records

LANGUAGE: python
CODE:
def _bytes_feature(value):
  """Returns a bytes_list from a string / byte."""
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _float_feature(value):
  """Returns a float_list from a float / double."""
  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))

def _int64_feature(value):
  """Returns an int64_list from a bool / enum / int / uint."""
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

----------------------------------------

TITLE: Installing Dependencies for Bird Vocalization Classification
DESCRIPTION: Installs required libraries tensorflow_io and librosa using pip.

LANGUAGE: bash
CODE:
!pip install -q "tensorflow_io==0.28.*"
!pip install -q librosa

----------------------------------------

TITLE: Demonstrating Code Block Syntax in Markdown
DESCRIPTION: Shows how to create a code block in Markdown using triple backticks, optionally specifying the programming language.

LANGUAGE: markdown
CODE:
```python
# some python code here
```

----------------------------------------

TITLE: Illustrating Op Return Values in Python
DESCRIPTION: Demonstrates how to show the return value of a TensorFlow operation using a comment with '# ' in markdown files.

LANGUAGE: python
CODE:
# 'input' is a tensor of shape [2, 3, 5]
tf.expand_dims(input, 0)  #  [1, 2, 3, 5]

----------------------------------------

TITLE: Defining Compressed Dense Layer
DESCRIPTION: Creates a compressed dense layer that stores weights in a compressed format and decompresses them on-the-fly.

LANGUAGE: python
CODE:
class CompressedDense(CustomDense):

  def build(self, input_shape, other=None):
    assert isinstance(other, CompressibleDense)
    self.input_channels = other.kernel.shape[0]
    self.kernel_compressed, self.kernel_log_step = compress_latent(
        other.kernel_latent, other.kernel_log_step, "kernel")
    self.bias_compressed, self.bias_log_step = compress_latent(
        other.bias_latent, other.bias_log_step, "bias")
    self.built = True

  @property
  def kernel(self):
    kernel_shape = (self.input_channels, self.filters)
    return decompress_latent(
        self.kernel_compressed, kernel_shape, self.kernel_log_step)

  @property
  def bias(self):
    bias_shape = (self.filters,)
    return decompress_latent(
        self.bias_compressed, bias_shape, self.bias_log_step)

----------------------------------------

TITLE: Creating Basic Tensors in TensorFlow
DESCRIPTION: Demonstrates how to create scalar, vector, and matrix tensors using tf.constant.

LANGUAGE: Python
CODE:
# This will be an int32 tensor by default; see "dtypes" below.
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)

# Let's make this a float tensor.
rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
print(rank_1_tensor)

# If you want to be specific, you can set the dtype (see below) at creation time
rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor)

----------------------------------------

TITLE: Creating Feature Columns in TensorFlow
DESCRIPTION: Defines categorical and numeric feature columns for the Titanic dataset.

LANGUAGE: Python
CODE:
CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',
                       'embark_town', 'alone']
NUMERIC_COLUMNS = ['age', 'fare']

feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:
  vocabulary = dftrain[feature_name].unique()
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))

----------------------------------------

TITLE: Loading Pre-trained HRNet Model from TensorFlow Hub in Python
DESCRIPTION: This code allows the user to choose a pre-trained HRNet model for semantic segmentation and loads it from TensorFlow Hub. The model name is constructed based on the user's selection.

LANGUAGE: python
CODE:
#@title Choose a pre-trained HRNet model to load.

hrnet_model_name = 'ade20k-hrnetv2-w48/1'  #@param ["ade20k-hrnetv2-w48/1", "isprs-hrnetv2-w48/1", "vkitti2-hrnetv2-w48/1", "vgallery-hrnetv2-w48/1", "sunrgbd-hrnetv2-w48/1", "suim-hrnetv2-w48/1", "scannet-hrnetv2-w48/1", "pvoc-hrnetv2-w48/1", "msegpcontext-hrnetv2-w48/1", "mapillary-hrnetv2-w48/1", "kitti-hrnetv2-w48/1", "isaid-hrnetv2-w48/1", "idd-hrnetv2-w48/1", "coco-hrnetv2-w48/1", "city-hrnetv2-w48/1", "camvid-hrnetv2-w48/1", "bdd-hrnetv2-w48/1"]

tfhub_model_name = 'https://tfhub.dev/google/HRNet/' + hrnet_model_name

print('HRNet model selected           :', tfhub_model_name)

----------------------------------------

TITLE: Handling Exceptions in TensorFlow DocTest
DESCRIPTION: This snippet shows how to handle exceptions in DocTest, where exception details are ignored except for the Exception that's raised.

LANGUAGE: Python
CODE:
>>> np_var = np.array([1, 2])
>>> tf.keras.backend.is_keras_tensor(np_var)
Traceback (most recent call last):
...
ValueError: Unexpectedly found an instance of type `<class 'numpy.ndarray'>`.

----------------------------------------

TITLE: Preprocessing LibriSpeech Dataset for Wav2Vec2 Fine-tuning
DESCRIPTION: Defines functions to read and preprocess audio and text data from the LibriSpeech dataset.

LANGUAGE: Python
CODE:
def read_txt_file(f):
  with open(f, "r") as f:
    samples = f.read().split("\n")
    samples = {s.split()[0]: " ".join(s.split()[1:]) for s in samples if len(s.split()) > 2}
  return samples

REQUIRED_SAMPLE_RATE = 16000

def read_flac_file(file_path):
  with open(file_path, "rb") as f:
      audio, sample_rate = sf.read(f)
  if sample_rate != REQUIRED_SAMPLE_RATE:
      raise ValueError(
          f"sample rate (={sample_rate}) of your files must be {REQUIRED_SAMPLE_RATE}"
      )
  file_id = os.path.split(file_path)[-1][:-len(".flac")]
  return {file_id: audio}

----------------------------------------

TITLE: Loading and Preprocessing Fashion MNIST Dataset
DESCRIPTION: Downloads Fashion MNIST dataset and preprocesses images by adding channel dimension and normalizing pixel values to [0,1] range

LANGUAGE: Python
CODE:
fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images = train_images[..., None]
test_images = test_images[..., None]

train_images = train_images / np.float32(255)
test_images = test_images / np.float32(255)

train_labels = train_labels.astype('int64')
test_labels = test_labels.astype('int64')

----------------------------------------

TITLE: Importing Required Libraries for Text-to-Video Retrieval
DESCRIPTION: Installs OpenCV and imports necessary Python libraries including TensorFlow, TensorFlow Hub, NumPy, OpenCV, and IPython display utilities.

LANGUAGE: python
CODE:
!pip install -q opencv-python

import os
import tensorflow.compat.v2 as tf
import tensorflow_hub as hub
import numpy as np
import cv2
from IPython import display
import math

----------------------------------------

TITLE: Defining dataset and model for MNIST classification
DESCRIPTION: Creates functions to load and preprocess the MNIST dataset, and defines a CNN model architecture for image classification.

LANGUAGE: Python
CODE:
def mnist_dataset(batch_size):
  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
  x_train = x_train / np.float32(255)
  y_train = y_train.astype(np.int64)
  train_dataset = tf.data.Dataset.from_tensor_slices(
      (x_train, y_train)).shuffle(60000)
  return train_dataset

def build_cnn_model():
  return tf.keras.Sequential([
      tf.keras.Input(shape=(28, 28)),
      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
      tf.keras.layers.Conv2D(32, 3, activation='relu'),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(10)
  ])

----------------------------------------

TITLE: Running Clang-Format Check on C++ Files
DESCRIPTION: Commands to check C++ file formatting using clang-format with Google style and compare against original.

LANGUAGE: bash
CODE:
$ clang-format <my_cc_file> --style=google > /tmp/my_cc_file.cc
$ diff <my_cc_file> /tmp/my_cc_file.cc

----------------------------------------

TITLE: Compressing Model Weights
DESCRIPTION: Defines functions to compress and decompress model weights using entropy coding.

LANGUAGE: python
CODE:
def compress_latent(latent, log_step, name):
  em = tfc.PowerLawEntropyModel(latent.shape.rank)
  compressed = em.compress(latent / tf.exp(log_step))
  compressed = tf.Variable(compressed, name=f"{name}_compressed")
  log_step = tf.cast(log_step, tf.float16)
  log_step = tf.Variable(log_step, name=f"{name}_log_step")
  return compressed, log_step

def decompress_latent(compressed, shape, log_step):
  latent = tfc.PowerLawEntropyModel(len(shape)).decompress(compressed, shape)
  step = tf.exp(tf.cast(log_step, latent.dtype))
  return latent * step

----------------------------------------

TITLE: Basic Ragged Tensor Operations
DESCRIPTION: Demonstrates creation and basic operations with ragged tensors including arithmetic, reduction, and string operations

LANGUAGE: python
CODE:
digits = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])
words = tf.ragged.constant([["So", "long"], ["thanks", "for", "all", "the", "fish"]])
print(tf.add(digits, 3))
print(tf.reduce_mean(digits, axis=1))
print(tf.concat([digits, [[5, 3]]], axis=0))
print(tf.tile(digits, [1, 2]))
print(tf.strings.substr(words, 0, 2))

----------------------------------------

TITLE: Creating Crossed Feature Column in TensorFlow
DESCRIPTION: Adds a crossed feature column combining age and gender to improve model performance.

LANGUAGE: Python
CODE:
age_x_gender = tf.feature_column.crossed_column(['age', 'sex'], hash_bucket_size=100)

----------------------------------------

TITLE: Displaying Array as Image in Jupyter Notebook
DESCRIPTION: This function converts a numpy array to an image and displays it in the Jupyter notebook. It scales the array values to the 0-255 range, converts it to a JPEG image, and uses IPython's display functionality to show the image.

LANGUAGE: Python
CODE:
def DisplayArray(a, fmt='jpeg', rng=[0,1]):
  """Display an array as a picture."""
  a = (a - rng[0])/float(rng[1] - rng[0])*255
  a = np.uint8(np.clip(a, 0, 255))
  f = BytesIO()
  PIL.Image.fromarray(a).save(f, fmt)
  clear_output(wait = True)
  display(Image(data=f.getvalue()))

----------------------------------------

TITLE: Device Placement Logging in TensorFlow
DESCRIPTION: Demonstrates how to enable device placement logging and perform basic matrix multiplication operations to observe where operations are executed.

LANGUAGE: python
CODE:
tf.debugging.set_log_device_placement(True)

# Create some tensors
a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b)

print(c)

----------------------------------------

TITLE: Importing Libraries for PDE Simulation in TensorFlow
DESCRIPTION: This snippet imports the necessary libraries for simulating partial differential equations using TensorFlow, as well as libraries for visualization. It includes TensorFlow, NumPy, PIL, and IPython display utilities.

LANGUAGE: Python
CODE:
import tensorflow.compat.v1 as tf

import numpy as np

#Imports for visualization
import PIL.Image
from io import BytesIO
from IPython.display import clear_output, Image, display

----------------------------------------

TITLE: Retrieving All Op Definitions in C++
DESCRIPTION: Shows how to get a list of all registered OpDefs using the C++ API.

LANGUAGE: C++
CODE:
OpRegistry::Global()->GetRegisteredOps()

----------------------------------------

TITLE: Manual Module Download Using Curl
DESCRIPTION: Shows how to manually download and extract a TF Hub module using curl when automatic download fails.

LANGUAGE: bash
CODE:
# Create a folder for the TF hub module.
$ mkdir /tmp/moduleA
# Download the module, and uncompress it to the destination folder. You might want to do this manually.
$ curl -L "https://tfhub.dev/google/universal-sentence-encoder/2?tf-hub-format=compressed" | tar -zxvC /tmp/moduleA
# Test to make sure it works.
$ python
> import tensorflow_hub as hub
> hub.Module("/tmp/moduleA")

----------------------------------------

TITLE: Tiled Gradient Implementation
DESCRIPTION: Enhanced implementation that processes large images in tiles to manage memory usage

LANGUAGE: Python
CODE:
class TiledGradients(tf.Module):
  def __init__(self, model):
    self.model = model

  @tf.function(
      input_signature=(
        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),
        tf.TensorSpec(shape=[2], dtype=tf.int32),
        tf.TensorSpec(shape=[], dtype=tf.int32),)
  )
  def __call__(self, img, img_size, tile_size=512):
    shift, img_rolled = random_roll(img, tile_size)
    gradients = tf.zeros_like(img_rolled)
    
    xs = tf.range(0, img_size[1], tile_size)[:-1]
    if not tf.cast(len(xs), bool):
      xs = tf.constant([0])
    ys = tf.range(0, img_size[0], tile_size)[:-1]
    if not tf.cast(len(ys), bool):
      ys = tf.constant([0])

    for x in xs:
      for y in ys:
        with tf.GradientTape() as tape:
          tape.watch(img_rolled)
          img_tile = img_rolled[y:y+tile_size, x:x+tile_size]
          loss = calc_loss(img_tile, self.model)
        gradients = gradients + tape.gradient(loss, img_rolled)

    gradients = tf.roll(gradients, shift=-shift, axis=[0,1])
    gradients /= tf.math.reduce_std(gradients) + 1e-8 

    return gradients

----------------------------------------

TITLE: Variable In-Place Operations
DESCRIPTION: Example of implicit type conversion in Variable operations where the result type matches the variable's dtype.

LANGUAGE: Python
CODE:
a = tf.Variable(10, tf.int32)
a.assign_add(tf.constant(5, tf.int16))

----------------------------------------

TITLE: Creating Compressible MNIST Classifier
DESCRIPTION: Defines a function to create an MNIST classifier using compressible layers.

LANGUAGE: python
CODE:
def make_mnist_classifier(regularizer):
  return tf.keras.Sequential([
      CompressibleConv2D(regularizer, 20, 5, strides=2, name="conv_1"),
      CompressibleConv2D(regularizer, 50, 5, strides=2, name="conv_2"),
      tf.keras.layers.Flatten(),
      CompressibleDense(regularizer, 500, name="fc_1"),
      CompressibleDense(regularizer, 10, name="fc_2"),
  ], name="classifier")

compressible_classifier = make_mnist_classifier(regularizer)

----------------------------------------

TITLE: Configuring Virtual CPU Devices
DESCRIPTION: Setting up multiple virtual CPU devices for simulating distributed training

LANGUAGE: python
CODE:
N_VIRTUAL_DEVICES = 2
physical_devices = tf.config.list_physical_devices("CPU")
tf.config.set_logical_device_configuration(
    physical_devices[0], [tf.config.LogicalDeviceConfiguration() for _ in range(N_VIRTUAL_DEVICES)])

----------------------------------------

TITLE: Retrieving All Op Definitions in C
DESCRIPTION: Demonstrates how to get a list of all registered OpDef protocol messages using the C API.

LANGUAGE: C
CODE:
TF_GetAllOpList

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Installing necessary Python packages h5py and pyyaml for model saving functionality

LANGUAGE: bash
CODE:
!pip install h5py pyyaml

----------------------------------------

TITLE: Registering a Basic TensorFlow Op in C++
DESCRIPTION: Demonstrates how to register a simple 'ZeroOut' op that takes an int32 tensor as input and outputs a tensor with all but the first element set to zero.

LANGUAGE: C++
CODE:
#include "tensorflow/core/framework/op.h"
#include "tensorflow/core/framework/shape_inference.h"

using namespace tensorflow;

REGISTER_OP("ZeroOut")
    .Input("to_zero: int32")
    .Output("zeroed: int32")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(0));
      return Status::OK();
    });

----------------------------------------

TITLE: Defining Constants for Image Path and Model URL
DESCRIPTION: Sets constants for the input image path and the TensorFlow Hub model URL for ESRGAN.

LANGUAGE: Python
CODE:
IMAGE_PATH = "original.png"
SAVED_MODEL_PATH = "https://tfhub.dev/captain-pool/esrgan-tf2/1"

----------------------------------------

TITLE: Generating Sample Images with BigBiGAN
DESCRIPTION: Uses the BigBiGAN generator to produce sample images from random latent vectors.

LANGUAGE: Python
CODE:
feed_dict = {gen_ph: np.random.randn(32, 120)}
_out_samples = sess.run(gen_samples, feed_dict=feed_dict)
print('samples shape:', _out_samples.shape)
imshow(imgrid(image_to_uint8(_out_samples), cols=4))

----------------------------------------

TITLE: Creating a Custom Layer for Printing Intermediate Values
DESCRIPTION: Demonstrates how to create a custom Keras layer that prints intermediate values during model execution.

LANGUAGE: Python
CODE:
class PrintLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    tf.print(inputs)
    return inputs

def get_model():
  inputs = tf.keras.layers.Input(shape=(1,))
  out_1 = tf.keras.layers.Dense(4)(inputs)
  out_2 = tf.keras.layers.Dense(1)(out_1)
  # use custom layer to tf.print intermediate features
  out_3 = PrintLayer()(out_2)
  model = tf.keras.Model(inputs=inputs, outputs=out_3)
  return model

model = get_model()
model.compile(optimizer="adam", loss="mse")
model.fit([1, 2, 3], [0.0, 0.0, 1.0])

----------------------------------------

TITLE: Defining Compressible Dense Layer
DESCRIPTION: Creates a compressible dense layer that applies quantization and regularization to weights.

LANGUAGE: python
CODE:
class CompressibleDense(CustomDense):

  def __init__(self, regularizer, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.regularizer = regularizer

  def build(self, input_shape, other=None):
    """Instantiates weights, optionally initializing them from `other`."""
    super().build(input_shape, other=other)
    if other is not None and hasattr(other, "kernel_log_step"):
      kernel_log_step = other.kernel_log_step
      bias_log_step = other.bias_log_step
    else:
      kernel_log_step = bias_log_step = -4.
    self.kernel_log_step = tf.Variable(
        tf.cast(kernel_log_step, self.variable_dtype), name="kernel_log_step")
    self.bias_log_step = tf.Variable(
        tf.cast(bias_log_step, self.variable_dtype), name="bias_log_step")
    self.add_loss(lambda: self.regularizer(
        self.kernel_latent / tf.exp(self.kernel_log_step)))
    self.add_loss(lambda: self.regularizer(
        self.bias_latent / tf.exp(self.bias_log_step)))

  @property
  def kernel(self):
    return quantize(self.kernel_latent, self.kernel_log_step)

  @kernel.setter
  def kernel(self, kernel):
    self.kernel_latent = tf.Variable(kernel, name="kernel_latent")

  @property
  def bias(self):
    return quantize(self.bias_latent, self.bias_log_step)

  @bias.setter
  def bias(self, bias):
    self.bias_latent = tf.Variable(bias, name="bias_latent")

----------------------------------------

TITLE: TensorFlow Citation in BibTeX Format
DESCRIPTION: BibTeX formatted citation for the original TensorFlow white paper published in 2015. Includes complete author list and publication details.

LANGUAGE: bibtex
CODE:
@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

----------------------------------------

TITLE: Installing Required Libraries for Semantic Search
DESCRIPTION: Installs the necessary Python libraries including Apache Beam, scikit-learn, and Annoy for performing semantic search with approximate nearest neighbors.

LANGUAGE: Bash
CODE:
!pip install -q apache_beam
!pip install -q 'scikit_learn~=0.23.0'  # For gaussian_random_matrix.
!pip install -q annoy

----------------------------------------

TITLE: Loading Model with Keras API
DESCRIPTION: Load the saved model using the Keras API and continue training without distribution strategy.

LANGUAGE: Python
CODE:
restored_keras_model = tf.keras.models.load_model(keras_model_path)
restored_keras_model.fit(train_dataset, epochs=2)

----------------------------------------

TITLE: Evaluating Trained Model on Test Dataset
DESCRIPTION: Loads the latest checkpoint and evaluates the model's performance on the test dataset.

LANGUAGE: Python
CODE:
model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))

eval_loss, eval_acc = model.evaluate(eval_dataset)
print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))

----------------------------------------

TITLE: Loading and Preprocessing IMDB Dataset
DESCRIPTION: Loading the IMDB dataset from Keras and limiting vocabulary to top 10,000 words for efficiency

LANGUAGE: python
CODE:
imdb = keras.datasets.imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

----------------------------------------

TITLE: Running Custom Estimator Example
DESCRIPTION: Command to run the custom_estimator.py script.

LANGUAGE: shell
CODE:
python custom_estimator.py

----------------------------------------

TITLE: Defining BigBiGAN Wrapper Class
DESCRIPTION: Creates a wrapper class for convenient access to various BigBiGAN functions including generation, encoding, and discrimination.

LANGUAGE: Python
CODE:
class BigBiGAN(object):

    def __init__(self, module):
        """Initialize a BigBiGAN from the given TF Hub module."""
        self._module = module

    def generate(self, z, upsample=False):
        """Run a batch of latents z through the generator to generate images."""
        # Method implementation...

    # Other methods...

    def losses(self, x, z):
        """Compute per-module BigBiGAN losses given data & latent sample batches."""
        # Method implementation...

----------------------------------------

TITLE: Defining TensorFlow 2 Model
DESCRIPTION: Creates a TensorFlow 2 model using the Keras API with custom training step and logging.

LANGUAGE: Python
CODE:
class SimpleModel(tf.keras.Model):
  def __init__(self, params, *args, **kwargs):
    super(SimpleModel, self).__init__(*args, **kwargs)
    # define the model
    self.dense_1 = tf.keras.layers.Dense(params['layer_1_size'])
    self.dense_2 = tf.keras.layers.Dense(params['layer_2_size'])
    self.out = tf.keras.layers.Dense(params['num_classes'])
    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(
      initial_learning_rate=params['init_lr'],
      decay_steps=params['decay_steps'],
      end_learning_rate=params['end_lr'],
      power=params['lr_power'])  
    self.optimizer = tf.keras.optimizers.legacy.SGD(learning_rate_fn)
    self.compiled_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
    self.logs = {
        'lr': [],
        'loss': [],
        'grads': [],
        'weights': [],
        'layer_out': []}

  def call(self, inputs):
    out_1 = self.dense_1(inputs)
    out_2 = self.dense_2(out_1)
    logits = self.out(out_2)
    # log output features for every layer for comparison
    layer_wise_out = {
        'layer_1': out_1,
        'layer_2': out_2,
        'logits': logits}
    self.logs['layer_out'].append(layer_wise_out)
    return logits

  def train_step(self, data):
    x, y = data
    with tf.GradientTape() as tape:
      logits = self(x)
      loss = self.compiled_loss(y, logits)
    grads = tape.gradient(loss, self.trainable_weights)
    # log training statistics
    step = self.optimizer.iterations.numpy()
    self.logs['lr'].append(self.optimizer.learning_rate(step).numpy())
    self.logs['loss'].append(loss.numpy())
    self.logs['grads'].append(grads)
    self.logs['weights'].append(self.trainable_weights)
    # update model
    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
    return

----------------------------------------

TITLE: Defining Basic Extension Types in TensorFlow
DESCRIPTION: Examples of defining basic extension types using tf.experimental.ExtensionType. Shows how to create custom types for graphs, masked tensors, and sparse matrices.

LANGUAGE: Python
CODE:
class TensorGraph(tf.experimental.ExtensionType):
  """A collection of labeled nodes connected by weighted edges."""
  edge_weights: tf.Tensor               # shape=[num_nodes, num_nodes]
  node_labels: Mapping[str, tf.Tensor]  # shape=[num_nodes]; dtype=any

class MaskedTensor(tf.experimental.ExtensionType):
  """A tensor paired with a boolean mask, indicating which values are valid."""
  values: tf.Tensor
  mask: tf.Tensor       # shape=values.shape; false for missing/invalid values.

class CSRSparseMatrix(tf.experimental.ExtensionType):
  """Compressed sparse row matrix (https://en.wikipedia.org/wiki/Sparse_matrix)."""
  values: tf.Tensor     # shape=[num_nonzero]; dtype=any
  col_index: tf.Tensor  # shape=[num_nonzero]; dtype=int64
  row_index: tf.Tensor  # shape=[num_rows+1]; dtype=int64

----------------------------------------

TITLE: Importing TensorFlow for Gradient Computation
DESCRIPTION: Basic TensorFlow import statement for compatibility with version 1.x

LANGUAGE: Python
CODE:
import tensorflow.compat.v1 as tf

----------------------------------------

TITLE: Enabling TF-NumPy Type Promotion
DESCRIPTION: Shows how to enable the new type promotion behavior in TensorFlow with the ALL mode for type conversions.

LANGUAGE: Python
CODE:
tnp.experimental_enable_numpy_behavior(dtype_conversion_mode="all")

----------------------------------------

TITLE: Cloning TensorFlow Models Repository
DESCRIPTION: Commands to download and access the Iris classification example code from TensorFlow's models repository.

LANGUAGE: shell
CODE:
git clone https://github.com/tensorflow/models/
cd models/samples/core/get_started

----------------------------------------

TITLE: Making Predictions with a Trained TensorFlow Estimator
DESCRIPTION: Shows how to use a trained Estimator model to make predictions on new, unlabeled data.

LANGUAGE: python
CODE:
# Generate predictions from the model
expected = ['Setosa', 'Versicolor', 'Virginica']
predict_x = {
    'SepalLength': [5.1, 5.9, 6.9],
    'SepalWidth': [3.3, 3.0, 3.1],
    'PetalLength': [1.7, 4.2, 5.4],
    'PetalWidth': [0.5, 1.5, 2.1],
}

predictions = classifier.predict(
    input_fn=lambda:iris_data.eval_input_fn(predict_x,
                                            batch_size=args.batch_size))

----------------------------------------

TITLE: Importing Libraries for Bird Vocalization Analysis
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Hub, and audio processing libraries.

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_io as tfio

import numpy as np
import librosa

import csv
import io

from IPython.display import Audio

----------------------------------------

TITLE: Defining an Artificial Dataset Class
DESCRIPTION: Creates a custom TensorFlow dataset class that simulates data loading with artificial delays.

LANGUAGE: Python
CODE:
class ArtificialDataset(tf.data.Dataset):
    def _generator(num_samples):
        # Opening the file
        time.sleep(0.03)
        
        for sample_idx in range(num_samples):
            # Reading data (line, record) from the file
            time.sleep(0.015)
            
            yield (sample_idx,)
    
    def __new__(cls, num_samples=3):
        return tf.data.Dataset.from_generator(
            cls._generator,
            output_signature = tf.TensorSpec(shape = (1,), dtype = tf.int64),
            args=(num_samples,)
        )

----------------------------------------

TITLE: Defining an Artificial Dataset Class
DESCRIPTION: Creates a custom TensorFlow dataset class that simulates data loading with artificial delays.

LANGUAGE: Python
CODE:
class ArtificialDataset(tf.data.Dataset):
    def _generator(num_samples):
        # Opening the file
        time.sleep(0.03)
        
        for sample_idx in range(num_samples):
            # Reading data (line, record) from the file
            time.sleep(0.015)
            
            yield (sample_idx,)
    
    def __new__(cls, num_samples=3):
        return tf.data.Dataset.from_generator(
            cls._generator,
            output_signature = tf.TensorSpec(shape = (1,), dtype = tf.int64),
            args=(num_samples,)
        )

----------------------------------------

TITLE: Implementing Training Input Function with TensorFlow Dataset API
DESCRIPTION: Demonstrates how to create an input function for training using TensorFlow's Dataset API, including shuffling and batching.

LANGUAGE: python
CODE:
def train_input_fn(features, labels, batch_size):
    """An input function for training"""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle, repeat, and batch the examples.
    return dataset.shuffle(1000).repeat().batch(batch_size)

----------------------------------------

TITLE: Downloading Stack Overflow Dataset
DESCRIPTION: Downloads the Stack Overflow dataset using tf.keras.utils.get_file and explores the directory structure.

LANGUAGE: Python
CODE:
data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'

dataset_dir = utils.get_file(
    origin=data_url,
    untar=True,
    cache_dir='stack_overflow',
    cache_subdir='')

dataset_dir = pathlib.Path(dataset_dir).parent

----------------------------------------

TITLE: Calculating AllReduce Communication Time
DESCRIPTION: Formula to estimate the expected time for gradient AllReduce operation in distributed training based on model parameters and communication bandwidth.

LANGUAGE: Python
CODE:
(number of parameters * 4bytes)/ (communication bandwidth)

----------------------------------------

TITLE: Running Image Classification with Python
DESCRIPTION: This snippet demonstrates how to run image classification using the Inception-v3 model with Python. It shows the command to execute the classification script and the expected output format.

LANGUAGE: bash
CODE:
cd models/tutorials/image/imagenet\npython classify_image.py

----------------------------------------

TITLE: Loading I3D Model from TensorFlow Hub
DESCRIPTION: Loads the pre-trained I3D model for action recognition from TensorFlow Hub.

LANGUAGE: Python
CODE:
i3d = hub.load("https://tfhub.dev/deepmind/i3d-kinetics-400/1").signatures['default']

----------------------------------------

TITLE: Installing TensorFlow Dependencies with pip
DESCRIPTION: Installs the required Python dependencies for building TensorFlow from source.

LANGUAGE: Batch
CODE:
pip3 install -U pip
pip3 install -U six numpy wheel packaging
pip3 install -U keras_preprocessing --no-deps

----------------------------------------

TITLE: Installing Pre-commit Hooks
DESCRIPTION: Commands to install the pre-commit framework and configure hooks for notebook formatting.

LANGUAGE: shell
CODE:
# Install pre-commit framework
$ pip3 install pre-commit

# Install hooks
$ pre-commit install

----------------------------------------

TITLE: Basic TensorFlow Input Pipeline Implementation
DESCRIPTION: Demonstrates a basic implementation of an input pipeline using tf.data API with parse function for TFExample records and data augmentation.

LANGUAGE: python
CODE:
def parse_fn(example):
  "Parse TFExample records and perform simple data augmentation."
  example_fmt = {
    "image": tf.FixedLengthFeature((), tf.string, ""),
    "label": tf.FixedLengthFeature((), tf.int64, -1)
  }
  parsed = tf.parse_single_example(example, example_fmt)
  image = tf.image.decode_image(parsed["image"])
  image = _augment_helper(image)  # augments image using slice, reshape, resize_bilinear
  return image, parsed["label"]

def input_fn():
  files = tf.data.Dataset.list_files("/path/to/dataset/train-*.tfrecord")
  dataset = files.interleave(tf.data.TFRecordDataset)
  dataset = dataset.shuffle(buffer_size=FLAGS.shuffle_buffer_size)
  dataset = dataset.map(map_func=parse_fn)
  dataset = dataset.batch(batch_size=FLAGS.batch_size)
  return dataset

----------------------------------------

TITLE: Loading and Using Image Feature Vector Model
DESCRIPTION: Example showing how to load a SavedModel for image feature extraction and process a batch of images to obtain feature vectors. The model accepts image tensors and outputs feature vectors.

LANGUAGE: python
CODE:
obj = hub.load("path/to/model")  # That's tf.saved_model.load() after download.
images = ...  # A batch of images with shape [batch_size, height, width, 3].
features = obj(images)   # A batch with shape [batch_size, num_features].

----------------------------------------

TITLE: Setting Up DeterministicRandomTestTool
DESCRIPTION: Initializes the DeterministicRandomTestTool for consistent random operations across TF1 and TF2.

LANGUAGE: Python
CODE:
random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')

----------------------------------------

TITLE: TF2 Custom Training Loop with Early Stopping
DESCRIPTION: Implementation of early stopping in a custom training loop using gradient tape

LANGUAGE: Python
CODE:
@tf.function
def train_step(x, y):
  with tf.GradientTape() as tape:
      logits = model(x, training=True)
      loss_value = loss_fn(y, logits)
  grads = tape.gradient(loss_value, model.trainable_weights)
  optimizer.apply_gradients(zip(grads, model.trainable_weights))
  train_acc_metric.update_state(y, logits)
  train_loss_metric.update_state(y, logits)
  return loss_value

@tf.function
def test_step(x, y):
  logits = model(x, training=False)
  val_acc_metric.update_state(y, logits)
  val_loss_metric.update_state(y, logits)

----------------------------------------

TITLE: Extracting BARD Dataset
DESCRIPTION: Unzips the downloaded BARD dataset file.

LANGUAGE: Bash
CODE:
unzip -qo bard.zip

----------------------------------------

TITLE: Loading TensorFlow Hub Model with Custom IO Device in Python
DESCRIPTION: This code demonstrates how to load a TensorFlow Hub model with custom LoadOptions to redirect all reads through the Colab host when running on TPUs.

LANGUAGE: python
CODE:
load_options =
tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')
reloaded_model = hub.load("https://tfhub.dev/...", options=load_options)

----------------------------------------

TITLE: Loading BAIR Test Videos in Python
DESCRIPTION: This code loads a batch of test videos from the BAIR dataset and extracts input frames for processing.

LANGUAGE: python
CODE:
batch_size = 16

ds = builder.as_dataset(split="test")
test_videos = ds.batch(batch_size)
first_batch = next(iter(test_videos))
input_frames = first_batch['image_aux1'][:, ::15]
input_frames = tf.cast(input_frames, tf.float32)

----------------------------------------

TITLE: Configuring nblint with Repository Arguments
DESCRIPTION: Example of using nblint with repository-specific arguments for link checking.

LANGUAGE: shell
CODE:
$ python3 -m tensorflow_docs.tools.nblint --arg=repo:tensorflow/docs notebook.ipynb

----------------------------------------

TITLE: TF1 Hub Module Initialization and Inference
DESCRIPTION: Demonstrates how to initialize and use a TF1 Hub module with placeholders in a request-response pattern.

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow_hub as hub

# Create graph and finalize (finalizing optional but recommended).
g = tf.Graph()
with g.as_default():
  # We will be feeding 1D tensors of text into the graph.
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/2")
  embedded_text = embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()

# Create session and initialize.
session = tf.Session(graph=g)
session.run(init_op)

LANGUAGE: python
CODE:
result = session.run(embedded_text, feed_dict={text_input: ["Hello world"]})

----------------------------------------

TITLE: Setting Up TensorFlow 2 Environment
DESCRIPTION: Installs TensorFlow 2.9 to ensure compatibility with the DeterministicRandomTestTool used in the notebook.

LANGUAGE: Python
CODE:
!pip install -q "tensorflow==2.9.*"

----------------------------------------

TITLE: TensorFlow 1 Estimator Configuration
DESCRIPTION: Configures a TF1 DNNClassifier with feature columns and input functions for training and evaluation

LANGUAGE: python
CODE:
feature_columns = [tf1.feature_column.numeric_column("x", shape=[28, 28])]

classifier = tf1.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[256, 32],
    optimizer=tf1.train.AdamOptimizer(0.001),
    n_classes=10,
    dropout=0.2
)

train_input_fn = tf1.estimator.inputs.numpy_input_fn(
    x={"x": x_train},
    y=y_train.astype(np.int32),
    num_epochs=10,
    batch_size=50,
    shuffle=True,
)

test_input_fn = tf1.estimator.inputs.numpy_input_fn(
    x={"x": x_test},
    y=y_test.astype(np.int32),
    num_epochs=10,
    shuffle=False
)

train_spec = tf1.estimator.TrainSpec(input_fn=train_input_fn, max_steps=10)
eval_spec = tf1.estimator.EvalSpec(input_fn=test_input_fn,
                                   steps=10,
                                   throttle_secs=0)

----------------------------------------

TITLE: Importing TensorFlow and Required Libraries
DESCRIPTION: Imports necessary Python libraries including TensorFlow, pandas, and scikit-learn for data processing and model building.

LANGUAGE: Python
CODE:
import numpy as np
import pandas as pd

import tensorflow as tf

from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

----------------------------------------

TITLE: Defining Input and Model Functions for TPUEstimator
DESCRIPTION: Define input functions for training and evaluation data, and a model function for TPUEstimator in TensorFlow 1.

LANGUAGE: Python
CODE:
def _input_fn(params):
  dataset = tf1.data.Dataset.from_tensor_slices((features, labels))
  dataset = dataset.repeat()
  return dataset.batch(params['batch_size'], drop_remainder=True)

def _eval_input_fn(params):
  dataset = tf1.data.Dataset.from_tensor_slices((eval_features, eval_labels))
  dataset = dataset.repeat()
  return dataset.batch(params['batch_size'], drop_remainder=True)

def _model_fn(features, labels, mode, params):
  logits = tf1.layers.Dense(1)(features)
  loss = tf1.losses.mean_squared_error(labels=labels, predictions=logits)
  optimizer = tf1.train.AdagradOptimizer(0.05)
  train_op = optimizer.minimize(loss, global_step=tf1.train.get_global_step())
  return tf1.estimator.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op)

----------------------------------------

TITLE: Using hub.KerasLayer in Sequential Model
DESCRIPTION: Example of incorporating a TensorFlow Hub model as a layer in a Keras Sequential model using hub.KerasLayer.

LANGUAGE: python
CODE:
model = tf.keras.Sequential([
    hub.KerasLayer(handle),
    ...])

----------------------------------------

TITLE: Running a TensorFlow Model on Random Input
DESCRIPTION: This snippet demonstrates how to run a TensorFlow model on a single batch of random data and inspect the output. It generates random input, passes it through the model, and prints various statistics about the result.

LANGUAGE: python
CODE:
result = model(tf.constant(np.random.randn(10,5), dtype = tf.float32)).numpy()

print("min:", result.min())
print("max:", result.max())
print("mean:", result.mean())
print("shape:", result.shape)

----------------------------------------

TITLE: Initializing DNNClassifier with Model Directory
DESCRIPTION: Creates a DNNClassifier instance with specified model directory for checkpoint storage.

LANGUAGE: python
CODE:
classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    hidden_units=[10, 10],
    n_classes=3,
    model_dir='models/iris')

----------------------------------------

TITLE: Using and Inspecting Keras Layers
DESCRIPTION: Shows how to use a layer by calling it, and how to inspect layer variables using various attributes.

LANGUAGE: Python
CODE:
layer(tf.zeros([10, 5]))
layer.variables
layer.kernel, layer.bias

----------------------------------------

TITLE: Importing TensorFlow in Python
DESCRIPTION: Imports the TensorFlow library for use in Python code.

LANGUAGE: Python
CODE:
import tensorflow as tf

----------------------------------------

TITLE: Initializing Sample Training Data
DESCRIPTION: Creating sample feature and label datasets for demonstration purposes

LANGUAGE: python
CODE:
features = [[1., 1.5], [2., 2.5], [3., 3.5]]
labels = [[0.3], [0.5], [0.7]]
eval_features = [[4., 4.5], [5., 5.5], [6., 6.5]]
eval_labels = [[0.8], [0.9], [1.]]

----------------------------------------

TITLE: Profiling with Context Manager
DESCRIPTION: Use context manager to profile a specific code block.

LANGUAGE: Python
CODE:
with tf.profiler.experimental.Profile('logdir'):
    # Train the model here
    pass

----------------------------------------

TITLE: Using Text Embedding Feature Columns in TensorFlow Estimator
DESCRIPTION: This code shows how to use text embedding feature columns with a TensorFlow Estimator. It creates a feature column using a Hub module, sets up an input function, and trains a DNNClassifier with the text embeddings.

LANGUAGE: python
CODE:
feature_columns = [
  hub.text_embedding_column("comment", "path/to/module", trainable=False),
]
input_fn = tf.estimator.inputs.numpy_input_fn(features, labels, shuffle=True)
estimator = tf.estimator.DNNClassifier(hidden_units, feature_columns)
estimator.train(input_fn, max_steps=100)

----------------------------------------

TITLE: Setting Environment Variables for Build
DESCRIPTION: Sets up environment variables to resolve common build issues.

LANGUAGE: Batch
CODE:
set PATH=path/to/python;%PATH%
set PATH=path/to/python/Scripts;%PATH%
set PYTHON_BIN_PATH=path/to/python_virtualenv/Scripts/python.exe
set PYTHON_LIB_PATH=path/to/python virtualenv/lib/site-packages
set PYTHON_DIRECTORY=path/to/python_virtualenv/Scripts

----------------------------------------

TITLE: Verifying NVIDIA Docker Installation
DESCRIPTION: Command to verify the NVIDIA Docker installation by running the nvidia-smi command within a CUDA-enabled container.

LANGUAGE: bash
CODE:
docker run --gpus all --rm nvidia/cuda nvidia-smi

----------------------------------------

TITLE: Running the TensorFlow RNN Tutorial for Drawing Classification
DESCRIPTION: Command to execute the tutorial code for training the RNN-based model. It specifies the paths for training data, evaluation data, and classes file.

LANGUAGE: Shell
CODE:
python train_model.py \
  --training_data=rnn_tutorial_data/training.tfrecord-?????-of-????? \
  --eval_data=rnn_tutorial_data/eval.tfrecord-?????-of-????? \
  --classes_file=rnn_tutorial_data/training.tfrecord.classes

----------------------------------------

TITLE: Manual Device Placement in TensorFlow
DESCRIPTION: Shows how to manually assign operations to specific devices using tf.device context manager.

LANGUAGE: python
CODE:
# Creates a graph.
with tf.device('/cpu:0'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))

----------------------------------------

TITLE: TensorFlow 1 Multiple GPU Training Implementation
DESCRIPTION: Complete implementation of multi-GPU training using tf.estimator.Estimator with MirroredStrategy in TensorFlow 1

LANGUAGE: python
CODE:
def _input_fn():
  return tf1.data.Dataset.from_tensor_slices((features, labels)).batch(1)

def _eval_input_fn():
  return tf1.data.Dataset.from_tensor_slices(
      (eval_features, eval_labels)).batch(1)

def _model_fn(features, labels, mode):
  logits = tf1.layers.Dense(1)(features)
  loss = tf1.losses.mean_squared_error(labels=labels, predictions=logits)
  optimizer = tf1.train.AdagradOptimizer(0.05)
  train_op = optimizer.minimize(loss, global_step=tf1.train.get_global_step())
  return tf1.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

strategy = tf1.distribute.MirroredStrategy()
config = tf1.estimator.RunConfig(
    train_distribute=strategy, eval_distribute=strategy)
estimator = tf1.estimator.Estimator(model_fn=_model_fn, config=config)

train_spec = tf1.estimator.TrainSpec(input_fn=_input_fn)
eval_spec = tf1.estimator.EvalSpec(input_fn=_eval_input_fn)
tf1.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

----------------------------------------

TITLE: Updating local repository from upstream
DESCRIPTION: Commands to update the local repository with changes from the upstream TensorFlow repository. This ensures the contributor is working with the latest code.

LANGUAGE: Bash
CODE:
$ git checkout master
$ git pull upstream master
$ git push origin master

----------------------------------------

TITLE: Demonstrating tf.print vs print Inside tf.function
DESCRIPTION: Shows the difference between using tf.print and print inside a tf.function for debugging.

LANGUAGE: Python
CODE:
@tf.function
def dummy_func(num):
  num += 1
  print(num)
  tf.print(num)
  return num

_ = dummy_func(tf.constant([1.0]))

# Output:
# Tensor("add:0", shape=(1,), dtype=float32)
# [2]

----------------------------------------

TITLE: Auto-fixing Lint Issues
DESCRIPTION: Command to automatically fix supported lint issues in a notebook.

LANGUAGE: shell
CODE:
$ python3 -m tensorflow_docs.tools.nblint --fix \
    --arg=repo:tensorflow/docs notebook.ipynb

----------------------------------------

TITLE: Creating Text Embeddings with TensorFlow Hub Module in Python
DESCRIPTION: This snippet demonstrates how to use a TensorFlow Hub module to create text embeddings. It loads a module and applies it to a batch of text inputs, generating dense vector representations.

LANGUAGE: python
CODE:
embed = hub.Module("path/to/module")
representations = embed([
    "A long sentence.",
    "single-word",
    "http://example.com"])

----------------------------------------

TITLE: Wrapping TensorFlow Session with TFDBG
DESCRIPTION: Example showing how to wrap a TensorFlow session with the debugger wrapper class

LANGUAGE: python
CODE:
from tensorflow.python import debug as tf_debug

sess = tf_debug.LocalCLIDebugWrapperSession(sess)

----------------------------------------

TITLE: Basic TensorFlow Setup and Utilities
DESCRIPTION: Initial setup code importing TensorFlow and defining utility functions for directory management and a simple add_two function.

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow.compat.v1 as tf1
import shutil

def remove_dir(path):
  try:
    shutil.rmtree(path)
  except:
    pass

def add_two(input):
  return input + 2

----------------------------------------

TITLE: Demonstrating tracing behavior of tf.function
DESCRIPTION: Shows how to detect when a tf.function is being traced by adding a print statement to the function body.

LANGUAGE: Python
CODE:
@tf.function
def a_function_with_python_side_effect(x):
  print("Tracing!") # An eager-only side effect.
  return x * x + tf.constant(2)

# This is traced the first time.
print(a_function_with_python_side_effect(tf.constant(2)))
# The second time through, you won't see the side effect.
print(a_function_with_python_side_effect(tf.constant(3)))

# This retraces each time the Python argument changes,
# as a Python argument could be an epoch count or other
# hyperparameter.
print(a_function_with_python_side_effect(2))
print(a_function_with_python_side_effect(3))

----------------------------------------

TITLE: Creating GraphDef Object in Python
DESCRIPTION: Initializes an empty GraphDef object using the protobuf-generated class. This object will be used to store the graph definition loaded from disk.

LANGUAGE: python
CODE:
graph_def = graph_pb2.GraphDef()

----------------------------------------

TITLE: Multi-Worker Training with tf.estimator APIs in TensorFlow 1
DESCRIPTION: Illustrates the canonical workflow for multi-worker training in TensorFlow 1 using tf.estimator.Estimator, tf.estimator.TrainSpec, tf.estimator.EvalSpec, and tf.estimator.train_and_evaluate APIs.

LANGUAGE: Python
CODE:
def _input_fn():
  return tf1.data.Dataset.from_tensor_slices((features, labels)).batch(1)

def _eval_input_fn():
  return tf1.data.Dataset.from_tensor_slices(
      (eval_features, eval_labels)).batch(1)

def _model_fn(features, labels, mode):
  logits = tf1.layers.Dense(1)(features)
  loss = tf1.losses.mean_squared_error(labels=labels, predictions=logits)
  optimizer = tf1.train.AdagradOptimizer(0.05)
  train_op = optimizer.minimize(loss, global_step=tf1.train.get_global_step())
  return tf1.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

estimator = tf1.estimator.Estimator(model_fn=_model_fn)
train_spec = tf1.estimator.TrainSpec(input_fn=_input_fn)
eval_spec = tf1.estimator.EvalSpec(input_fn=_eval_input_fn)
tf1.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

----------------------------------------

TITLE: Linear Regression Model Implementation
DESCRIPTION: Implements a linear regression model as a TensorFlow Module with randomly initialized weights and bias.

LANGUAGE: python
CODE:
class LinearRegression(tf.Module):

  def __init__(self):
    self.built = False

  @tf.function
  def __call__(self, x):
    # Initialize the model parameters on the first call
    if not self.built:
      # Randomly generate the weight vector and bias term
      rand_w = tf.random.uniform(shape=[x.shape[-1], 1])
      rand_b = tf.random.uniform(shape=[])
      self.w = tf.Variable(rand_w)
      self.b = tf.Variable(rand_b)
      self.built = True
    y = tf.add(tf.matmul(x, self.w), self.b)
    return tf.squeeze(y, axis=1)

----------------------------------------

TITLE: Installing TensorFlow Compression
DESCRIPTION: Installs the latest version of TensorFlow Compression compatible with the installed TensorFlow version using pip.

LANGUAGE: Bash
CODE:
read MAJOR MINOR <<< "$(pip show tensorflow | perl -p -0777 -e 's/.*Version: (\d+)\.(\d+).*/\1 \2/sg')"
pip install "tensorflow-compression<$MAJOR.$(($MINOR+1))"


----------------------------------------

TITLE: Creating CentralStorageStrategy for Multi-GPU Training in TensorFlow
DESCRIPTION: Creates a CentralStorageStrategy instance for synchronous training. Variables are placed on the CPU, and operations are replicated across all local GPUs.

LANGUAGE: Python
CODE:
central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()

----------------------------------------

TITLE: Fine-tuning SavedModel - Python
DESCRIPTION: Shows how to perform basic fine-tuning on a loaded SavedModel using gradient tape and optimization.

LANGUAGE: python
CODE:
optimizer = tf.keras.optimizers.SGD(0.05)

def train_step():
  with tf.GradientTape() as tape:
    loss = (10. - imported(tf.constant(2.))) ** 2
  variables = tape.watched_variables()
  grads = tape.gradient(loss, variables)
  optimizer.apply_gradients(zip(grads, variables))
  return loss

----------------------------------------

TITLE: Generating TensorFlow 2 API Reference Docs
DESCRIPTION: Series of commands to clone the TensorFlow repo, install TensorFlow, and generate API reference documentation for TensorFlow 2.

LANGUAGE: bash
CODE:
git clone https://github.com/tensorflow/tensorflow tensorflow
cd tensorflow/tensorflow/tools/docs
pip install tensorflow
python generate2.py --output_dir=/tmp/out

----------------------------------------

TITLE: Configuring Remote Storage Reading in Python
DESCRIPTION: This code snippet demonstrates how to configure TensorFlow Hub to read models directly from remote storage (GCS) instead of downloading them locally.

LANGUAGE: shell
CODE:
os.environ["TFHUB_MODEL_LOAD_FORMAT"] = "UNCOMPRESSED"

----------------------------------------

TITLE: Generating Synthetic Training Data
DESCRIPTION: Creates synthetic training data by adding Gaussian noise to points along a line defined by true weight and bias values.

LANGUAGE: Python
CODE:
TRUE_W = 3.0
TRUE_B = 2.0

NUM_EXAMPLES = 201

x = tf.linspace(-2,2, NUM_EXAMPLES)
x = tf.cast(x, tf.float32)

def f(x):
  return x * TRUE_W + TRUE_B

noise = tf.random.normal(shape=[NUM_EXAMPLES])

y = f(x) + noise

----------------------------------------

TITLE: Generating Interpolated Video Frames in Python
DESCRIPTION: This code uses the loaded TensorFlow Hub module to generate interpolated frames between the start and end frames of the input videos.

LANGUAGE: python
CODE:
filled_frames = module(input_frames)['default'] / 255.0

----------------------------------------

TITLE: Importing Dependencies for CORD-19 Embedding Analysis
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Hub, and data visualization tools. Disables eager execution and sets up a helper function to display DataFrames.

LANGUAGE: python
CODE:
import functools
import itertools
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd

import tensorflow.compat.v1 as tf
tf.disable_eager_execution()
tf.logging.set_verbosity('ERROR')

import tensorflow_datasets as tfds
import tensorflow_hub as hub

try:
  from google.colab import data_table
  def display_df(df):
    return data_table.DataTable(df, include_index=False)
except ModuleNotFoundError:
  # If google-colab is not available, just display the raw DataFrame
  def display_df(df):
    return df

----------------------------------------

TITLE: Creating a Keras Functional Model
DESCRIPTION: Demonstrates how to create a Keras model using the functional API.

LANGUAGE: Python
CODE:
inputs = tf.keras.Input(shape=[3,])

x = FlexibleDense(3)(inputs)
x = FlexibleDense(2)(x)

my_functional_model = tf.keras.Model(inputs=inputs, outputs=x)

my_functional_model.summary()

----------------------------------------

TITLE: Specifying GPUs for MirroredStrategy in TensorFlow
DESCRIPTION: Creates a MirroredStrategy instance using only specific GPUs. This allows fine-grained control over which devices are used for distributed training.

LANGUAGE: Python
CODE:
mirrored_strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"])

----------------------------------------

TITLE: Installing TensorFlow on Windows Native with GPU Support
DESCRIPTION: Commands to install TensorFlow with GPU support on Windows using Conda and pip, for versions before 2.11.

LANGUAGE: bash
CODE:
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
python -m pip install "tensorflow<2.11"
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"

----------------------------------------

TITLE: Distributed Inference with Loaded Model
DESCRIPTION: Load the saved model and perform distributed inference using a strategy.

LANGUAGE: Python
CODE:
another_strategy = tf.distribute.MirroredStrategy()
with another_strategy.scope():
  loaded = tf.saved_model.load(saved_model_path)
  inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]

  dist_predict_dataset = another_strategy.experimental_distribute_dataset(
      predict_dataset)

  # Calling the function in a distributed manner
  for batch in dist_predict_dataset:
    result = another_strategy.run(inference_func, args=(batch,))
    print(result)
    break

----------------------------------------

TITLE: Distributed Inference with Loaded Model
DESCRIPTION: Load the saved model and perform distributed inference using a strategy.

LANGUAGE: Python
CODE:
another_strategy = tf.distribute.MirroredStrategy()
with another_strategy.scope():
  loaded = tf.saved_model.load(saved_model_path)
  inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]

  dist_predict_dataset = another_strategy.experimental_distribute_dataset(
      predict_dataset)

  # Calling the function in a distributed manner
  for batch in dist_predict_dataset:
    result = another_strategy.run(inference_func, args=(batch,))
    print(result)
    break

----------------------------------------

TITLE: Committing Changes to TensorFlow Docs
DESCRIPTION: Git commands to view changes, stage files, and commit changes to the documentation.

LANGUAGE: bash
CODE:
git status
git diff
git add <path/to/file.md>
git commit -m "Your meaningful commit message for the change."

----------------------------------------

TITLE: Video Processing Utility Functions
DESCRIPTION: Defines helper functions for video processing including center cropping, loading videos from URLs, and displaying video results with scores.

LANGUAGE: python
CODE:
def crop_center_square(frame):
  y, x = frame.shape[0:2]
  min_dim = min(y, x)
  start_x = (x // 2) - (min_dim // 2)
  start_y = (y // 2) - (min_dim // 2)
  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]

def load_video(video_url, max_frames=32, resize=(224, 224)):
  path = tf.keras.utils.get_file(os.path.basename(video_url)[-128:], video_url)
  cap = cv2.VideoCapture(path)
  frames = []
  try:
    while True:
      ret, frame = cap.read()
      if not ret:
        break
      frame = crop_center_square(frame)
      frame = cv2.resize(frame, resize)
      frame = frame[:, :, [2, 1, 0]]
      frames.append(frame)

      if len(frames) == max_frames:
        break
  finally:
    cap.release()
  frames = np.array(frames)
  if len(frames) < max_frames:
    n_repeat = int(math.ceil(max_frames / float(len(frames))))
    frames = frames.repeat(n_repeat, axis=0)
  frames = frames[:max_frames]
  return frames / 255.0

----------------------------------------

TITLE: Video Processing Utility Functions
DESCRIPTION: Defines helper functions for video processing including center cropping, loading videos from URLs, and displaying video results with scores.

LANGUAGE: python
CODE:
def crop_center_square(frame):
  y, x = frame.shape[0:2]
  min_dim = min(y, x)
  start_x = (x // 2) - (min_dim // 2)
  start_y = (y // 2) - (min_dim // 2)
  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]

def load_video(video_url, max_frames=32, resize=(224, 224)):
  path = tf.keras.utils.get_file(os.path.basename(video_url)[-128:], video_url)
  cap = cv2.VideoCapture(path)
  frames = []
  try:
    while True:
      ret, frame = cap.read()
      if not ret:
        break
      frame = crop_center_square(frame)
      frame = cv2.resize(frame, resize)
      frame = frame[:, :, [2, 1, 0]]
      frames.append(frame)

      if len(frames) == max_frames:
        break
  finally:
    cap.release()
  frames = np.array(frames)
  if len(frames) < max_frames:
    n_repeat = int(math.ceil(max_frames / float(len(frames))))
    frames = frames.repeat(n_repeat, axis=0)
  frames = frames[:max_frames]
  return frames / 255.0

----------------------------------------

TITLE: Markdown Header Configuration
DESCRIPTION: Basic markdown header defining the project name placeholder for a TensorFlow examples repository.

LANGUAGE: markdown
CODE:
# PROJECT_NAME examples

----------------------------------------

TITLE: Compiling and Running Image Classification with C++
DESCRIPTION: This snippet shows how to compile and run the C++ version of the image classification program using the Inception-v3 model. It includes commands for downloading the model, compiling the binary, and executing it.

LANGUAGE: bash
CODE:
curl -L "https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz" |\n  tar -C tensorflow/examples/label_image/data -xz\n\nbazel build tensorflow/examples/label_image/...\n\nbazel-bin/tensorflow/examples/label_image/label_image

----------------------------------------

TITLE: Importing Dependencies for CVAE Implementation
DESCRIPTION: Imports necessary libraries including TensorFlow, TensorFlow Probability, and visualization tools.

LANGUAGE: Python
CODE:
from IPython import display

import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf
import tensorflow_probability as tfp
import time

----------------------------------------

TITLE: Training DNNClassifier Model
DESCRIPTION: Demonstrates how to train a DNNClassifier model with specified batch size and steps.

LANGUAGE: python
CODE:
classifier.train(
    input_fn=lambda: train_input_fn(train_x, train_y, batch_size=100),
    steps=200)

----------------------------------------

TITLE: Manual Device Placement for TPU Computation in TensorFlow
DESCRIPTION: This snippet demonstrates manual device placement to perform matrix multiplication on a single TPU device.

LANGUAGE: Python
CODE:
a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

with tf.device('/TPU:0'):
  c = tf.matmul(a, b)

print("c device: ", c.device)
print(c)

----------------------------------------

TITLE: Installing TensorFlow Compression
DESCRIPTION: Installs the latest version of TensorFlow Compression compatible with the installed TensorFlow version using pip.

LANGUAGE: bash
CODE:
read MAJOR MINOR <<< "$(pip show tensorflow | perl -p -0777 -e 's/.*Version: (\d+)\.(\d+).*/\1 \2/sg')"
pip install "tensorflow-compression<$MAJOR.$(($MINOR+1))"

----------------------------------------

TITLE: Splitting the Dataset into Train, Validation, and Test Sets
DESCRIPTION: Split the DataFrame into training, validation, and test sets using an 80:10:10 ratio.

LANGUAGE: Python
CODE:
train, val, test = np.split(dataframe.sample(frac=1), [int(0.8*len(dataframe)), int(0.9*len(dataframe))])

----------------------------------------

TITLE: Instantiating DNNClassifier Estimator in TensorFlow
DESCRIPTION: This code creates a Deep Neural Network Classifier Estimator with specified feature columns, hidden layers, and number of classes for the Iris classification problem.

LANGUAGE: Python
CODE:
# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.
classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    # Two hidden layers of 30 and 10 nodes respectively.
    hidden_units=[30, 10],
    # The model must choose between 3 classes.
    n_classes=3)

----------------------------------------

TITLE: Main Function for Image Classification in C++
DESCRIPTION: This is the main function of the C++ image classification program. It ties together the process of loading the model, processing the image, running inference, and printing the results.

LANGUAGE: C++
CODE:
int main(int argc, char* argv[]) {\n  tensorflow::port::InitMain(argv[0], &argc, &argv);\n  Status s = tensorflow::ParseCommandLineFlags(&argc, argv);\n  if (!s.ok()) {\n    LOG(ERROR) << "Error parsing command line flags: " << s.ToString();\n    return -1;\n  }\n\n  std::unique_ptr<tensorflow::Session> session;\n  string graph_path = tensorflow::io::JoinPath(FLAGS_root_dir, FLAGS_graph);\n  Status load_graph_status = LoadGraph(graph_path, &session);\n  // ... (additional code for image processing and inference)\n  Status print_status = PrintTopLabels(outputs, FLAGS_labels);\n  if (!print_status.ok()) {\n    LOG(ERROR) << "Running print failed: " << print_status;\n    return -1;\n  }\n}

----------------------------------------

TITLE: Creating Basic TensorFlow Datasets
DESCRIPTION: Example showing how to create and inspect basic TensorFlow datasets using tf.data.Dataset

LANGUAGE: python
CODE:
dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))
print(dataset1.output_types)  # ==> "tf.float32"
print(dataset1.output_shapes)  # ==> "(10,)"

dataset2 = tf.data.Dataset.from_tensor_slices(
   (tf.random_uniform([4]),
    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))
print(dataset2.output_types)  # ==> "(tf.float32, tf.int32)"
print(dataset2.output_shapes)  # ==> "((), (100,))"

dataset3 = tf.data.Dataset.zip((dataset1, dataset2))
print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))
print(dataset3.output_shapes)  # ==> "(10, ((), (100,)))"

----------------------------------------

TITLE: Saving and Loading Keras Model Weights
DESCRIPTION: Demonstrates how to save and load the weights of a Keras model.

LANGUAGE: Python
CODE:
# Save weights to a TensorFlow Checkpoint file
model.save_weights('./weights/my_model')

# Restore the model's state,
# this requires a model with the same architecture.
model.load_weights('./weights/my_model')

# Save weights to a HDF5 file
model.save_weights('my_model.h5', save_format='h5')

# Restore the model's state
model.load_weights('my_model.h5')

----------------------------------------

TITLE: Loading and Processing Image Data
DESCRIPTION: Shows how to load image files, decode them and resize to a fixed size using tf.data pipeline.

LANGUAGE: Python
CODE:
def parse_image(filename):
  parts = tf.strings.split(filename, os.sep)
  label = parts[-2]

  image = tf.io.read_file(filename)
  image = tf.io.decode_jpeg(image)
  image = tf.image.convert_image_dtype(image, tf.float32)
  image = tf.image.resize(image, [128, 128])
  return image, label

----------------------------------------

TITLE: Configuring nblint with Multiple Styles
DESCRIPTION: Example of using nblint with multiple style modules and repository arguments.

LANGUAGE: shell
CODE:
$ python3 -m tensorflow_docs.tools.nblint \
    --styles=tensorflow,tensorflow_docs_l10n --arg=repo:tensorflow/docs-1l0n \
    notebook.ipynb

----------------------------------------

TITLE: Recording Runtime Statistics in TensorFlow Training Loop
DESCRIPTION: This code snippet shows how to record summaries and runtime statistics during a TensorFlow training loop, including execution stats every 100 steps.

LANGUAGE: python
CODE:
  def feed_dict(train):
    """Make a TensorFlow feed_dict: maps data onto Tensor placeholders."""
    if train or FLAGS.fake_data:
      xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)
      k = FLAGS.dropout
    else:
      xs, ys = mnist.test.images, mnist.test.labels
      k = 1.0
    return {x: xs, y_: ys, keep_prob: k}

  for i in range(FLAGS.max_steps):
    if i % 10 == 0:  # Record summaries and test-set accuracy
      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))
      test_writer.add_summary(summary, i)
      print('Accuracy at step %s: %s' % (i, acc))
    else:  # Record train set summaries, and train
      if i % 100 == 99:  # Record execution stats
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        summary, _ = sess.run([merged, train_step],
                              feed_dict=feed_dict(True),
                              options=run_options,
                              run_metadata=run_metadata)
        train_writer.add_run_metadata(run_metadata, 'step%d' % i)
        train_writer.add_summary(summary, i)
        print('Adding run metadata for', i)
      else:  # Record a summary
        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))
        train_writer.add_summary(summary, i)

----------------------------------------

TITLE: Defining Custom Model Architecture
DESCRIPTION: Implements a custom CNN model using Keras Model subclassing API with convolutional and dense layers.

LANGUAGE: python
CODE:
class MyModel(Model):
  def __init__(self):
    super().__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
    self.d2 = Dense(10)

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)

# Create an instance of the model
model = MyModel()

----------------------------------------

TITLE: Text Processing Helper Function
DESCRIPTION: Defines a utility function to process sentences with SentencePiece processor and return results in sparse tensor format

LANGUAGE: python
CODE:
def process_to_IDs_in_sparse_format(sp, sentences):
  # An utility method that processes sentences with the sentence piece processor
  # 'sp' and returns the results in tf.SparseTensor-similar format:
  # (values, indices, dense_shape)
  ids = [sp.EncodeAsIds(x) for x in sentences]
  max_len = max(len(x) for x in ids)
  dense_shape=(len(ids), max_len)
  values=[item for sublist in ids for item in sublist]
  indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]
  return (values, indices, dense_shape)

----------------------------------------

TITLE: Installing TensorFlow on Linux with GPU Support
DESCRIPTION: Commands to install TensorFlow with GPU support on Linux systems using pip, including verification steps.

LANGUAGE: bash
CODE:
python3 -m pip install 'tensorflow[and-cuda]'
python3 -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"

----------------------------------------

TITLE: Installing Dependencies for Universal Sentence Encoder SentEval Demo
DESCRIPTION: This code installs the required dependencies for the demo, including tensorflow-text and torch libraries.

LANGUAGE: bash
CODE:
pip install --quiet "tensorflow-text==2.11.*"
pip install --quiet torch==1.8.1

----------------------------------------

TITLE: Compiling a Keras Model
DESCRIPTION: Demonstrates how to compile a Keras model by specifying the optimizer, loss function, and metrics.

LANGUAGE: Python
CODE:
model = tf.keras.Sequential([
# Adds a densely-connected layer with 64 units to the model:
layers.Dense(64, activation='relu', input_shape=(32,)),
# Add another:
layers.Dense(64, activation='relu'),
# Add a softmax layer with 10 output units:
layers.Dense(10, activation='softmax')])

model.compile(optimizer=tf.train.AdamOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

----------------------------------------

TITLE: Loading the Fashion MNIST Dataset
DESCRIPTION: This code loads the Fashion MNIST dataset, which contains 70,000 grayscale images of clothing items in 10 categories.

LANGUAGE: Python
CODE:
fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

----------------------------------------

TITLE: Initializing TensorFlow Environment
DESCRIPTION: Basic imports required for TensorFlow programming including numpy and tensorflow modules.

LANGUAGE: python
CODE:
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import tensorflow as tf

----------------------------------------

TITLE: Configuring Maven Dependencies for TensorFlow with GPU Support
DESCRIPTION: Maven dependency configuration for TensorFlow with GPU support, including both core library and GPU-enabled JNI bindings.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.tensorflow</groupId>
  <artifactId>libtensorflow</artifactId>
  <version>2.4.0</version>
</dependency>
<dependency>
  <groupId>org.tensorflow</groupId>
  <artifactId>libtensorflow_jni_gpu</artifactId>
  <version>2.4.0</version>
</dependency>

----------------------------------------

TITLE: Training and Evaluating Estimator with MirroredStrategy in TensorFlow
DESCRIPTION: Shows how to train and evaluate an Estimator using MirroredStrategy. An input function is defined to provide data, and the model is trained for 10 steps.

LANGUAGE: Python
CODE:
def input_fn():
  dataset = tf.data.Dataset.from_tensors(({"feats":[1.]}, [1.]))
  return dataset.repeat(1000).batch(10)
regressor.train(input_fn=input_fn, steps=10)
regressor.evaluate(input_fn=input_fn, steps=10)

----------------------------------------

TITLE: Data Preparation and Feature Columns Setup
DESCRIPTION: Load and preprocess the Titanic dataset and create feature columns for models

LANGUAGE: python
CODE:
x_train = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')
x_eval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')
x_train['sex'].replace(('male', 'female'), (0, 1), inplace=True)
x_eval['sex'].replace(('male', 'female'), (0, 1), inplace=True)

x_train['alone'].replace(('n', 'y'), (0, 1), inplace=True)
x_eval['alone'].replace(('n', 'y'), (0, 1), inplace=True)

x_train['class'].replace(('First', 'Second', 'Third'), (1, 2, 3), inplace=True)
x_eval['class'].replace(('First', 'Second', 'Third'), (1, 2, 3), inplace=True)

x_train.drop(['embark_town', 'deck'], axis=1, inplace=True)
x_eval.drop(['embark_town', 'deck'], axis=1, inplace=True)

y_train = x_train.pop('survived')
y_eval = x_eval.pop('survived')

----------------------------------------

TITLE: Running TensorFlow 1.x Model in Graph Mode
DESCRIPTION: Executes the TensorFlow 1.x model in graph mode and collects statistics for the first 3 training steps.

LANGUAGE: Python
CODE:
with random_tool.scope():
  graph = tf.Graph()
  with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:
    model_tf1 = SimpleModelWrapper()
    # build the model
    inputs = tf.compat.v1.placeholder(tf.float32, shape=(None, params['input_size']))
    labels = tf.compat.v1.placeholder(tf.float32, shape=(None, params['num_classes']))
    spec = model_tf1.model_fn(inputs, labels, tf.estimator.ModeKeys.TRAIN, params)
    train_op = spec.train_op

    sess.run(tf.compat.v1.global_variables_initializer())
    for step in range(step_num):
      # log everything and update the model for one step
      logs, _ = sess.run(
          [model_tf1.logged_ops, train_op],
          feed_dict={inputs: fake_x, labels: fake_y})
      model_tf1.update_logs(logs)

----------------------------------------

TITLE: Environment Setup and Package Installation
DESCRIPTION: Installs required Python packages including TensorFlow, simpleneighbors, NLTK, and tqdm

LANGUAGE: python
CODE:
%%capture
#@title Setup Environment
# Install the latest Tensorflow version.
!pip install -q "tensorflow-text==2.11.*"
!pip install -q simpleneighbors[annoy]
!pip install -q nltk
!pip install -q tqdm

----------------------------------------

TITLE: Training a Keras Model with NumPy Data
DESCRIPTION: Shows how to train a Keras model using NumPy arrays as input data and labels.

LANGUAGE: Python
CODE:
import numpy as np

def random_one_hot_labels(shape):
  n, n_class = shape
  classes = np.random.randint(0, n_class, n)
  labels = np.zeros((n, n_class))
  labels[np.arange(n), classes] = 1
  return labels

data = np.random.random((1000, 32))
labels = random_one_hot_labels((1000, 10))

model.fit(data, labels, epochs=10, batch_size=32)

----------------------------------------

TITLE: Importing TensorFlow and Setting Up AUTOTUNE
DESCRIPTION: Imports TensorFlow and sets up the AUTOTUNE constant for automatic tuning of parallel calls.

LANGUAGE: Python
CODE:
import tensorflow.compat.v1 as tf

tf.__version__

AUTOTUNE = tf.data.AUTOTUNE

----------------------------------------

TITLE: Formatting Jupyter Notebooks for TensorFlow Docs
DESCRIPTION: Command to run the nbfmt tool for formatting Jupyter notebooks according to TensorFlow docs standards.

LANGUAGE: bash
CODE:
python3 -m tensorflow_docs.tools.nbfmt [options] notebook.ipynb [...]

----------------------------------------

TITLE: Running tf_upgrade_v2 on a Single File
DESCRIPTION: Demonstrates how to use the tf_upgrade_v2 script to upgrade a single Python file from TensorFlow 1.x to 2.x.

LANGUAGE: bash
CODE:
tf_upgrade_v2 \
  --infile models/samples/cookbook/regression/custom_regression.py \
  --outfile /tmp/custom_regression_v2.py

----------------------------------------

TITLE: Installing and Testing TensorFlow Hub Package
DESCRIPTION: Commands to install the built wheel file and test the installation

LANGUAGE: shell
CODE:
pip install /tmp/tensorflow_hub_pkg/*.whl

LANGUAGE: python
CODE:
python -c "import tensorflow_hub as hub"

----------------------------------------

TITLE: GPU Device Placement and Performance Testing
DESCRIPTION: Demonstrates explicit device placement for tensor operations and compares performance between CPU and GPU execution.

LANGUAGE: python
CODE:
import time

def time_matmul(x):
  start = time.time()
  for loop in range(10):
    tf.matmul(x, x)

  result = time.time()-start

  print("10 loops: {:0.2f}ms".format(1000*result))


# Force execution on CPU
print("On CPU:")
with tf.device("CPU:0"):
  x = tf.random_uniform([1000, 1000])
  assert x.device.endswith("CPU:0")
  time_matmul(x)

# Force execution on GPU #0 if available
if tf.config.list_physical_devices('GPU'):
  with tf.device("GPU:0"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.
    x = tf.random_uniform([1000, 1000])
    assert x.device.endswith("GPU:0")
    time_matmul(x)

----------------------------------------

TITLE: Including Intermediate Outputs in Model for Debugging
DESCRIPTION: Shows how to include intermediate layer outputs in the model's outputs for debugging purposes.

LANGUAGE: Python
CODE:
def get_model():
  inputs = tf.keras.layers.Input(shape=(1,))
  out_1 = tf.keras.layers.Dense(4)(inputs)
  out_2 = tf.keras.layers.Dense(1)(out_1)
  # include intermediate values in model outputs
  model = tf.keras.Model(
      inputs=inputs,
      outputs={
          'inputs': inputs,
          'out_1': out_1,
          'out_2': out_2})
  return model

----------------------------------------

TITLE: Evaluating and Predicting with a Keras Model
DESCRIPTION: Shows how to evaluate a Keras model and make predictions using both NumPy arrays and tf.data.Dataset.

LANGUAGE: Python
CODE:
data = np.random.random((1000, 32))
labels = random_one_hot_labels((1000, 10))

model.evaluate(data, labels, batch_size=32)

model.evaluate(dataset, steps=30)

result = model.predict(data, batch_size=32)
print(result.shape)

----------------------------------------

TITLE: Vectorizing Mapping Operations
DESCRIPTION: Demonstrates how to vectorize mapping operations by applying them to batches of data instead of individual elements.

LANGUAGE: Python
CODE:
fast_benchmark(
    fast_dataset
    .batch(256)
    # Apply function on a batch of items
    # The tf.Tensor.__add__ method already handle batches
    .map(increment)
)

----------------------------------------

TITLE: Vectorizing Mapping Operations
DESCRIPTION: Demonstrates how to vectorize mapping operations by applying them to batches of data instead of individual elements.

LANGUAGE: Python
CODE:
fast_benchmark(
    fast_dataset
    .batch(256)
    # Apply function on a batch of items
    # The tf.Tensor.__add__ method already handle batches
    .map(increment)
)

----------------------------------------

TITLE: Installing Python Virtualenv on Linux
DESCRIPTION: Command to install Python virtualenv package using apt-get

LANGUAGE: shell
CODE:
sudo apt-get install python-virtualenv

----------------------------------------

TITLE: Creating and Training TPUEstimator
DESCRIPTION: Sets up TPUClusterResolver, RunConfig, and TPUEstimator for training and evaluation on TPUs.

LANGUAGE: python
CODE:
cluster_resolver = tf1.distribute.cluster_resolver.TPUClusterResolver(tpu='')
print("All devices: ", tf1.config.list_logical_devices('TPU'))

tpu_config = tf1.estimator.tpu.TPUConfig(
    iterations_per_loop=10,
    per_host_input_for_training=tf1.estimator.tpu.InputPipelineConfig
          .PER_HOST_V2)
config = tf1.estimator.tpu.RunConfig(
    cluster=cluster_resolver,
    save_checkpoints_steps=None,
    tpu_config=tpu_config)
estimator = tf1.estimator.tpu.TPUEstimator(
    model_fn=_model_fn, config=config, train_batch_size=8, eval_batch_size=8,
    embedding_config_spec=embedding_config_spec)

estimator.train(_input_fn, steps=1)
estimator.evaluate(_eval_input_fn, steps=1)

----------------------------------------

TITLE: Installing TensorFlow Examples Package
DESCRIPTION: Installs the tensorflow_examples package to enable importing of the generator and discriminator models.

LANGUAGE: Python
CODE:
!pip install git+https://github.com/tensorflow/examples.git

----------------------------------------

TITLE: Creating Training Examples and Targets
DESCRIPTION: Prepare the dataset by creating input sequences and corresponding target sequences.

LANGUAGE: Python
CODE:
# The maximum length sentence you want for a single input in characters
seq_length = 100
examples_per_epoch = len(text)//seq_length

# Create training examples / targets
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

sequences = char_dataset.batch(seq_length+1, drop_remainder=True)

def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

dataset = sequences.map(split_input_target)

----------------------------------------

TITLE: Broadcasting with Ragged Tensors
DESCRIPTION: Demonstrates broadcasting rules and examples for operations involving ragged tensors.

LANGUAGE: python
CODE:
# x       (2D ragged):  2 x (num_rows)
# y       (scalar)
# result  (2D ragged):  2 x (num_rows)
x = tf.ragged.constant([[1, 2], [3]])
y = 3
print(x + y)

# x         (2d ragged):  3 x (num_rows)
# y         (2d tensor):  3 x          1
# Result    (2d ragged):  3 x (num_rows)
x = tf.ragged.constant(
   [[10, 87, 12],
    [19, 53],
    [12, 32]])
y = [[1000], [2000], [3000]]
print(x + y)

# x      (3d ragged):  2 x (r1) x 2
# y      (2d ragged):         1 x 1
# Result (3d ragged):  2 x (r1) x 2
x = tf.ragged.constant(
    [[[1, 2], [3, 4], [5, 6]],
     [[7, 8]]],
    ragged_rank=1)
y = tf.constant([[10]])
print(x + y)

----------------------------------------

TITLE: Basic TPUEstimator Local Setup
DESCRIPTION: Shows how to create a TPUEstimator for local testing by setting use_tpu=False and providing a basic RunConfig.

LANGUAGE: python
CODE:
my_tpu_estimator = tf.estimator.tpu.TPUEstimator(
    model_fn=my_model_fn,
    config=tf.estimator.tpu.RunConfig(),
    use_tpu=False)

----------------------------------------

TITLE: Training Linear Model in TensorFlow
DESCRIPTION: Complete example showing how to create, train and evaluate a simple linear model using TensorFlow layers and optimizers.

LANGUAGE: python
CODE:
x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)
y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)

linear_model = tf.layers.Dense(units=1)

y_pred = linear_model(x)
loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)

optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)
for i in range(100):
  _, loss_value = sess.run((train, loss))
  print(loss_value)

print(sess.run(y_pred))

----------------------------------------

TITLE: Using Neural Network Libraries in Python
DESCRIPTION: Shows various TensorFlow modules used for creating and training neural network models in Python.

LANGUAGE: Python
CODE:
tf.train, tf.nn, tf.contrib.layers, tf.contrib.slim

----------------------------------------

TITLE: Configuring TensorFlow Imports and Environment
DESCRIPTION: Basic setup code that imports required libraries and verifies TensorFlow version and eager execution status

LANGUAGE: Python
CODE:
import os
import matplotlib.pyplot as plt

import tensorflow.compat.v1 as tf

print("TensorFlow version: {}".format(tf.__version__))
print("Eager execution: {}".format(tf.executing_eagerly()))

----------------------------------------

TITLE: Verifying Numerical Equivalence in Inference Mode
DESCRIPTION: Compare the outputs and regularization losses of TF1 and TF2 Inception ResNet V2 models in inference mode using DeterministicRandomTestTool.

LANGUAGE: Python
CODE:
random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')
with random_tool.scope():
  graph = tf.Graph()
  with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:
    height, width = 299, 299
    num_classes = 1000
    inputs = tf.ones( (1, height, width, 3))

    out, endpoints = inception_resnet_v2(inputs, num_classes, is_training=False)

    sess.run(tf.compat.v1.global_variables_initializer())

    reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)
    tf1_regularization_loss = sess.run(tf.math.add_n(reg_losses))
    tf1_output = sess.run(out)

# TF2 model
random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')
with random_tool.scope():
  model = InceptionResnetV2(num_classes)
  inputs = tf.ones((1, height, width, 3))
  tf2_output, endpoints = model(inputs, training=False)
  tf2_regularization_loss = tf.math.add_n(model.losses)

# Verify equivalence
np.testing.assert_allclose(tf1_regularization_loss, tf2_regularization_loss.numpy(), rtol=1e-06, atol=1e-05)
np.testing.assert_allclose(tf1_output, tf2_output.numpy(), rtol=1e-06, atol=1e-05)

----------------------------------------

TITLE: Importing TensorFlow and Basic Setup
DESCRIPTION: Initial setup code importing TensorFlow v1 compatibility mode and math library

LANGUAGE: python
CODE:
import math
import tensorflow.compat.v1 as tf

----------------------------------------

TITLE: Image Download and Resize Function
DESCRIPTION: Helper function to download images from URLs and resize them to specified dimensions using LANCZOS interpolation

LANGUAGE: python
CODE:
def download_and_resize(name, url, new_width=256, new_height=256):
  path = tf.keras.utils.get_file(url.split('/')[-1], url)
  image = Image.open(path)
  image = ImageOps.fit(image, (new_width, new_height), Image.LANCZOS)
  return image

----------------------------------------

TITLE: Creating and Switching to a New Git Branch
DESCRIPTION: Git commands to create a new branch for making documentation changes.

LANGUAGE: bash
CODE:
git checkout -b <feature-name>
git branch

----------------------------------------

TITLE: Cloning TensorFlow Hub Repository
DESCRIPTION: Git commands to clone the TensorFlow Hub repository

LANGUAGE: shell
CODE:
git clone https://github.com/tensorflow/hub
cd hub

----------------------------------------

TITLE: Creating MirroredStrategy for Distributed Training
DESCRIPTION: Initializes a MirroredStrategy object to handle distribution of training across multiple GPUs on a single machine.

LANGUAGE: Python
CODE:
strategy = tf.distribute.MirroredStrategy()

----------------------------------------

TITLE: Importing TensorFlow
DESCRIPTION: Imports TensorFlow v1 compatibility mode for the tutorial

LANGUAGE: python
CODE:
import tensorflow.compat.v1 as tf

----------------------------------------

TITLE: Defining TF1 Inception ResNet V2 Forward Pass
DESCRIPTION: Define the TensorFlow 1.x forward pass for the Inception ResNet V2 model using slim layers.

LANGUAGE: Python
CODE:
def inception_resnet_v2(inputs, num_classes, is_training):
  with slim.arg_scope(
    inception.inception_resnet_v2_arg_scope(batch_norm_scale=True)):
    return inception.inception_resnet_v2(inputs, num_classes, is_training=is_training)

----------------------------------------

TITLE: Setting up TensorFlow Imports
DESCRIPTION: Imports required TensorFlow libraries and disables eager execution which is necessary for multi-worker mirrored strategy with estimators.

LANGUAGE: Python
CODE:
import tensorflow_datasets as tfds
import tensorflow as tf

import os, json

tf.compat.v1.disable_eager_execution()

----------------------------------------

TITLE: Implementing Basic Training Input Function in TensorFlow
DESCRIPTION: A function that converts features and labels into a TensorFlow Dataset, applies shuffling, repeating, and batching operations for training.

LANGUAGE: python
CODE:
def train_input_fn(features, labels, batch_size):
    """An input function for training"""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)

    # Return the dataset.
    return dataset

----------------------------------------

TITLE: Visualizing Object Detection Results
DESCRIPTION: Visualizes the object detection results by drawing bounding boxes, labels, and optionally keypoints on the input image.

LANGUAGE: python
CODE:
label_id_offset = 0
image_np_with_detections = image_np.copy()

# Use keypoints if available in detections
keypoints, keypoint_scores = None, None
if 'detection_keypoints' in result:
  keypoints = result['detection_keypoints'][0]
  keypoint_scores = result['detection_keypoint_scores'][0]

viz_utils.visualize_boxes_and_labels_on_image_array(
      image_np_with_detections[0],
      result['detection_boxes'][0],
      (result['detection_classes'][0] + label_id_offset).astype(int),
      result['detection_scores'][0],
      category_index,
      use_normalized_coordinates=True,
      max_boxes_to_draw=200,
      min_score_thresh=.30,
      agnostic_mode=False,
      keypoints=keypoints,
      keypoint_scores=keypoint_scores,
      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)

plt.figure(figsize=(24,32))
plt.imshow(image_np_with_detections[0])
plt.show()

----------------------------------------

TITLE: Using Legacy TF1 Models with KerasLayer
DESCRIPTION: Example of loading and using a legacy TF1 Hub format model with the new hub.KerasLayer API in TF2.

LANGUAGE: python
CODE:
m = hub.KerasLayer(handle)
tensor_out = m(tensor_in)

----------------------------------------

TITLE: Defining Data Preprocessing and Batch Size
DESCRIPTION: Sets up constants for data preprocessing, including buffer size and batch size calculation based on available devices.

LANGUAGE: Python
CODE:
num_train_examples = ds_info.splits['train'].num_examples
num_test_examples = ds_info.splits['test'].num_examples

BUFFER_SIZE = 10000

BATCH_SIZE_PER_REPLICA = 64
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

----------------------------------------

TITLE: Training Model Function
DESCRIPTION: Defines a function to compile and train a model, returning the validation accuracy.

LANGUAGE: python
CODE:
def train_model(model, training_data, validation_data, **kwargs):
  model.compile(
      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
  )
  kwargs.setdefault("epochs", 5)
  kwargs.setdefault("verbose", 1)
  log = model.fit(
      training_data.batch(128).prefetch(8),
      validation_data=validation_data.batch(128).cache(),
      validation_freq=1,
      **kwargs,
  )
  return log.history["val_sparse_categorical_accuracy"][-1]

----------------------------------------

TITLE: Creating TensorFlow API Documentation Build Script in Python
DESCRIPTION: Instructions for creating a build_docs.py script that uses the api_generator API to generate TensorFlow API reference documentation. The script should be implemented using the tools provided in the tensorflow_docs/api_generator module.

LANGUAGE: python
CODE:
# Create a build_docs.py script using the api_generator API:
# https://github.com/tensorflow/docs/tree/master/tools/tensorflow_docs/api_generator

----------------------------------------

TITLE: Configuring TensorFlow 1 Estimator with Fault Tolerance
DESCRIPTION: Sets up a TensorFlow 1 Estimator with RunConfig to save checkpoints at every step, enabling fault tolerance.

LANGUAGE: python
CODE:
feature_columns = [tf1.feature_column.numeric_column("x", shape=[28, 28])]
config = tf1.estimator.RunConfig(save_summary_steps=1,
                                 save_checkpoints_steps=1)

path = tempfile.mkdtemp()

classifier = tf1.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[256, 32],
    optimizer=tf1.train.AdamOptimizer(0.001),
    n_classes=10,
    dropout=0.2,
    model_dir=path,
    config = config
)

train_input_fn = tf1.estimator.inputs.numpy_input_fn(
    x={"x": x_train},
    y=y_train.astype(np.int32),
    num_epochs=10,
    batch_size=50,
    shuffle=True,
)

----------------------------------------

TITLE: Avoiding feed_dict for Better Performance in TensorFlow
DESCRIPTION: This snippet demonstrates the use of feed_dict, which is not recommended for optimal performance in TensorFlow. It's provided as an example of what to avoid in production code.

LANGUAGE: python
CODE:
sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

----------------------------------------

TITLE: Creating TensorFlow 1.x model formats
DESCRIPTION: Creates TF1 SavedModel, Keras model, and frozen GraphDef for conversion examples.

LANGUAGE: Python
CODE:
# Create a TF1 SavedModel
SAVED_MODEL_DIR = "tf_saved_model/"
remove_dir(SAVED_MODEL_DIR)
with tf1.Graph().as_default() as g:
  with tf1.Session() as sess:
    input = tf1.placeholder(tf.float32, shape=(3,), name='input')
    output = input + 2
    tf1.saved_model.simple_save(
        sess, SAVED_MODEL_DIR,
        inputs={'input': input}, 
        outputs={'output': output})
print("TF1 SavedModel path: ", SAVED_MODEL_DIR)

# Create a TF1 Keras model
KERAS_MODEL_PATH = 'tf_keras_model.h5'
model = tf1.keras.models.Sequential([
    tf1.keras.layers.InputLayer(input_shape=(128, 128, 3,), name='input'),
    tf1.keras.layers.Dense(units=16, input_shape=(128, 128, 3,), activation='relu'),
    tf1.keras.layers.Dense(units=1, name='output')
])
model.save(KERAS_MODEL_PATH, save_format='h5')
print("TF1 Keras Model path: ", KERAS_MODEL_PATH)

# Create a TF1 frozen GraphDef model
GRAPH_DEF_MODEL_PATH = tf.keras.utils.get_file(
    'mobilenet_v1_0.25_128',
    origin='https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_0.25_128_frozen.tgz',
    untar=True,
) + '/frozen_graph.pb'

print("TF1 frozen GraphDef path: ", GRAPH_DEF_MODEL_PATH)

----------------------------------------

TITLE: Creating TensorFlow 1.x model formats
DESCRIPTION: Creates TF1 SavedModel, Keras model, and frozen GraphDef for conversion examples.

LANGUAGE: Python
CODE:
# Create a TF1 SavedModel
SAVED_MODEL_DIR = "tf_saved_model/"
remove_dir(SAVED_MODEL_DIR)
with tf1.Graph().as_default() as g:
  with tf1.Session() as sess:
    input = tf1.placeholder(tf.float32, shape=(3,), name='input')
    output = input + 2
    tf1.saved_model.simple_save(
        sess, SAVED_MODEL_DIR,
        inputs={'input': input}, 
        outputs={'output': output})
print("TF1 SavedModel path: ", SAVED_MODEL_DIR)

# Create a TF1 Keras model
KERAS_MODEL_PATH = 'tf_keras_model.h5'
model = tf1.keras.models.Sequential([
    tf1.keras.layers.InputLayer(input_shape=(128, 128, 3,), name='input'),
    tf1.keras.layers.Dense(units=16, input_shape=(128, 128, 3,), activation='relu'),
    tf1.keras.layers.Dense(units=1, name='output')
])
model.save(KERAS_MODEL_PATH, save_format='h5')
print("TF1 Keras Model path: ", KERAS_MODEL_PATH)

# Create a TF1 frozen GraphDef model
GRAPH_DEF_MODEL_PATH = tf.keras.utils.get_file(
    'mobilenet_v1_0.25_128',
    origin='https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_0.25_128_frozen.tgz',
    untar=True,
) + '/frozen_graph.pb'

print("TF1 frozen GraphDef path: ", GRAPH_DEF_MODEL_PATH)

----------------------------------------

TITLE: Loading and Preprocessing MNIST Dataset
DESCRIPTION: Loads the MNIST dataset, normalizes pixel values, and adds a channels dimension for CNN processing.

LANGUAGE: python
CODE:
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# Add a channels dimension
x_train = x_train[..., tf.newaxis].astype("float32")
x_test = x_test[..., tf.newaxis].astype("float32")

----------------------------------------

TITLE: Configuring Estimator for Remote Devices
DESCRIPTION: Configuration setup for using TF2 SavedModel in an Estimator with parameter servers or remote devices, enabling shared cluster devices in session.

LANGUAGE: python
CODE:
session_config = tf.compat.v1.ConfigProto()
session_config.experimental.share_cluster_devices_in_session = True
run_config = tf.estimator.RunConfig(..., session_config=session_config)
estimator = tf.estimator.Estimator(..., config=run_config)

----------------------------------------

TITLE: Defining Custom Training and Testing Steps
DESCRIPTION: Implements custom training and testing step functions within TPU strategy scope, including gradient calculations and metric updates.

LANGUAGE: python
CODE:
with strategy.scope():
  def train_step(inputs):
    x, y = inputs
    logits, loss_value, grads = grad(model, x, y)
    update_loss = training_loss.update_state(loss_value)
    update_accuracy = training_accuracy.update_state(y, logits)
    grads = grads * 2
    update_vars = optimizer.apply_gradients(zip(grads, model.trainable_variables))
    with tf.control_dependencies([update_vars, update_loss, update_accuracy]):
      return tf.identity(loss_value)

  def test_step(inputs):
    x, y = inputs
    logits, loss_value = loss(model, x, y)
    update_loss = test_loss.update_state(loss_value)
    update_accuracy = test_accuracy.update_state(y, logits)
    with tf.control_dependencies([update_loss, update_accuracy]):
      return tf.identity(loss_value)

----------------------------------------

TITLE: Defining Power Law Regularizer
DESCRIPTION: Creates a custom regularizer class that applies a power law entropy penalty to model weights.

LANGUAGE: python
CODE:
class PowerLawRegularizer(tf.keras.regularizers.Regularizer):

  def __init__(self, lmbda):
    super().__init__()
    self.lmbda = lmbda

  def __call__(self, variable):
    em = tfc.PowerLawEntropyModel(coding_rank=variable.shape.rank)
    return self.lmbda * em.penalty(variable)

regularizer = PowerLawRegularizer(lmbda=2./classifier.count_params())

----------------------------------------

TITLE: Text Embedding with Keras Layer
DESCRIPTION: Shows how to use text embeddings within a Keras model using hub.KerasLayer wrapper.

LANGUAGE: python
CODE:
embeddings = hub.KerasLayer("path/to/model", trainable=...)(text_input)

----------------------------------------

TITLE: Loading and Processing Iris Dataset
DESCRIPTION: Downloads the Iris training dataset and creates a tf.data.Dataset pipeline for training

LANGUAGE: Python
CODE:
train_dataset_url = "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv"

train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),
                                           origin=train_dataset_url)

print("Local copy of the dataset file: {}".format(train_dataset_fp))

----------------------------------------

TITLE: Loading and Processing Iris Dataset
DESCRIPTION: Downloads the Iris training dataset and creates a tf.data.Dataset pipeline for training

LANGUAGE: Python
CODE:
train_dataset_url = "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv"

train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),
                                           origin=train_dataset_url)

print("Local copy of the dataset file: {}".format(train_dataset_fp))

----------------------------------------

TITLE: Generating Multimodal Distribution Data in Python
DESCRIPTION: Creates histogram summaries with bimodal distribution by combining two normal distributions with different parameters. One has a moving mean while the other has shrinking variance.

LANGUAGE: python
CODE:
import tensorflow as tf

k = tf.placeholder(tf.float32)

# Make a normal distribution, with a shifting mean
mean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)
# Record that distribution into a histogram summary
tf.summary.histogram("normal/moving_mean", mean_moving_normal)

# Make a normal distribution with shrinking variance
variance_shrinking_normal = tf.random_normal(shape=[1000], mean=0, stddev=1-(k))
# Record that distribution too
tf.summary.histogram("normal/shrinking_variance", variance_shrinking_normal)

# Let's combine both of those distributions into one dataset
normal_combined = tf.concat([mean_moving_normal, variance_shrinking_normal], 0)
# We add another histogram summary to record the combined distribution
tf.summary.histogram("normal/bimodal", normal_combined)

summaries = tf.summary.merge_all()

# Setup a session and summary writer
sess = tf.Session()
writer = tf.summary.FileWriter("/tmp/histogram_example")

# Setup a loop and write the summaries to disk
N = 400
for step in range(N):
  k_val = step/float(N)
  summ = sess.run(summaries, feed_dict={k: k_val})
  writer.add_summary(summ, global_step=step)

----------------------------------------

TITLE: Loading Auto MPG dataset using pandas in Python
DESCRIPTION: Downloads the Auto MPG dataset from UCI Machine Learning Repository and loads it into a pandas DataFrame.

LANGUAGE: Python
CODE:
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',
                'Acceleration', 'Model Year', 'Origin']

raw_dataset = pd.read_csv(url, names=column_names,
                          na_values='?', comment='\t',
                          sep=' ', skipinitialspace=True)

----------------------------------------

TITLE: TF2 SavedModel Initialization and Inference
DESCRIPTION: Shows how to properly initialize and use a TF2 SavedModel for inference in a request-response pattern.

LANGUAGE: python
CODE:
import tensorflow_hub as hub

embedding_fn = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

LANGUAGE: python
CODE:
embedding_fn(["Hello world"])

----------------------------------------

TITLE: DeepDream Model Implementation
DESCRIPTION: Core DeepDream class that implements gradient ascent to enhance patterns in images

LANGUAGE: Python
CODE:
class DeepDream(tf.Module):
  def __init__(self, model):
    self.model = model

  @tf.function(
      input_signature=(
        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),
        tf.TensorSpec(shape=[], dtype=tf.int32),
        tf.TensorSpec(shape=[], dtype=tf.float32),)
  )
  def __call__(self, img, steps, step_size):
      print("Tracing")
      loss = tf.constant(0.0)
      for n in tf.range(steps):
        with tf.GradientTape() as tape:
          tape.watch(img)
          loss = calc_loss(img, self.model)

        gradients = tape.gradient(loss, img)
        gradients /= tf.math.reduce_std(gradients) + 1e-8 
        img = img + gradients*step_size
        img = tf.clip_by_value(img, -1, 1)

      return loss, img

----------------------------------------

TITLE: Defining Helper Functions for UCF101 Dataset
DESCRIPTION: Defines utility functions for fetching and processing videos from the UCF101 dataset, including video listing, downloading, and preprocessing.

LANGUAGE: Python
CODE:
UCF_ROOT = "https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/"
_VIDEO_LIST = None
_CACHE_DIR = tempfile.mkdtemp()
unverified_context = ssl._create_unverified_context()

def list_ucf_videos():
    # Function implementation

def fetch_ucf_video(video):
    # Function implementation

def crop_center_square(frame):
    # Function implementation

def load_video(path, max_frames=0, resize=(224, 224)):
    # Function implementation

def to_gif(images):
    # Function implementation

----------------------------------------

TITLE: Implementing PosixWritableFile in C++ for TensorFlow
DESCRIPTION: Example implementation of the WritableFile interface for the POSIX filesystem. It provides methods for appending data, closing, flushing, and syncing a file using standard POSIX functions.

LANGUAGE: C++
CODE:
class PosixWritableFile : public WritableFile {
 public:
  PosixWritableFile(const string& fname, FILE* f)
      : filename_(fname), file_(f) {}

  ~PosixWritableFile() override {
    if (file_ != NULL) {
      fclose(file_);
    }
  }

  Status Append(const StringPiece& data) override {
    size_t r = fwrite(data.data(), 1, data.size(), file_);
    if (r != data.size()) {
      return IOError(filename_, errno);
    }
    return Status::OK();
  }

  Status Close() override {
    Status result;
    if (fclose(file_) != 0) {
      result = IOError(filename_, errno);
    }
    file_ = NULL;
    return result;
  }

  Status Flush() override {
    if (fflush(file_) != 0) {
      return IOError(filename_, errno);
    }
    return Status::OK();
  }

  Status Sync() override {
    Status s;
    if (fflush(file_) != 0) {
      s = IOError(filename_, errno);
    }
    return s;
  }

 private:
  string filename_;
  FILE* file_;
};

----------------------------------------

TITLE: Saving and Loading Keras Models
DESCRIPTION: Shows how to save a Keras model to a .keras file and load it back.

LANGUAGE: Python
CODE:
my_sequential_model.save("exname_of_file.keras")

reconstructed_model = tf.keras.models.load_model("exname_of_file.keras")

reconstructed_model(tf.constant([[2.0, 2.0, 2.0]]))

----------------------------------------

TITLE: Loading Titanic Dataset in Python
DESCRIPTION: Loads the Titanic dataset from CSV files and prepares training and evaluation data.

LANGUAGE: Python
CODE:
dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')
dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')
y_train = dftrain.pop('survived')
y_eval = dfeval.pop('survived')

----------------------------------------

TITLE: Training the Model
DESCRIPTION: This code trains the model on the training data for 5 epochs.

LANGUAGE: Python
CODE:
model.fit(train_images, train_labels, epochs=5)

----------------------------------------

TITLE: Downloading Compressed TensorFlow Model using wget
DESCRIPTION: Shell command demonstrating how to download a compressed TensorFlow model from tfhub.dev using wget with the tf-hub-format parameter.

LANGUAGE: shell
CODE:
wget https://tfhub.dev/tensorflow/albert_en_xxlarge/1?tf-hub-format=compressed

----------------------------------------

TITLE: Serializing tf.Example Messages
DESCRIPTION: Function to create and serialize tf.Example messages with multiple features

LANGUAGE: python
CODE:
def serialize_example(feature0, feature1, feature2, feature3):
  feature = {
      'feature0': _int64_feature(feature0),
      'feature1': _int64_feature(feature1),
      'feature2': _bytes_feature(feature2),
      'feature3': _float_feature(feature3),
  }
  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
  return example_proto.SerializeToString()

----------------------------------------

TITLE: Writing TFRecord File in TensorFlow
DESCRIPTION: Shows how to write tf.train.Example messages to a TFRecord file using tf.io.TFRecordWriter.

LANGUAGE: Python
CODE:
filename = 'test.tfrecord'

# Write the `tf.train.Example` observations to the file.
with tf.io.TFRecordWriter(filename) as writer:
  for i in range(n_observations):
    example = serialize_example(feature0[i], feature1[i], feature2[i], feature3[i])
    writer.write(example.numpy())

----------------------------------------

TITLE: Creating Text Classification Dataset with TensorFlow
DESCRIPTION: This code creates a labeled tf.data.Dataset for text classification using the text_dataset_from_directory utility. It splits the data into training and validation sets.

LANGUAGE: Python
CODE:
batch_size = 32
seed = 42

raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='training',
    seed=seed)

----------------------------------------

TITLE: Implementing Prediction Logic in TensorFlow Estimator
DESCRIPTION: Python code to implement prediction logic in a TensorFlow Estimator, including class prediction and probability calculation.

LANGUAGE: python
CODE:
# Compute predictions.
predicted_classes = tf.argmax(logits, 1)
if mode == tf.estimator.ModeKeys.PREDICT:
    predictions = {
        'class_ids': predicted_classes[:, tf.newaxis],
        'probabilities': tf.nn.softmax(logits),
        'logits': logits,
    }
    return tf.estimator.EstimatorSpec(mode, predictions=predictions)

----------------------------------------

TITLE: Training Neural Network Model
DESCRIPTION: Train the model on MNIST training data for 5 epochs.

LANGUAGE: python
CODE:
model.fit(x_train, y_train, epochs=5)

----------------------------------------

TITLE: Creating Baseline Model in TensorFlow Keras
DESCRIPTION: Defines and compiles a baseline neural network model with two hidden layers and a sigmoid output layer for binary classification.

LANGUAGE: Python
CODE:
baseline_model = keras.Sequential([
    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),
    keras.layers.Dense(16, activation=tf.nn.relu),
    keras.layers.Dense(1, activation=tf.nn.sigmoid)
])

baseline_model.compile(optimizer='adam',
                       loss='binary_crossentropy',
                       metrics=['accuracy', 'binary_crossentropy'])

baseline_model.summary()

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Installation of required Python packages: tensorflow-hub, tensorflow-datasets, and tf-keras

LANGUAGE: python
CODE:
!pip install tensorflow-hub
!pip install tensorflow-datasets
!pip install tf-keras

----------------------------------------

TITLE: Seeding Random Number Generators in TensorFlow Tests
DESCRIPTION: Example showing how to properly seed different random number generators to ensure test determinism. This includes seeding Python's random module, NumPy's random number generator, and TensorFlow's random seed.

LANGUAGE: python
CODE:
# Python RNG
import random
random.seed(42)

# Numpy RNG
import numpy as np
np.random.seed(42)

# TF RNG
from tensorflow.python.framework import random_seed
random_seed.set_seed(42)

----------------------------------------

TITLE: Importing TensorFlow and Keras
DESCRIPTION: Imports the required TensorFlow and Keras libraries, as well as datetime for logging.

LANGUAGE: Python
CODE:
import tensorflow as tf
import keras
from datetime import datetime

%load_ext tensorboard

----------------------------------------

TITLE: Defining Model Function Signature for Custom TensorFlow Estimator
DESCRIPTION: Python function signature for the model function used in a custom TensorFlow Estimator.

LANGUAGE: python
CODE:
def my_model_fn(
   features, # This is batch_features from input_fn
   labels,   # This is batch_labels from input_fn
   mode,     # An instance of tf.estimator.ModeKeys
   params):  # Additional configuration

----------------------------------------

TITLE: Training and evaluating the image classification model
DESCRIPTION: Implements the training loop for the image classification model, including batching, training steps, and periodic evaluation on the test set.

LANGUAGE: Python
CODE:
NUM_TRAIN_STEPS = 100
TRAIN_BATCH_SIZE = 10
EVAL_EVERY = 10

def get_batch(batch_size=None, test=False):
    """Get a random batch of examples."""
    examples = TEST_EXAMPLES if test else TRAIN_EXAMPLES
    batch_examples = random.sample(examples, batch_size) if batch_size else examples
    return batch_examples

def get_images_and_labels(batch_examples):
    images = [get_encoded_image(e) for e in batch_examples]
    one_hot_labels = [get_label_one_hot(e) for e in batch_examples]
    return images, one_hot_labels

def get_label_one_hot(example):
    """Get the one hot encoding vector for the example."""
    one_hot_vector = np.zeros(NUM_CLASSES)
    np.put(one_hot_vector, get_label(example), 1)
    return one_hot_vector

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(NUM_TRAIN_STEPS):
        # Get a random batch of training examples.
        train_batch = get_batch(batch_size=TRAIN_BATCH_SIZE)
        batch_images, batch_labels = get_images_and_labels(train_batch)
        # Run the train_op to train the model.
        train_loss, _, train_accuracy = sess.run(
            [cross_entropy_mean, train_op, accuracy],
            feed_dict={encoded_images: batch_images, labels: batch_labels})
        is_final_step = (i == (NUM_TRAIN_STEPS - 1))
        if i % EVAL_EVERY == 0 or is_final_step:
            # Get a batch of test examples.
            test_batch = get_batch(batch_size=None, test=True)
            batch_images, batch_labels = get_images_and_labels(test_batch)
            # Evaluate how well our model performs on the test set.
            test_loss, test_accuracy, test_prediction, correct_predicate = sess.run(
                [cross_entropy_mean, accuracy, prediction, correct_prediction],
                feed_dict={encoded_images: batch_images, labels: batch_labels})
            print('Test accuracy at step %s: %.2f%%' % (i, (test_accuracy * 100)))

----------------------------------------

TITLE: Setting Up Loss Function and Optimizer for Wav2Vec2 Fine-tuning
DESCRIPTION: Defines the CTC loss function and Adam optimizer for fine-tuning the Wav2Vec2 model.

LANGUAGE: Python
CODE:
from wav2vec2 import CTCLoss

LEARNING_RATE = 5e-5

loss_fn = CTCLoss(config, (BATCH_SIZE, AUDIO_MAXLEN), division_factor=BATCH_SIZE)
optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)

----------------------------------------

TITLE: Setting upstream remote in Git
DESCRIPTION: Command to set the upstream remote in Git. This is done once per project to facilitate syncing with the main TensorFlow repository.

LANGUAGE: Bash
CODE:
$ git remote add upstream git@github.com:tensorflow/project-repo-name

----------------------------------------

TITLE: Using DeterministicRandomTestTool for TF1 and TF2 Comparison
DESCRIPTION: Demonstrate how to use the DeterministicRandomTestTool to align random number generation between TF1 and TF2 for model comparison.

LANGUAGE: Python
CODE:
random_tool = v1.keras.utils.DeterministicRandomTestTool()
with random_tool.scope():
  graph = tf.Graph()
  with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:
    a = tf.random.uniform(shape=(3,1))
    a = a * 3
    b = tf.random.uniform(shape=(3,3))
    b = b * 3
    c = tf.random.uniform(shape=(3,3))
    c = c * 3
    graph_a, graph_b, graph_c = sess.run([a, b, c])

graph_a, graph_b, graph_c

----------------------------------------

TITLE: Importing Dependencies and Version Check
DESCRIPTION: Import required libraries and display version information for TensorFlow, Hub, and check GPU availability

LANGUAGE: python
CODE:
import os
import numpy as np

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds
import tf_keras as keras

print("Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.list_physical_devices("GPU") else "NOT AVAILABLE")

----------------------------------------

TITLE: Defining Markdown Header for TensorFlow Tutorials
DESCRIPTION: This snippet defines a top-level markdown header for TensorFlow project tutorials. It uses a placeholder PROJECT_NAME which would likely be replaced with 'TensorFlow' in the actual documentation.

LANGUAGE: Markdown
CODE:
# PROJECT_NAME tutorials

----------------------------------------

TITLE: Training History Visualization
DESCRIPTION: Function to plot training and validation metrics history

LANGUAGE: python
CODE:
def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [MPG]')
  plt.plot(hist['epoch'], hist['mean_absolute_error'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],
           label = 'Val Error')
  plt.ylim([0,5])
  plt.legend()

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Square Error [$MPG^2$]')
  plt.plot(hist['epoch'], hist['mean_squared_error'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mean_squared_error'],
           label = 'Val Error')
  plt.ylim([0,20])
  plt.legend()
  plt.show()

----------------------------------------

TITLE: Creating Keras Model with MirroredStrategy Scope
DESCRIPTION: Defines and compiles a Keras sequential model within the context of the MirroredStrategy scope for distributed training.

LANGUAGE: Python
CODE:
with strategy.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])

  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

----------------------------------------

TITLE: Building TensorFlow Package with Bazel (CPU)
DESCRIPTION: Builds the TensorFlow pip package for CPU using Bazel build system.

LANGUAGE: Batch
CODE:
bazel build --config=opt --repo_env=TF_PYTHON_VERSION=3.11 //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu

----------------------------------------

TITLE: Loading TensorFlow Module Checkpoint
DESCRIPTION: Shows how to load a saved TensorFlow module checkpoint into a new model instance.

LANGUAGE: Python
CODE:
new_model = MySequentialModule()
new_checkpoint = tf.train.Checkpoint(model=new_model)
new_checkpoint.restore("my_checkpoint")

# Should be the same result as above
new_model(tf.constant([[2.0, 2.0, 2.0]]))

----------------------------------------

TITLE: Implementing Training Loop
DESCRIPTION: Defines a training function that performs gradient descent using GradientTape to update model parameters.

LANGUAGE: Python
CODE:
def train(model, x, y, learning_rate):
  with tf.GradientTape() as t:
    current_loss = loss(y, model(x))
  dw, db = t.gradient(current_loss, [model.w, model.b])
  model.w.assign_sub(learning_rate * dw)
  model.b.assign_sub(learning_rate * db)

----------------------------------------

TITLE: Saving Variables to Checkpoint
DESCRIPTION: Example showing how to create TensorFlow variables and save them to checkpoint files using tf.train.Saver

LANGUAGE: python
CODE:
# Create some variables.
v1 = tf.get_variable("v1", shape=[3], initializer = tf.zeros_initializer)
v2 = tf.get_variable("v2", shape=[5], initializer = tf.zeros_initializer)

inc_v1 = v1.assign(v1+1)
dec_v2 = v2.assign(v2-1)

# Add an op to initialize the variables.
init_op = tf.global_variables_initializer()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, initialize the variables, do some work, and save the
# variables to disk.
with tf.Session() as sess:
  sess.run(init_op)
  # Do some work with the model.
  inc_v1.op.run()
  dec_v2.op.run()
  # Save the variables to disk.
  save_path = saver.save(sess, "/tmp/model.ckpt")
  print("Model saved in path: %s" % save_path)

----------------------------------------

TITLE: Installing Pre-release Versions of TensorFlow and TensorFlow Hub
DESCRIPTION: Commands to install nightly builds of TensorFlow and TensorFlow Hub for developers who want to try the latest code without building from source. These versions are not release tested.

LANGUAGE: bash
CODE:
$ pip install tf-nightly
$ pip install --upgrade tf-hub-nightly

----------------------------------------

TITLE: Importing TensorFlow and Other Libraries
DESCRIPTION: Import the necessary libraries including TensorFlow, NumPy, and Pandas for data processing and model building.

LANGUAGE: Python
CODE:
import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow.keras import layers

----------------------------------------

TITLE: Installing TensorFlow 2 and TensorFlow Hub
DESCRIPTION: Commands to install TensorFlow 2 and the latest version of TensorFlow Hub using pip. This is the recommended setup for new users and requires TensorFlow Hub version 0.5.0 or newer.

LANGUAGE: bash
CODE:
$ pip install "tensorflow>=2.0.0"
$ pip install --upgrade tensorflow-hub

----------------------------------------

TITLE: Performing Hyperparameter Search
DESCRIPTION: Execute the hyperparameter search using the tuner and retrieve the best hyperparameters.

LANGUAGE: python
CODE:
stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])

# Get the optimal hyperparameters
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

----------------------------------------

TITLE: Unicode String Conversions
DESCRIPTION: Shows how to convert between different Unicode representations using TensorFlow operations

LANGUAGE: Python
CODE:
# Decode UTF-8 string to code points
tf.strings.unicode_decode(text_utf8, input_encoding='UTF-8')

# Encode code points to UTF-8 string
tf.strings.unicode_encode(text_chars, output_encoding='UTF-8')

# Transcode between encodings
tf.strings.unicode_transcode(text_utf8, input_encoding='UTF8', output_encoding='UTF-16-BE')

----------------------------------------

TITLE: Training Keras Model with NumPy Arrays and MirroredStrategy in TensorFlow
DESCRIPTION: Demonstrates training a Keras model using MirroredStrategy with NumPy arrays as input. The model is trained for 2 epochs with a batch size of 10.

LANGUAGE: Python
CODE:
import numpy as np
inputs, targets = np.ones((100, 1)), np.ones((100, 1))
model.fit(inputs, targets, epochs=2, batch_size=10)

----------------------------------------

TITLE: Using a Custom FileSystem in Python with TensorFlow's gfile
DESCRIPTION: Example of how to use a custom FileSystem implementation in Python using TensorFlow's gfile interface after loading the filesystem library.

LANGUAGE: Python
CODE:
with gfile.Open("foobar://path/to/file.txt") as w:
  w.write("hi")

----------------------------------------

TITLE: Image Processing and Visualization Functions
DESCRIPTION: Helper functions for downloading, resizing and displaying images, as well as drawing bounding boxes for object detection visualization. Adapted from TF object detection API.

LANGUAGE: python
CODE:
def display_image(image):
  fig = plt.figure(figsize=(20, 15))
  plt.grid(False)
  plt.imshow(image)

def download_and_resize_image(url, new_width=256, new_height=256, display=False):
  _, filename = tempfile.mkstemp(suffix=".jpg")
  response = urlopen(url)
  image_data = response.read()
  image_data = BytesIO(image_data)
  pil_image = Image.open(image_data)
  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.LANCZOS)
  pil_image_rgb = pil_image.convert("RGB")
  pil_image_rgb.save(filename, format="JPEG", quality=90)
  print("Image downloaded to %s." % filename)
  if display:
    display_image(pil_image)
  return filename

----------------------------------------

TITLE: Installing MSYS2 Tools
DESCRIPTION: Installs necessary tools using MSYS2 package manager for building TensorFlow.

LANGUAGE: Batch
CODE:
pacman -Syu
pacman -S git patch unzip
pacman -S git patch unzip rsync

----------------------------------------

TITLE: Configuring Learning Rate Schedule
DESCRIPTION: Sets up a hyperbolic learning rate decay schedule using InverseTimeDecay

LANGUAGE: Python
CODE:
lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  0.001,
  decay_steps=STEPS_PER_EPOCH*1000,
  decay_rate=1,
  staircase=False)

def get_optimizer():
  return tf.keras.optimizers.Adam(lr_schedule)

----------------------------------------

TITLE: Loading and Inference with TF2 SavedModel in TF2
DESCRIPTION: Demonstrates how to load and perform inference with a TF2 SavedModel in TensorFlow 2 using the hub.load function.

LANGUAGE: python
CODE:
m = hub.load(handle)
outputs = m(inputs)

----------------------------------------

TITLE: Building TensorFlow with Optimizations for Specific CPU Architecture
DESCRIPTION: This bash command shows how to build TensorFlow from source with optimizations for a specific CPU architecture (Intel Broadwell in this example) using bazel.

LANGUAGE: bash
CODE:
# This command optimizes for Intel's Broadwell processor
bazel build -c opt --copt=-march="broadwell" --config=cuda //tensorflow/tools/pip_package:build_pip_package

----------------------------------------

TITLE: Saving and Loading Entire Keras Models
DESCRIPTION: Shows how to save and load entire Keras models, including weights, model configuration, and optimizer state.

LANGUAGE: Python
CODE:
# Create a trivial model
model = tf.keras.Sequential([
  layers.Dense(64, activation='relu', input_shape=(32,)),
  layers.Dense(10, activation='softmax')
])
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(data, labels, batch_size=32, epochs=5)


# Save entire model to a HDF5 file
model.save('my_model.h5')

# Recreate the exact same model, including weights and optimizer.
model = tf.keras.models.load_model('my_model.h5')

----------------------------------------

TITLE: Loading MNIST Dataset with TensorFlow Datasets
DESCRIPTION: Demonstrates loading and preparing the MNIST dataset using tfds, including scaling, shuffling and batching the data.

LANGUAGE: python
CODE:
datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
mnist_train, mnist_test = datasets['train'], datasets['test']

BUFFER_SIZE = 10 # Use a much larger value for real code
BATCH_SIZE = 64
NUM_EPOCHS = 5

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255
  return image, label

----------------------------------------

TITLE: Configuring Linux Linker for TensorFlow C Library
DESCRIPTION: This bash command configures the Linux linker to include the TensorFlow C library extracted to /usr/local/lib.

LANGUAGE: bash
CODE:
sudo ldconfig /usr/local/lib

----------------------------------------

TITLE: Re-exporting Fine-tuned SavedModel
DESCRIPTION: Shows how to save a fine-tuned TF Hub model back into a new SavedModel for reuse.

LANGUAGE: python
CODE:
loaded_obj = hub.load("https://tfhub.dev/...")
hub_layer = hub.KerasLayer(loaded_obj, trainable=True, ...)

model = keras.Sequential([..., hub_layer, ...])
model.compile(...)
model.fit(...)

export_module_dir = os.path.join(os.getcwd(), "finetuned_model_export")
tf.saved_model.save(loaded_obj, export_module_dir)

----------------------------------------

TITLE: Replacing Project Name in Template Files using Bash
DESCRIPTION: This command uses find and sed to replace 'PROJECT_NAME' with the actual project name in all files within the g3doc directory. This step customizes the template for the specific project.

LANGUAGE: bash
CODE:
$ find tensorflow/myproject/g3doc/ -type f | xargs sed -i 's/PROJECT_NAME/myproject/g'

----------------------------------------

TITLE: Printing Predictions and Probabilities in TensorFlow
DESCRIPTION: Demonstrates how to iterate through predictions, extract class IDs and probabilities, and print them alongside expected values.

LANGUAGE: python
CODE:
template = ('\nPrediction is "{}" ({:.1f}%), expected "{}"')

for pred_dict, expec in zip(predictions, expected):
    class_id = pred_dict['class_ids'][0]
    probability = pred_dict['probabilities'][class_id]

    print(template.format(iris_data.SPECIES[class_id],
                          100 * probability, expec))

----------------------------------------

TITLE: Importing Required Libraries for Boundless Image Extrapolation in Python
DESCRIPTION: This code snippet imports necessary libraries for working with TensorFlow, TensorFlow Hub, image processing, and visualization.

LANGUAGE: Python
CODE:
import tensorflow as tf
import tensorflow_hub as hub
from io import BytesIO
from PIL import Image as PilImage
import numpy as np
from matplotlib import pyplot as plt
from six.moves.urllib.request import urlopen

----------------------------------------

TITLE: Importing TensorFlow and Dependencies
DESCRIPTION: Imports TensorFlow 1.x compatibility mode and helper libraries numpy and os

LANGUAGE: Python
CODE:
import tensorflow.compat.v1 as tf

# Helper libraries
import numpy as np
import os

print(tf.__version__)

----------------------------------------

TITLE: Similarity Visualization Function
DESCRIPTION: Defines functions to plot semantic similarity between sentences using a heatmap visualization

LANGUAGE: python
CODE:
def plot_similarity(labels, features, rotation):
  corr = np.inner(features, features)
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap="YlOrRd")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Textual Similarity")

----------------------------------------

TITLE: Using custom cross-device operations with MirroredStrategy
DESCRIPTION: Creates a MirroredStrategy instance with a custom cross-device operation specified.

LANGUAGE: Python
CODE:
mirrored_strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())

----------------------------------------

TITLE: Importing Required Libraries
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TF-Hub, data analysis and visualization packages

LANGUAGE: python
CODE:
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import zipfile

from sklearn import model_selection

----------------------------------------

TITLE: Evaluating DNNClassifier Estimator in TensorFlow
DESCRIPTION: This code evaluates the trained DNNClassifier Estimator on the test dataset and prints the accuracy.

LANGUAGE: Python
CODE:
eval_result = classifier.evaluate(
    input_fn=lambda: input_fn(test, test_y, training=False))

print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))

----------------------------------------

TITLE: Creating a MirroredStrategy instance
DESCRIPTION: Creates a basic MirroredStrategy instance to distribute training across multiple GPUs on a single machine.

LANGUAGE: Python
CODE:
mirrored_strategy = tf.distribute.MirroredStrategy()

----------------------------------------

TITLE: Creating a MirroredStrategy instance
DESCRIPTION: Creates a basic MirroredStrategy instance to distribute training across multiple GPUs on a single machine.

LANGUAGE: Python
CODE:
mirrored_strategy = tf.distribute.MirroredStrategy()

----------------------------------------

TITLE: Distributed Variable Placement
DESCRIPTION: Demonstrates placing variables in a distributed setting using replica_device_setter.

LANGUAGE: python
CODE:
cluster_spec = {
    "ps": ["ps0:2222", "ps1:2222"],
    "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
with tf.device(tf.train.replica_device_setter(cluster=cluster_spec)):
  v = tf.get_variable("v", shape=[20, 20])

----------------------------------------

TITLE: Defining Constants for Iris Dataset in Python
DESCRIPTION: This code defines constants for column names and species in the Iris dataset, which will be used for data parsing and classification.

LANGUAGE: Python
CODE:
CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']
SPECIES = ['Setosa', 'Versicolor', 'Virginica']

----------------------------------------

TITLE: Loading TF1 Hub Module in TF2 using hub.load
DESCRIPTION: Shows how to load a TF1 Hub module in TensorFlow 2 using the hub.load function and access its signatures.

LANGUAGE: python
CODE:
m = hub.load(handle)
outputs = m.signatures["sig"](inputs)

----------------------------------------

TITLE: Installing Required Libraries for Video Classification
DESCRIPTION: Installs necessary Python libraries including remotezip, tqdm, OpenCV, einops, and TensorFlow for video processing and machine learning tasks.

LANGUAGE: Python
CODE:
!pip install remotezip tqdm opencv-python einops 
!pip install -U tensorflow keras

----------------------------------------

TITLE: Creating a Custom Keras Model by Subclassing
DESCRIPTION: Shows how to create a custom Keras model by subclassing tf.keras.Model and defining a custom forward pass.

LANGUAGE: Python
CODE:
class MyModel(tf.keras.Model):

  def __init__(self, num_classes=10):
    super(MyModel, self).__init__(name='my_model')
    self.num_classes = num_classes
    # Define your layers here.
    self.dense_1 = layers.Dense(32, activation='relu')
    self.dense_2 = layers.Dense(num_classes, activation='sigmoid')

  def call(self, inputs):
    # Define your forward pass here,
    # using layers you previously defined (in `__init__`).
    x = self.dense_1(inputs)
    return self.dense_2(x)

  def compute_output_shape(self, input_shape):
    # You need to override this function if you want to use the subclassed model
    # as part of a functional-style model.
    # Otherwise, this method is optional.
    shape = tf.TensorShape(input_shape).as_list()
    shape[-1] = self.num_classes
    return tf.TensorShape(shape)

model = MyModel(num_classes=10)

# The compile step specifies the training configuration.
model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Trains for 5 epochs.
model.fit(data, labels, batch_size=32, epochs=5)

----------------------------------------

TITLE: Making Predictions with Trained Model in TensorFlow
DESCRIPTION: Uses the trained model to make predictions on the evaluation set and visualize the results.

LANGUAGE: Python
CODE:
pred_dicts = list(linear_est.predict(eval_input_fn))
probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])

probs.plot(kind='hist', bins=20, title='predicted probabilities')

----------------------------------------

TITLE: Creating and Inspecting Compressed Model Archives
DESCRIPTION: Shell commands showing how to create a compressed tar.gz archive from a SavedModel directory and inspect its contents. The commands demonstrate proper file ownership and compression format.

LANGUAGE: shell
CODE:
# Create a compressed model from a SavedModel directory.
$ tar -cz -f model.tar.gz --owner=0 --group=0 -C /tmp/export-model/ .

# Inspect files inside a compressed model
$ tar -tf model.tar.gz
./
./variables/
./variables/variables.data-00000-of-00001
./variables/variables.index
./assets/
./saved_model.pb

----------------------------------------

TITLE: Compiling and Running TensorFlow C Example
DESCRIPTION: This bash script compiles the example C program with the TensorFlow C library and runs the resulting executable.

LANGUAGE: bash
CODE:
gcc hello_tf.c -ltensorflow -o hello_tf

./hello_tf

----------------------------------------

TITLE: Converting Keras Model to TensorFlow Estimator
DESCRIPTION: Complete example of creating an Inception v3 Keras model and converting it to a TensorFlow Estimator with training setup.

LANGUAGE: python
CODE:
# Instantiate a Keras inception v3 model.
keras_inception_v3 = tf.keras.applications.inception_v3.InceptionV3(weights=None)

# Compile model with the optimizer, loss, and metrics you'd like to train with.
keras_inception_v3.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),
                          loss='categorical_crossentropy',
                          metric='accuracy')
                          
# Create an Estimator from the compiled Keras model. Note the initial model
# state of the keras model is preserved in the created Estimator.
est_inception_v3 = tf.keras.estimator.model_to_estimator(keras_model=keras_inception_v3)

# Treat the derived Estimator as you would with any other Estimator.
# First, recover the input name(s) of Keras model, so we can use them as the
# feature column name(s) of the Estimator input function:
keras_inception_v3.input_names  # print out: ['input_1']

# Once we have the input name(s), we can create the input function, for example,
# for input(s) in the format of numpy ndarray:
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"input_1": train_data},
    y=train_labels,
    num_epochs=1,
    shuffle=False)
    
# To train, we call Estimator's train function:
est_inception_v3.train(input_fn=train_input_fn, steps=2000)

----------------------------------------

TITLE: Importing TensorFlow and NumPy in Python
DESCRIPTION: This snippet demonstrates how to import the TensorFlow and NumPy libraries, which are commonly used in machine learning projects.

LANGUAGE: python
CODE:
import tensorflow as tf

import numpy as np

----------------------------------------

TITLE: Running Pylint Check on TensorFlow Files
DESCRIPTION: Command to run pylint on a specific TensorFlow Python file using the project's custom pylint configuration.

LANGUAGE: bash
CODE:
$ pylint --rcfile=tensorflow/tools/ci_build/pylintrc tensorflow/python/keras/losses.py

----------------------------------------

TITLE: Evaluating Model Accuracy on Test Dataset
DESCRIPTION: Calculates and prints the model's accuracy on the test dataset.

LANGUAGE: Python
CODE:
test_accuracy = tf.keras.metrics.Accuracy()
ds_test_batch = ds_test.batch(10)

for (x, y) in ds_test_batch:
  logits = model(x, training=False)
  prediction = tf.math.argmax(logits, axis=1, output_type=tf.int64)
  test_accuracy(prediction, y)

print("Test set accuracy: {:.3%}".format(test_accuracy.result()))

----------------------------------------

TITLE: Installing Required Libraries in Python
DESCRIPTION: Installs the scikit-learn library using pip.

LANGUAGE: Python
CODE:
!pip install sklearn

----------------------------------------

TITLE: Importing Required Libraries for Image Classification
DESCRIPTION: Imports necessary Python libraries including TensorFlow, TensorFlow Hub, and matplotlib for building and training an image classifier.

LANGUAGE: Python
CODE:
import itertools
import os

import matplotlib.pylab as plt
import numpy as np

import tensorflow as tf
import tensorflow_hub as hub

print("TF version:", tf.__version__)
print("Hub version:", hub.__version__)
print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "NOT AVAILABLE")

----------------------------------------

TITLE: Handling Non-Deterministic Output in TensorFlow DocTest
DESCRIPTION: This snippet shows how to handle non-deterministic output in DocTest by using ellipsis (...) for uncertain parts of the output.

LANGUAGE: Python
CODE:
>>> x = tf.random.normal((1,))
>>> print(x)
<tf.Tensor: shape=(1,), dtype=float32, numpy=..., dtype=float32)>

----------------------------------------

TITLE: Installing APU Example Plugin for TensorFlow
DESCRIPTION: Demonstrates how to install a sample Awesome Processing Unit (APU) plugin package for TensorFlow using pip.

LANGUAGE: sh
CODE:
# Install the APU example plug-in package
$ pip install tensorflow-apu-0.0.1-cp36-cp36m-linux_x86_64.whl
...
Successfully installed tensorflow-apu-0.0.1

----------------------------------------

TITLE: Loading and Preprocessing Fashion MNIST Dataset
DESCRIPTION: Loads the Fashion MNIST dataset and preprocesses the images by scaling pixel values from 0-255 to 0-1 range.

LANGUAGE: python
CODE:
fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images = train_images / 255.0
test_images = test_images / 255.0

----------------------------------------

TITLE: Custom Reduce Mean Layer Definition
DESCRIPTION: Custom Keras layer implementation for reducing mean across specified axis

LANGUAGE: python
CODE:
class ReduceMeanLayer(tf.keras.layers.Layer):
  def __init__(self, axis=0, **kwargs):
    super(ReduceMeanLayer, self).__init__(**kwargs)
    self.axis = axis

  def call(self, input):
    return tf.math.reduce_mean(input, axis=self.axis)

----------------------------------------

TITLE: Writing Multi-line Blocks in TensorFlow DocTest
DESCRIPTION: This snippet demonstrates how to write multi-line blocks in DocTest, using (...) for continued lines.

LANGUAGE: Python
CODE:
>>> if x > 0:
...   print("X is positive")
>>> model.compile(
...   loss="mse",
...   optimizer="adam")

----------------------------------------

TITLE: MNIST Model Training Loop
DESCRIPTION: Implements a custom training loop for MNIST classification using gradient tape and optimizer

LANGUAGE: python
CODE:
for (batch, (images, labels)) in enumerate(dataset.take(400)):
  if batch % 10 == 0:
    print('.', end='')
  with tf.GradientTape() as tape:
    logits = mnist_model(images, training=True)
    loss_value = tf.losses.sparse_softmax_cross_entropy(labels, logits)

  loss_history.append(loss_value.numpy())
  grads = tape.gradient(loss_value, mnist_model.trainable_variables)
  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables),
                            global_step=tf.train.get_or_create_global_step())

----------------------------------------

TITLE: Training a TensorFlow Estimator Model
DESCRIPTION: Shows how to train an Estimator model using the train method, specifying the input function and number of training steps.

LANGUAGE: python
CODE:
# Train the Model.
classifier.train(
    input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),
    steps=args.train_steps)

----------------------------------------

TITLE: Building TensorFlow GPU Package
DESCRIPTION: Bazel command to build TensorFlow pip package with GPU support

LANGUAGE: bash
CODE:
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel

----------------------------------------

TITLE: One-hot Encoding with Category Encoding Layer
DESCRIPTION: Shows how to perform one-hot encoding of integer inputs using the CategoryEncoding layer

LANGUAGE: Python
CODE:
one_hot_layer = tf.keras.layers.CategoryEncoding(
    num_tokens=3, output_mode='one_hot')
one_hot_layer([0, 1, 2])

----------------------------------------

TITLE: TensorFlow 2 Dataset Preparation
DESCRIPTION: Creating TensorFlow 2 datasets for training and evaluation

LANGUAGE: python
CODE:
dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(1)
eval_dataset = tf.data.Dataset.from_tensor_slices(
      (eval_features, eval_labels)).batch(1)

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Installing matplotlib version 3.2.2 as a required dependency for visualization

LANGUAGE: bash
CODE:
!pip install matplotlib==3.2.2

----------------------------------------

TITLE: Instantiating Hyperband Tuner
DESCRIPTION: Create an instance of the Hyperband tuner with the model builder and search parameters.

LANGUAGE: python
CODE:
tuner = kt.Hyperband(model_builder,
                     objective='val_accuracy',
                     max_epochs=10,
                     factor=3,
                     directory='my_dir',
                     project_name='intro_to_kt')

----------------------------------------

TITLE: Basic Tensor Slicing with tf.slice
DESCRIPTION: Demonstrates basic tensor slicing using tf.slice and Python slice syntax on a 1D tensor

LANGUAGE: Python
CODE:
t1 = tf.constant([0, 1, 2, 3, 4, 5, 6, 7])

print(tf.slice(t1,
               begin=[1],
               size=[3]))